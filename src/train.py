import os
import time 
import numpy as np
import torch
import torch.nn as nn
import matplotlib.pyplot as plt
from torch.utils.data import DataLoader
from typing import Optional, Tuple, List
from src.config import *
from src.config import TrainingConfig, DEFAULT_TRAINING_CONFIG
from src.logger import Logger
from src.model import SimpleLSTM
from src.dataset import MaichartDataset, collate_fn

def train_model(model, train_loader, val_loader, config: TrainingConfig, logger: Logger) -> Tuple[List[float], List[float]]:
    """
    æ”¹è¿›çš„æ¨¡å‹è®­ç»ƒå‡½æ•°ï¼ŒåŒ…å«éªŒè¯é›†ç›‘æ§ã€æ—©åœæœºåˆ¶ã€å­¦ä¹ ç‡è°ƒåº¦å’Œæ—¥å¿—è®°å½•ã€‚
    
    Args:
        model (nn.Module): è¦è®­ç»ƒçš„ PyTorch æ¨¡å‹
        train_loader (DataLoader): è®­ç»ƒæ•°æ®åŠ è½½å™¨
        val_loader (DataLoader): éªŒè¯æ•°æ®åŠ è½½å™¨
        config (TrainingConfig): è®­ç»ƒé…ç½®å¯¹è±¡
        logger (Logger): ç”¨äºè®°å½•æ—¥å¿—çš„ Logger å®ä¾‹
    
    Returns:
        tuple[list, list]: åŒ…å«è®­ç»ƒå’ŒéªŒè¯æŸå¤±çš„å…ƒç»„
    """
    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(
        model.parameters(), 
        lr=config.learning_rate, 
        weight_decay=config.weight_decay
    )
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, 
        config.scheduler_mode, 
        patience=config.scheduler_patience, 
        factor=config.scheduler_factor
    )
    
    best_val_loss = float('inf')
    patience_counter = 0
    
    train_losses = []
    val_losses = []
    
    # æ€»è®­ç»ƒå¼€å§‹æ—¶é—´
    total_start_time = time.time()
    
    for epoch in range(config.num_epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0.0
        train_batch_times = []
        print(f"Epoch [{epoch+1}/{config.num_epochs}] - è®­ç»ƒé˜¶æ®µ")
        print("-" * 60)
        
        for batch_idx, (sequences, labels) in enumerate(train_loader):
            batch_start_time = time.time()
            
            # å°†æ•°æ®ç§»åŠ¨åˆ° GPU
            sequences, labels = sequences.to(DEVICE), labels.to(DEVICE)
            
            optimizer.zero_grad()
            outputs = model(sequences)
            loss = criterion(outputs, labels)
            loss.backward()
            
            # æ¢¯åº¦è£å‰ªé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=config.grad_clip_max_norm)
            
            optimizer.step()
            batch_time = time.time() - batch_start_time
            train_batch_times.append(batch_time)
            train_loss += loss.item()            # æ¯10ä¸ªbatchè¾“å‡ºä¸€æ¬¡è¿›åº¦ï¼ˆå¯è°ƒæ•´é¢‘ç‡ï¼‰
            if (batch_idx + 1) % config.log_interval == 0 or (batch_idx + 1) == len(train_loader):
                avg_batch_time = np.mean(train_batch_times[-10:])  # æœ€è¿‘10ä¸ªbatchçš„å¹³å‡æ—¶é—´
                print(f"  Batch [{batch_idx+1:4d}/{len(train_loader):4d}] | "
                      f"Loss: {loss.item():.4f} | "
                      f"Batch Time: {batch_time:.2f}s | "
                      f"Avg Time: {avg_batch_time:.2f}s | "
                      f"Seq Shape: {sequences.shape}")
        
        avg_train_loss = train_loss / len(train_loader)
        train_losses.append(avg_train_loss)
        
        # è®°å½•è®­ç»ƒæŸå¤±åˆ° TensorBoard
        logger.log_scalar('Loss/Train', avg_train_loss, epoch)
        logger.log_scalar('Learning_Rate', optimizer.param_groups[0]['lr'], epoch)
        
        # è®°å½•æ¨¡å‹å‚æ•°åˆ†å¸ƒ
        for name, param in model.named_parameters():
            if param.grad is not None:
                logger.log_histogram(f'Parameters/{name}', param, epoch)
                logger.log_histogram(f'Gradients/{name}', param.grad, epoch)

        # === éªŒè¯é˜¶æ®µ ===
        print(f"\nEpoch [{epoch+1}/{config.num_epochs}] - éªŒè¯é˜¶æ®µ")
        model.eval()
        val_loss = 0.0
        with torch.no_grad():
            for sequences, labels in val_loader:
                # å°†æ•°æ®ç§»åŠ¨åˆ° GPU
                sequences, labels = sequences.to(DEVICE), labels.to(DEVICE)
                
                outputs = model(sequences)
                loss = criterion(outputs, labels)
                val_loss += loss.item()
        
        avg_val_loss = val_loss / len(val_loader)
        val_losses.append(avg_val_loss)
        
        # è®°å½•éªŒè¯æŸå¤±åˆ° TensorBoard
        logger.log_scalar('Loss/Validation', avg_val_loss, epoch)
        
        # å­¦ä¹ ç‡è°ƒåº¦
        scheduler.step(avg_val_loss)
        
        # æ—©åœæ£€æŸ¥
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            patience_counter = 0
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            model_save_path = os.path.join(MODEL_DIR, "best_model.pth")
            torch.save(model.state_dict(), model_save_path)
            print(f"æ–°çš„æœ€ä½³æ¨¡å‹å·²ä¿å­˜ (éªŒè¯æŸå¤±: {best_val_loss:.6f})")
        else:
            patience_counter += 1
            print(f"æ—©åœè®¡æ•°å™¨: {patience_counter}/{config.early_stop_patience}")
        
        print(f"Epoch [{epoch+1}/{config.num_epochs}] - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}")

        # ä¼°ç®—å‰©ä½™æ—¶é—´
        elapsed_time = time.time() - total_start_time
        avg_epoch_time = elapsed_time / (epoch + 1)
        remaining_epochs = config.num_epochs - (epoch + 1)
        estimated_remaining_time = avg_epoch_time * remaining_epochs
        print(f"  å·²ç”¨æ—¶é—´: {elapsed_time/60:.1f}åˆ†é’Ÿ | é¢„è®¡å‰©ä½™: {estimated_remaining_time/60:.1f}åˆ†é’Ÿ")
        print(f"{'='*60}")
        
        if patience_counter >= config.early_stop_patience:
            print(f"Early stopping at epoch {epoch+1}")
            break
    
    total_training_time = time.time() - total_start_time
    print(f"\nğŸ‰ è®­ç»ƒå®Œæˆ!")
    print(f"æ€»è®­ç»ƒæ—¶é—´: {total_training_time/60:.1f} åˆ†é’Ÿ")
    print(f"æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f}")
    
    return train_losses, val_losses

def evaluate_model(model, data_loader, config: TrainingConfig, logger: Logger = None, epoch: int = None, phase: str = ""):
    """å…¨é¢è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Œæ”¯æŒ TensorBoard è®°å½•"""
    model.eval()
    predictions = []
    true_values = []
    
    with torch.no_grad():
        for sequences, labels in data_loader:
            sequences, labels = sequences.to(DEVICE), labels.to(DEVICE)
            outputs = model(sequences)
            predictions.extend(outputs.cpu().numpy().flatten())
            true_values.extend(labels.cpu().numpy().flatten())
    
    predictions = np.array(predictions)
    true_values = np.array(true_values)
    
    # è®¡ç®—å„ç§æŒ‡æ ‡
    mse = np.mean((predictions - true_values) ** 2)
    mae = np.mean(np.abs(predictions - true_values))
    r2 = 1 - np.sum((true_values - predictions) ** 2) / np.sum((true_values - np.mean(true_values)) ** 2)
    
    # å‡†ç¡®æ€§åˆ†æï¼ˆåœ¨ä¸åŒè¯¯å·®èŒƒå›´å†…çš„æ¯”ä¾‹ï¼‰
    accuracy_results = {}
    accuracy_thresholds = config.accuracy_thresholds
    for threshold in accuracy_thresholds:
        accuracy_results[f'acc_{str(threshold).replace(".", "")}'] = np.mean(np.abs(predictions - true_values) <= threshold)    
    print(f"è¯„ä¼°ç»“æœ {phase}:")
    print(f"  MSE: {mse:.4f}")
    print(f"  MAE: {mae:.4f}")
    print(f"  RÂ²: {r2:.4f}")
    for threshold in accuracy_thresholds:
        acc_key = f'acc_{str(threshold).replace(".", "")}'
        print(f"  Â±{threshold}å‡†ç¡®ç‡: {accuracy_results[acc_key]:.3f}")
    
    results = {
        'mse': mse, 'mae': mae, 'r2': r2,
        **accuracy_results,
        'predictions': np.array(predictions), 
        'true_values': np.array(true_values)
    }

    # è®°å½•åˆ° TensorBoard
    if logger and epoch is not None:
        logger.log_evaluation_metrics(results, epoch, phase)
        logger.log_evaluation_histograms(results, epoch, phase)
        
        # åˆ›å»ºå¹¶è®°å½•æ•£ç‚¹å›¾
        if len(predictions) <= config.max_scatter_points:
            scatter_fig = create_prediction_scatter_plot(
                predictions, true_values, 
                title=f"Predictions vs True Values {phase}"
            )
            logger.log_scatter_plot(scatter_fig, epoch, phase)
            plt.close(scatter_fig)
            
    return results

def train_complete_pipeline():
    """å®Œæ•´çš„è®­ç»ƒæµç¨‹ï¼ŒåŒ…å« TensorBoard æ—¥å¿—è®°å½•"""

    # å®éªŒé…ç½®
    config = TrainingConfig()
    print(f"å®éªŒé…ç½®: {config.to_dict()}")
    print("="*80)

    # åˆå§‹åŒ– Logger
    logger = Logger(BASE_DIR, experiment_name_prefix="maimai_difficulty_prediction")

    # åˆ›å»ºæ•°æ®é›†
    print("\nåˆ›å»ºæ•°æ®é›†...")
    dataset_start_time = time.time()
    train_dataset = MaichartDataset(SERIALIZED_DIR, TRAIN_DATA_PATH)
    test_dataset = MaichartDataset(SERIALIZED_DIR, TEST_DATA_PATH)
    dataset_time = time.time() - dataset_start_time
    print(f"æ•°æ®é›†åˆ›å»ºå®Œæˆ ({dataset_time:.3f}s)")
    print(f"è®­ç»ƒé›†å¤§å°: {len(train_dataset)}, æµ‹è¯•é›†å¤§å°: {len(test_dataset)}")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    print("\nåˆ›å»ºæ•°æ®åŠ è½½å™¨...")
    loader_start_time = time.time()

    train_loader = DataLoader(
        train_dataset,
        batch_size=config.batch_size,
        shuffle=True,
        collate_fn=collate_fn,
        num_workers=0
    )

    # æµ‹è¯•åŠ è½½å™¨ä¸éœ€è¦åˆ†æ¡¶æˆ–æ‰“ä¹±
    test_loader = DataLoader(
        test_dataset,
        batch_size=config.batch_size,
        shuffle=False,
        collate_fn=collate_fn,
        num_workers=0
    )
    loader_time = time.time() - loader_start_time
    print(f"æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆ ({loader_time:.3f}s)")
    print(f"è®­ç»ƒæ‰¹æ¬¡æ•°: {len(train_loader)}, æµ‹è¯•æ‰¹æ¬¡æ•°: {len(test_loader)}")    # åˆ›å»ºæ¨¡å‹
    print("\nåˆ›å»ºæ¨¡å‹...")
    model_start_time = time.time()
    model = SimpleLSTM(
        config.input_size, 
        config.hidden_size, 
        config.output_size, 
        config.num_layers
    )
    model_time = time.time() - model_start_time
    param_count = sum(p.numel() for p in model.parameters())
    print(f"æ¨¡å‹åˆ›å»ºå®Œæˆ ({model_time:.3f}s)")
    print(f"æ¨¡å‹å‚æ•°æ•°é‡: {param_count:,}")
    print(f"æ¨¡å‹è®¾å¤‡: {next(model.parameters()).device}")

    # è®°å½•æ¨¡å‹ç»“æ„å›¾
    try:
        sample_input, _ = next(iter(train_loader))
        sample_input = sample_input.to(DEVICE)
        logger.log_model_graph(model, sample_input)
    except Exception as e:
        print(f"è®°å½•æ¨¡å‹ç»“æ„å›¾å¤±è´¥: {e}")

    # è®­ç»ƒæ¨¡å‹
    print("\nå¼€å§‹è®­ç»ƒ...")
    train_losses, val_losses = train_model(
        model, train_loader, test_loader, 
        config=config,
        logger=logger
    )
    
    # åŠ è½½æœ€ä½³æ¨¡å‹å¹¶è¯„ä¼°
    model.load_state_dict(torch.load(os.path.join(MODEL_DIR, "best_model.pth")))
    
    print("\n=== è®­ç»ƒé›†è¯„ä¼° ===")
    train_results = evaluate_model(model, train_loader, config, logger, len(train_losses), "Train")
    
    print("\n=== æµ‹è¯•é›†è¯„ä¼° ===")
    test_results = evaluate_model(model, test_loader, config, logger, len(train_losses), "Test")

    # ä¿å­˜å®éªŒè®°å½•
    logger.save_experiment_summary(model, config, train_losses, val_losses, train_results, test_results)

    # å…³é—­ logger
    logger.close()

    return

def create_prediction_scatter_plot(predictions, true_values, title="Predictions vs True Values"):
    """åˆ›å»ºé¢„æµ‹å€¼ä¸çœŸå®å€¼çš„æ•£ç‚¹å›¾"""
    plt.figure(figsize=(8, 8))
    plt.scatter(true_values, predictions, alpha=0.6, s=20)
    
    # æ·»åŠ å®Œç¾é¢„æµ‹çº¿ (y=x)
    min_val = min(min(predictions), min(true_values))
    max_val = max(max(predictions), max(true_values))
    plt.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')
    
    plt.xlabel('True Values')
    plt.ylabel('Predictions')
    plt.title(title)
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # ä½¿ç”¨ plt.tight_layout() ç¡®ä¿å¸ƒå±€ç´§å‡‘
    plt.tight_layout()
    
    return plt.gcf()
