import os
import time 
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from src.config import *
from src.model import SimpleLSTM
from src.dataset import MaichartDataset, collate_fn

def train_model(model, train_loader, val_loader, num_epochs=50, learning_rate=0.001):
    """
    æ”¹è¿›çš„æ¨¡å‹è®­ç»ƒå‡½æ•°ï¼ŒåŒ…å«éªŒè¯é›†ç›‘æ§ã€æ—©åœæœºåˆ¶å’Œå­¦ä¹ ç‡è°ƒåº¦ã€‚
    
    Args:
        model (nn.Module): è¦è®­ç»ƒçš„ PyTorch æ¨¡å‹
        train_loader (DataLoader): è®­ç»ƒæ•°æ®åŠ è½½å™¨ï¼ŒåŒ…å« (sequences, labels) æˆ– (sequences, labels, padding_mask)
        val_loader (DataLoader): éªŒè¯æ•°æ®åŠ è½½å™¨ï¼Œæ ¼å¼åŒè®­ç»ƒæ•°æ®åŠ è½½å™¨
        num_epochs (int, optional): æœ€å¤§è®­ç»ƒè½®æ•°ã€‚é»˜è®¤ä¸º 50
        learning_rate (float, optional): åˆå§‹å­¦ä¹ ç‡ã€‚é»˜è®¤ä¸º 0.001
    
    Returns:
        tuple[list, list]: åŒ…å«ä¸¤ä¸ªåˆ—è¡¨çš„å…ƒç»„
            - train_losses (list): æ¯ä¸ª epoch çš„å¹³å‡è®­ç»ƒæŸå¤±
            - val_losses (list): æ¯ä¸ª epoch çš„å¹³å‡éªŒè¯æŸå¤±
    
    Training Strategy:
        - Loss Function: MSE (å‡æ–¹è¯¯å·®) ç”¨äºå›å½’ä»»åŠ¡
        - Optimizer: Adam ä¼˜åŒ–å™¨ï¼ŒåŒ…å« L2 æ­£åˆ™åŒ– (weight_decay=1e-5)
        - Scheduler: ReduceLROnPlateauï¼ŒéªŒè¯æŸå¤±åœæ­¢æ”¹å–„æ—¶é™ä½å­¦ä¹ ç‡
        - Early Stopping: è¿ç»­ 10 ä¸ª epoch éªŒè¯æŸå¤±æ— æ”¹å–„æ—¶åœæ­¢è®­ç»ƒ
        - Gradient Clipping: æœ€å¤§æ¢¯åº¦èŒƒæ•°é™åˆ¶ä¸º 1.0ï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
    
    Model Checkpointing:
        - è‡ªåŠ¨ä¿å­˜éªŒè¯æŸå¤±æœ€ä½çš„æ¨¡å‹æƒé‡åˆ° 'best_model.pth'
        - è®­ç»ƒç»“æŸåå¯é€šè¿‡ model.load_state_dict(torch.load('best_model.pth')) åŠ è½½æœ€ä½³æ¨¡å‹
    """
    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)
    
    best_val_loss = float('inf')
    patience_counter = 0
    early_stop_patience = 10
    
    train_losses = []
    val_losses = []
    
    # æ€»è®­ç»ƒå¼€å§‹æ—¶é—´
    total_start_time = time.time()
    
    for epoch in range(num_epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0.0
        train_batch_times = []
        print(f"\nEpoch [{epoch+1}/{num_epochs}] - è®­ç»ƒé˜¶æ®µ")
        print("-" * 60)
        
        for batch_idx, (sequences, labels) in enumerate(train_loader):
            batch_start_time = time.time()
            
            # å°†æ•°æ®ç§»åŠ¨åˆ° GPU
            sequences, labels = sequences.to(DEVICE), labels.to(DEVICE)
            
            optimizer.zero_grad()
            outputs = model(sequences)
            loss = criterion(outputs, labels)
            loss.backward()
            
            # æ¢¯åº¦è£å‰ªé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()
            batch_time = time.time() - batch_start_time
            train_batch_times.append(batch_time)
            train_loss += loss.item()

            # æ¯10ä¸ªbatchè¾“å‡ºä¸€æ¬¡è¿›åº¦ï¼ˆå¯è°ƒæ•´é¢‘ç‡ï¼‰
            if (batch_idx + 1) % 10 == 0 or (batch_idx + 1) == len(train_loader):
                avg_batch_time = np.mean(train_batch_times[-10:])  # æœ€è¿‘10ä¸ªbatchçš„å¹³å‡æ—¶é—´
                print(f"  Batch [{batch_idx+1:4d}/{len(train_loader):4d}] | "
                      f"Loss: {loss.item():.4f} | "
                      f"Batch Time: {batch_time:.2f}s | "
                      f"Avg Time: {avg_batch_time:.2f}s | "
                      f"Seq Shape: {sequences.shape}")
        
        avg_train_loss = train_loss / len(train_loader)
        train_losses.append(avg_train_loss)
        
        # === éªŒè¯é˜¶æ®µ ===
        print(f"\nEpoch [{epoch+1}/{num_epochs}] - éªŒè¯é˜¶æ®µ")
        model.eval()
        val_loss = 0.0
        with torch.no_grad():
            for sequences, labels in val_loader:
                # å°†æ•°æ®ç§»åŠ¨åˆ° GPU
                sequences, labels = sequences.to(DEVICE), labels.to(DEVICE)
                
                outputs = model(sequences)
                loss = criterion(outputs, labels)
                val_loss += loss.item()
        
        avg_val_loss = val_loss / len(val_loader)
        val_losses.append(avg_val_loss)
        
        # å­¦ä¹ ç‡è°ƒåº¦
        scheduler.step(avg_val_loss)
        
        # æ—©åœæ£€æŸ¥
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            patience_counter = 0
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            model_save_path = os.path.join(MODEL_DIR, "best_model.pth")
            torch.save(model.state_dict(), model_save_path)
            print(f"æ–°çš„æœ€ä½³æ¨¡å‹å·²ä¿å­˜ (éªŒè¯æŸå¤±: {best_val_loss:.6f})")
        else:
            patience_counter += 1
            print(f"æ—©åœè®¡æ•°å™¨: {patience_counter}/{early_stop_patience}")
        
        print(f"Epoch [{epoch+1}/{num_epochs}] - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}")

        # ä¼°ç®—å‰©ä½™æ—¶é—´
        elapsed_time = time.time() - total_start_time
        avg_epoch_time = elapsed_time / (epoch + 1)
        remaining_epochs = num_epochs - (epoch + 1)
        estimated_remaining_time = avg_epoch_time * remaining_epochs
        
        print(f"  å·²ç”¨æ—¶é—´: {elapsed_time/60:.1f}åˆ†é’Ÿ | é¢„è®¡å‰©ä½™: {estimated_remaining_time/60:.1f}åˆ†é’Ÿ")
        print(f"{'='*60}")
        
        if patience_counter >= early_stop_patience:
            print(f"Early stopping at epoch {epoch+1}")
            break
        
    
    total_training_time = time.time() - total_start_time
    print(f"\nğŸ‰ è®­ç»ƒå®Œæˆ!")
    print(f"æ€»è®­ç»ƒæ—¶é—´: {total_training_time/60:.1f} åˆ†é’Ÿ")
    print(f"æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f}")
    
    return train_losses, val_losses

def evaluate_model(model, data_loader):
    """å…¨é¢è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
    model.eval()
    predictions = []
    true_values = []
    
    with torch.no_grad():
        for sequences, labels in data_loader:
            outputs = model(sequences)
            predictions.extend(outputs.cpu().numpy().flatten())
            true_values.extend(labels.cpu().numpy().flatten())
    
    predictions = np.array(predictions)
    true_values = np.array(true_values)
    
    # è®¡ç®—å„ç§æŒ‡æ ‡
    mse = np.mean((predictions - true_values) ** 2)
    mae = np.mean(np.abs(predictions - true_values))
    r2 = 1 - np.sum((true_values - predictions) ** 2) / np.sum((true_values - np.mean(true_values)) ** 2)
    
    # å‡†ç¡®æ€§åˆ†æï¼ˆåœ¨ä¸åŒè¯¯å·®èŒƒå›´å†…çš„æ¯”ä¾‹ï¼‰
    accuracy_01 = np.mean(np.abs(predictions - true_values) <= 0.1)
    accuracy_02 = np.mean(np.abs(predictions - true_values) <= 0.2)
    accuracy_05 = np.mean(np.abs(predictions - true_values) <= 0.5)
    
    print(f"è¯„ä¼°ç»“æœ:")
    print(f"  MSE: {mse:.4f}")
    print(f"  MAE: {mae:.4f}")
    print(f"  RÂ²: {r2:.4f}")
    print(f"  Â±0.1å‡†ç¡®ç‡: {accuracy_01:.3f}")
    print(f"  Â±0.2å‡†ç¡®ç‡: {accuracy_02:.3f}")
    print(f"  Â±0.5å‡†ç¡®ç‡: {accuracy_05:.3f}")
    
    return {
        'mse': mse, 'mae': mae, 'r2': r2,
        'acc_01': accuracy_01, 'acc_02': accuracy_02, 'acc_05': accuracy_05,
        'predictions': predictions, 'true_values': true_values
    }

import json
import datetime
from datetime import datetime

def save_experiment_log(model, train_losses, val_losses, train_results, test_results, config):
    """ä¿å­˜å®éªŒè®°å½•"""
    
    # åˆ›å»ºå®éªŒæ—¥å¿—ç›®å½•
    log_dir = os.path.join(BASE_DIR, "experiment_logs")
    os.makedirs(log_dir, exist_ok=True)
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    # å®éªŒé…ç½®å’Œç»“æœ
    experiment_log = {
        'timestamp': timestamp,
        'model_config': config,
        'model_parameters': sum(p.numel() for p in model.parameters()),
        'training': {
            'num_epochs': len(train_losses),
            'final_train_loss': train_losses[-1],
            'final_val_loss': val_losses[-1],
            'best_val_loss': min(val_losses),
            'train_losses': train_losses,
            'val_losses': val_losses
        },
        'evaluation': {
            'train_results': {k: float(v) if isinstance(v, (int, float, np.number)) else str(v) 
                            for k, v in train_results.items() if k not in ['predictions', 'true_values']},
            'test_results': {k: float(v) if isinstance(v, (int, float, np.number)) else str(v) 
                           for k, v in test_results.items() if k not in ['predictions', 'true_values']}
        }
    }
    
    # ä¿å­˜æ—¥å¿—
    log_file = os.path.join(log_dir, f"experiment_{timestamp}.json")
    with open(log_file, 'w', encoding='utf-8') as f:
        json.dump(experiment_log, f, indent=2, ensure_ascii=False)
    
    print(f"å®éªŒæ—¥å¿—å·²ä¿å­˜: {log_file}")
    return log_file

def train_complete_pipeline():

    # å®éªŒé…ç½®
    config = {
        'model_type': 'SimpleLSTM',
        'input_size': 21,
        'hidden_size': 64,
        'output_size': 1,
        'num_layers': 2,
        'batch_size': 16,
        'learning_rate': 0.001,
        'num_epochs': 50,
        'early_stop_patience': 10
    }
    print(f"å®éªŒé…ç½®: {config}")
    print("="*80)

    # åˆ›å»ºæ•°æ®é›†
    print("åˆ›å»ºæ•°æ®é›†...")
    dataset_start_time = time.time()
    train_dataset = MaichartDataset(SERIALIZED_DIR, TRAIN_DATA_PATH)
    test_dataset = MaichartDataset(SERIALIZED_DIR, TEST_DATA_PATH)
    dataset_time = time.time() - dataset_start_time
    print(f"æ•°æ®é›†åˆ›å»ºå®Œæˆ ({dataset_time:.3f}s)")
    print(f"è®­ç»ƒé›†å¤§å°: {len(train_dataset)}, æµ‹è¯•é›†å¤§å°: {len(test_dataset)}")

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    print("\nåˆ›å»ºæ•°æ®åŠ è½½å™¨...")
    loader_start_time = time.time()

    # ä½¿ç”¨åˆ†æ¡¶é‡‡æ ·å™¨åˆ›å»ºè®­ç»ƒæ•°æ®åŠ è½½å™¨
    # æ³¨æ„ï¼šä½¿ç”¨ batch_sampler æ—¶ï¼ŒDataLoaderçš„ batch_size, shuffle, sampler, drop_last å‚æ•°å¿…é¡»ä¸ºé»˜è®¤å€¼
    # train_sampler = LevelIndexBucketSampler(
    #     train_dataset,
    #     batch_size=config['batch_size'],
    #     shuffle=True,
    #     drop_last=True # åœ¨è®­ç»ƒæ—¶ä¸¢å¼ƒä¸å®Œæ•´çš„batché€šå¸¸æ˜¯å¥½çš„å®è·µ
    # )
    # train_loader = DataLoader(
    #     train_dataset,
    #     batch_sampler=train_sampler,
    #     collate_fn=collate_fn,
    #     num_workers=0   
    # )

    train_loader = DataLoader(
    train_dataset,
    batch_size=config['batch_size'],
    shuffle=True,
    collate_fn=collate_fn,
    num_workers=0
    )

    # æµ‹è¯•åŠ è½½å™¨ä¸éœ€è¦åˆ†æ¡¶æˆ–æ‰“ä¹±
    test_loader = DataLoader(
        test_dataset,
        batch_size=config['batch_size'],
        shuffle=False,
        collate_fn=collate_fn,
        num_workers=0
    )
    loader_time = time.time() - loader_start_time
    print(f"æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆ ({loader_time:.3f}s)")
    print(f"æµ‹è¯•æ‰¹æ¬¡æ•°: {len(test_loader)}")

    # åˆ›å»ºæ¨¡å‹
    print("\nåˆ›å»ºæ¨¡å‹...")
    model_start_time = time.time()
    model = SimpleLSTM(
        config['input_size'], 
        config['hidden_size'], 
        config['output_size'], 
        config['num_layers']
    )
    model_time = time.time() - model_start_time
    param_count = sum(p.numel() for p in model.parameters())
    print(f"æ¨¡å‹åˆ›å»ºå®Œæˆ ({model_time:.3f}s)")
    print(f"æ¨¡å‹å‚æ•°æ•°é‡: {param_count:,}")
    print(f"æ¨¡å‹è®¾å¤‡: {next(model.parameters()).device}")

    # è®­ç»ƒæ¨¡å‹
    train_losses, val_losses = train_model(
        model, train_loader, test_loader, 
        num_epochs=config['num_epochs'], 
        learning_rate=config['learning_rate']
    )
    
    # åŠ è½½æœ€ä½³æ¨¡å‹å¹¶è¯„ä¼°
    model.load_state_dict(torch.load(os.path.join(MODEL_DIR, "best_model.pth")))
    print("\n=== è®­ç»ƒé›†è¯„ä¼° ===")
    train_results = evaluate_model(model, train_loader)
    print("\n=== æµ‹è¯•é›†è¯„ä¼° ===")
    test_results = evaluate_model(model, test_loader)

    # ä¿å­˜å®éªŒè®°å½•
    log_file = save_experiment_log(model, train_losses, val_losses, train_results, test_results, config)


    return model, train_losses, val_losses, train_results, test_results
