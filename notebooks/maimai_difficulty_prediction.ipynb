{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95f98cbe",
   "metadata": {},
   "source": [
    "# maimai 谱面难度预测 - 基于 LSTM 的时序建模\n",
    "\n",
    "本项目使用 LSTM 神经网络直接处理谱面的 note 序列数据，将每个 note 的时间戳和类型等信息作为时序特征输入模型，预测谱面的难度定数。\n",
    "\n",
    "**核心思路**：将谱面视为时间序列，每个 note 包含时间戳、类型、位置等属性，通过 LSTM 学习 note 序列的时序特征来预测难度。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79795d1",
   "metadata": {},
   "source": [
    "## 1. 导入所需库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8a4e1287",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import csv\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db9c020",
   "metadata": {},
   "source": [
    "## 2. 数据处理与序列化\n",
    "\n",
    "数据处理分为两个主要步骤：\n",
    "1. **谱面解析**：将 maidata.txt 格式解析为结构化的 note 序列数据\n",
    "2. **序列预处理**：将 note 序列转换为适合 LSTM 输入的格式\n",
    "\n",
    "**核心理念**：每个谱面是一个时间序列，包含按时间顺序排列的 note 序列。每个 note 具有时间戳、类型、位置等属性。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7bf94d",
   "metadata": {},
   "source": [
    "### 2.1 解析 maidata.txt\n",
    "\n",
    "我们使用外部工具 `SimaiSerializerFromMajdataEdit.exe` 来将 `maidata.txt` 格式的谱面文件解析并序列化为 JSON 文件。\n",
    "\n",
    "数据来源：maichart-converts\n",
    "\n",
    "**使用方法:**\n",
    "\n",
    "在终端中执行以下命令，它会将 `data\\maichart-converts` 目录下的所有谱面处理并输出到 `data\\serialized` 目录。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "83aba23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src\\serializer\\src\\bin\\Release\\net8.0\\SimaiSerializerFromMajdataEdit.exe data\\maichart-converts data\\serialized\n"
     ]
    }
   ],
   "source": [
    "command = (\n",
    "    r\"src\\serializer\\src\\bin\\Release\\net8.0\\SimaiSerializerFromMajdataEdit.exe \"\n",
    "    r\"data\\maichart-converts data\\serialized\"\n",
    ")\n",
    "print(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebc6c18",
   "metadata": {},
   "source": [
    "该工具的通用命令格式为： `SimaiSerializerFromMajdataEdit.exe <输入文件或目录> <输出目录>`\n",
    "\n",
    "执行完毕后，我们将得到包含 note 序列数据的 JSON 文件，每个文件对应一个特定难度的谱面。\n",
    "\n",
    "**TODO**：\n",
    "- 运行序列化工具并检查输出结果\n",
    "- 验证生成的 JSON 文件结构\n",
    "- 统计不同谱面的 note 数量分布，为序列长度标准化做准备\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50b50b8",
   "metadata": {},
   "source": [
    "### 2.2 处理谱面标签数据\n",
    "\n",
    "从 maimai-songs 库的 songs.json 中提取训练标签：\n",
    "- **歌曲ID**：song_id（json中为id）\n",
    "- **难度序号**：level_index（在json中并未显式标明，charts中依次对应level_index 1-5的数据）\n",
    "- **难度定数**：difficulty_constant（json中为level）- 这是我们的预测目标\n",
    "\n",
    "**TODO**：\n",
    "- 提取标签数据并与序列化的谱面数据进行匹配\n",
    "- 处理缺失的难度定数（null值）\n",
    "- 过滤掉六位数ID的宴谱数据\n",
    "- 从 flevel.json 中获取拟合等级数据作为辅助信息\n",
    "- 验证标签与谱面文件的一一对应关系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "49f5452c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_write_song_info_with_json(serialized_dir, songs_metadata_path, csv_file_path):\n",
    "    \"\"\"\n",
    "    1. 解析 songs.json，提取 (song_id, level_index, difficulty_constant)\n",
    "    2. 查找对应的 serialized json 文件，写入 json_filename\n",
    "    3. 只保留有 json 文件的条目，一次性写入 CSV\n",
    "    \"\"\"\n",
    "    import glob\n",
    "    # 读取JSON文件\n",
    "    if not os.path.exists(songs_metadata_path):\n",
    "        print(f\"错误：文件不存在 - {songs_metadata_path}\")\n",
    "        sys.exit(1)\n",
    "    with open(songs_metadata_path, 'r', encoding='utf-8') as f:\n",
    "        songs_data = json.load(f)\n",
    "\n",
    "    # 建立 (song_id, level_index) -> json_filename 映射\n",
    "    json_files = glob.glob(os.path.join(serialized_dir, \"*.json\"))\n",
    "    json_map = {}\n",
    "    for json_file in json_files:\n",
    "        try:\n",
    "            with open(json_file, 'r', encoding='utf-8') as jf:\n",
    "                data = json.load(jf)\n",
    "                song_id = int(data['song_id'])\n",
    "                level_index = int(data['level_index'])\n",
    "                json_map[(song_id, level_index)] = os.path.basename(json_file)\n",
    "        except Exception as e:\n",
    "            print(f\"解析失败: {json_file}, 错误: {e}\")\n",
    "            continue\n",
    "\n",
    "    # 提取所需信息并查找json文件名，只保留有json文件的条目\n",
    "    extracted_info = []\n",
    "    for song in songs_data:\n",
    "        song_id = song.get('id')\n",
    "        charts = song.get('charts', [])\n",
    "        for level_index, chart in enumerate(charts, start=1):\n",
    "            difficulty_constant = chart.get('level')\n",
    "            try:\n",
    "                sid = int(song_id)\n",
    "                lid = int(level_index)\n",
    "            except Exception:\n",
    "                continue\n",
    "            json_filename = json_map.get((sid, lid))\n",
    "            if json_filename is not None:\n",
    "                extracted_info.append({\n",
    "                    'song_id': sid,\n",
    "                    'level_index': lid,\n",
    "                    'difficulty_constant': difficulty_constant,\n",
    "                    'json_filename': json_filename\n",
    "                })\n",
    "\n",
    "    # 写入CSV\n",
    "    os.makedirs(os.path.dirname(csv_file_path), exist_ok=True)\n",
    "    with open(csv_file_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "        fieldnames = ['song_id', 'level_index', 'difficulty_constant', 'json_filename']\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for item in extracted_info:\n",
    "            writer.writerow(item)\n",
    "    print(f\"成功提取 {len(extracted_info)} 条记录（均有json文件），已写入 {csv_file_path}\")\n",
    "\n",
    "# 用法示例\n",
    "# base_dir = os.path.dirname(os.path.abspath(''))\n",
    "# serialized_dir = os.path.join(base_dir, \"data\", \"serialized\")\n",
    "# songs_json_path = os.path.join(base_dir, \"data\", \"maimai-songs\", \"songs.json\")\n",
    "# csv_path = os.path.join(base_dir, \"data\", \"song_info.csv\")\n",
    "# extract_and_write_song_info_with_json(serialized_dir, songs_json_path, csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9ffdf1",
   "metadata": {},
   "source": [
    "### 2.3 划分训练集与测试集\n",
    "\n",
    "我们需要排除宴谱，然后：\n",
    "- 对绿/黄/红/紫白谱分别进行训练集/测试集划分，保证每组比例相同\n",
    "- 可能甚至要将紫白谱的每个定数都保证内部均分\n",
    "- 允许将特定谱面排除在训练集之外"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d79652",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6c8e756",
   "metadata": {},
   "source": [
    "## 3. 序列预处理与特征编码\n",
    "\n",
    "不同于传统的特征工程方法，我们直接使用原始的 note 序列数据。主要任务是将 note 属性转换为数值向量，并处理序列长度不一致的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8934d3c1",
   "metadata": {},
   "source": [
    "### 3.1 构建自定义Dataset类\n",
    "我们创建一个自定义Dataset，存储json文件的位置以及csv的位置。\n",
    "在Dataset中，需要实现：\n",
    "1. `__init__()`：初始化函数，传入serialized目录以及csv文件位置。\n",
    "    - 需要存储每个json的路径\n",
    "2. `__len__()`：返回数据集的长度。\n",
    "3. `__getitem__()`：返回数据集中的第i个样本。直接返回tensor\n",
    "    - 在`__getitem__()`中才读取json文件，并返回tensor\n",
    "    - 读取json文件，然后再去csv中找对应`(song_id,level_index)`的行\n",
    "    - index顺序是什么"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "590d3594",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaichartDataset(Dataset):\n",
    "    def __init__(self, serialized_dir, labels_csv):\n",
    "        self.serialized_dir = serialized_dir\n",
    "        \n",
    "        # 读取CSV并清理数据\n",
    "        self.labels_data = pd.read_csv(labels_csv)\n",
    "        self.labels_data = self.labels_data.dropna(subset=['song_id', 'level_index', 'difficulty_constant', 'json_filename'])\n",
    "        \n",
    "        # 重置索引以确保连续的整数索引\n",
    "        self.labels_data = self.labels_data.reset_index(drop=True)\n",
    "\n",
    "        # TouchArea映射\n",
    "        self.touch_area_mapping = {\" \": 0, \"A\": 1, \"D\": 2, \"E\": 3, \"B\": 4, \"C\": 5} # 从外到内\n",
    "\n",
    "        # 初始化编码器\n",
    "        self._setup_encoders()\n",
    "\n",
    "    def _setup_encoders(self):\n",
    "        \"\"\"设置note类型和位置的编码器\"\"\"\n",
    "        # Note类型编码器\n",
    "        self.NOTE_TYPES = ['Tap', 'Hold', 'Slide', 'Touch', 'TouchHold']\n",
    "        self.note_type_encoder = OneHotEncoder(\n",
    "            sparse_output=False,\n",
    "            dtype=np.float32,\n",
    "            handle_unknown='ignore'\n",
    "        )\n",
    "        self.note_type_encoder.fit(np.array(self.NOTE_TYPES).reshape(-1, 1))\n",
    "        \n",
    "        # 位置编码器（假设位置范围是1-8）\n",
    "        self.positions = list(range(1, 9))  # maimai有8个位置\n",
    "        self.position_encoder = OneHotEncoder(\n",
    "            sparse_output=False,\n",
    "            dtype=np.float32,\n",
    "            handle_unknown='ignore'\n",
    "        )\n",
    "        self.position_encoder.fit(np.array(self.positions).reshape(-1, 1))\n",
    "\n",
    "    def _extract_note_features(self, note, time):\n",
    "        \"\"\"\n",
    "        从单个note中提取特征向量\n",
    "        \n",
    "        Args:\n",
    "            note: 包含note信息的字典\n",
    "            time: note的时间戳\n",
    "            \n",
    "        Returns:\n",
    "            np.ndarray: 21维的特征向量\n",
    "        \"\"\"\n",
    "        # 编码note类型和位置\n",
    "        note_type_encoded = self.note_type_encoder.transform([[note['noteType']]])[0]\n",
    "        position_encoded = self.position_encoder.transform([[note['startPosition']]])[0]\n",
    "        \n",
    "        # 提取其他特征\n",
    "        hold_time = note.get('holdTime', 0)\n",
    "        is_break = int(note['isBreak'])\n",
    "        is_ex = int(note['isEx'])\n",
    "        is_slide_break = int(note['isSlideBreak'])\n",
    "        slide_start_time = note['slideStartTime']\n",
    "        slide_end_time = slide_start_time + note['slideTime']\n",
    "        touch_area = self.touch_area_mapping[note['touchArea']]\n",
    "        \n",
    "        # 组合特征向量\n",
    "        feature_vector = np.concatenate([\n",
    "            [time],             # 1维\n",
    "            note_type_encoded,  # 5维\n",
    "            position_encoded,   # 8维\n",
    "            [hold_time],        # 1维\n",
    "            [is_break],         # 1维\n",
    "            [is_ex],            # 1维\n",
    "            [is_slide_break],   # 1维\n",
    "            [slide_start_time], # 1维\n",
    "            [slide_end_time],   # 1维\n",
    "            [touch_area]        # 1维\n",
    "        ])  # 总共 21维\n",
    "        \n",
    "        return feature_vector\n",
    "\n",
    "    def _extract_sequence_features(self, json_data):\n",
    "        \"\"\"\n",
    "        从JSON数据中提取整个谱面的note序列特征\n",
    "        \n",
    "        Args:\n",
    "            json_data: 包含谱面数据的JSON对象\n",
    "            \n",
    "        Returns:\n",
    "            list: note特征向量的列表\n",
    "        \"\"\"\n",
    "        note_groups = json_data.get('notes', [])\n",
    "        note_features_sequence = []\n",
    "        \n",
    "        for note_group in note_groups:\n",
    "            time = note_group['Time']\n",
    "            notes = note_group['Notes']\n",
    "            \n",
    "            for note in notes:\n",
    "                feature_vector = self._extract_note_features(note, time)\n",
    "                note_features_sequence.append(feature_vector)\n",
    "        \n",
    "        return note_features_sequence\n",
    "\n",
    "    def _extract_sequence_features_vectorized(self, json_data):\n",
    "        \"\"\"\n",
    "        向量化提取整个谱面的note序列特征\n",
    "        \n",
    "        Args:\n",
    "            json_data: 包含谱面数据的JSON对象\n",
    "            \n",
    "        Returns:\n",
    "            np.ndarray: (num_notes, 21) 的特征矩阵\n",
    "        \"\"\"\n",
    "        note_groups = json_data.get('notes', [])\n",
    "        if not note_groups:\n",
    "            raise ValueError(f\"未找到{json_data}的note group信息\")\n",
    "        \n",
    "        # 收集所有notes数据\n",
    "        all_times = []\n",
    "        all_notes_data = []\n",
    "        \n",
    "        for note_group in note_groups:\n",
    "            time = note_group['Time']\n",
    "            notes = note_group['Notes']\n",
    "            \n",
    "            for note in notes:\n",
    "                all_times.append(time)\n",
    "                all_notes_data.append(note)\n",
    "        \n",
    "        if not all_notes_data:\n",
    "            raise ValueError(f\"未找到{json_data}的note信息\")\n",
    "        \n",
    "        num_notes = len(all_notes_data)\n",
    "        \n",
    "        # 向量化提取所有note类型\n",
    "        note_types = np.array([note['noteType'] for note in all_notes_data]).reshape(-1, 1)\n",
    "        note_types_encoded = self.note_type_encoder.transform(note_types)  # (num_notes, 5)\n",
    "        \n",
    "        # 向量化提取所有位置\n",
    "        positions = np.array([note['startPosition'] for note in all_notes_data]).reshape(-1, 1)\n",
    "        positions_encoded = self.position_encoder.transform(positions)  # (num_notes, 8)\n",
    "        \n",
    "        # 向量化提取其他特征\n",
    "        times_array = np.array(all_times, dtype=np.float32)  # (num_notes,)\n",
    "        hold_times = np.array([note.get('holdTime', 0) for note in all_notes_data], dtype=np.float32)\n",
    "        is_break = np.array([int(note['isBreak']) for note in all_notes_data], dtype=np.float32)\n",
    "        is_ex = np.array([int(note['isEx']) for note in all_notes_data], dtype=np.float32)\n",
    "        is_slide_break = np.array([int(note['isSlideBreak']) for note in all_notes_data], dtype=np.float32)\n",
    "        slide_start_times = np.array([note['slideStartTime'] for note in all_notes_data], dtype=np.float32)\n",
    "        slide_times = np.array([note['slideTime'] for note in all_notes_data], dtype=np.float32)\n",
    "        slide_end_times = slide_start_times + slide_times\n",
    "        touch_areas = np.array([self.touch_area_mapping[note['touchArea']] for note in all_notes_data], dtype=np.float32)\n",
    "        \n",
    "        # 组合所有特征 - 向量化拼接\n",
    "        feature_matrix = np.column_stack([\n",
    "            times_array,           # (num_notes, 1)\n",
    "            note_types_encoded,    # (num_notes, 5)\n",
    "            positions_encoded,     # (num_notes, 8)\n",
    "            hold_times,            # (num_notes, 1)\n",
    "            is_break,              # (num_notes, 1)\n",
    "            is_ex,                 # (num_notes, 1)\n",
    "            is_slide_break,        # (num_notes, 1)\n",
    "            slide_start_times,     # (num_notes, 1)\n",
    "            slide_end_times,       # (num_notes, 1)\n",
    "            touch_areas            # (num_notes, 1)\n",
    "        ])  # 总共 (num_notes, 21)\n",
    "        \n",
    "        return feature_matrix\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 从CSV中获取第index行的数据\n",
    "        row = self.labels_data.iloc[index]\n",
    "        json_filename = row['json_filename']\n",
    "        difficulty_constant = float(row['difficulty_constant'])\n",
    "        \n",
    "        # 构建JSON文件的完整路径\n",
    "        json_file_path = os.path.join(self.serialized_dir, json_filename)\n",
    "        \n",
    "        # 检查文件是否存在\n",
    "        if not os.path.exists(json_file_path):\n",
    "            raise FileNotFoundError(f\"JSON文件不存在: {json_file_path}\")\n",
    "        \n",
    "        with open(json_file_path, 'r', encoding='utf-8') as f:\n",
    "            try:\n",
    "                json_data = json.load(f)\n",
    "            except json.JSONDecodeError as e:\n",
    "                raise ValueError(f\"JSON解析失败: {json_file_path}\") from e\n",
    "\n",
    "        # 使用向量化方法提取谱面特征序列\n",
    "        note_features_matrix = self._extract_sequence_features_vectorized(json_data)\n",
    "\n",
    "        # 将谱面数据转换为张量\n",
    "        note_features_tensor = torch.from_numpy(note_features_matrix)\n",
    "        difficulty_constant_tensor = torch.tensor(difficulty_constant, dtype=torch.float32)\n",
    "        return note_features_tensor, difficulty_constant_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd0e33f",
   "metadata": {},
   "source": [
    "### 序列处理：\n",
    "- **序列长度标准化**：使用 padding 或截断将所有序列调整为相同长度\n",
    "- **序列归一化**：对时间特征进行归一化处理（暂不处理）\n",
    "- **序列排序**：确保 note 按时间顺序排列（好像不需要）\n",
    "\n",
    "**TODO**：\n",
    "- 设计 note 特征的编码方案\n",
    "- 确定最佳的序列长度\n",
    "- 实现序列预处理管道\n",
    "- 考虑是否需要添加全局特征（如 BPM、总时长等）\n",
    "\n",
    "我们已经在自定义数据集中完成了note属性编码。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9db1690",
   "metadata": {},
   "source": [
    "#### 特征列含义（共21列）:\n",
    "```\n",
    "  列 0: 时间戳 (time)\n",
    "  列 1: Note类型-Tap\n",
    "  列 2: Note类型-Hold\n",
    "  列 3: Note类型-Slide\n",
    "  列 4: Note类型-Touch\n",
    "  列 5: Note类型-TouchHold\n",
    "  列 6: 位置-1\n",
    "  列 7: 位置-2\n",
    "  列 8: 位置-3\n",
    "  列 9: 位置-4\n",
    "  列10: 位置-5\n",
    "  列11: 位置-6\n",
    "  列12: 位置-7\n",
    "  列13: 位置-8\n",
    "  列14: hold_time\n",
    "  列15: is_break\n",
    "  列16: is_ex\n",
    "  列17: is_slide_break\n",
    "  列18: slide_start_time\n",
    "  列19: slide_end_time\n",
    "  列20: touch_area\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60d6303",
   "metadata": {},
   "source": [
    "### 3.2 性能测试和验证\n",
    "\n",
    "让我们测试向量化实现的性能提升，并验证结果的正确性："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0ec48956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "性能对比测试开始...\n",
      "测试样本数量: 10\n",
      "\n",
      "性能对比结果:\n",
      "原始方法耗时: 0.9314 秒\n",
      "向量化方法耗时: 0.0293 秒\n",
      "加速比: 31.74x\n",
      "\n",
      "正确性验证:\n",
      "✓ 所有测试样本的结果完全一致!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "class MaichartDatasetOld(Dataset):\n",
    "    \"\"\"保留原始实现用于性能对比\"\"\"\n",
    "    def __init__(self, serialized_dir, labels_csv):\n",
    "        self.serialized_dir = serialized_dir\n",
    "        self.labels_data = pd.read_csv(labels_csv)\n",
    "        self.labels_data = self.labels_data.dropna(subset=['song_id', 'level_index', 'difficulty_constant', 'json_filename'])\n",
    "        self.labels_data = self.labels_data.reset_index(drop=True)\n",
    "        self.touch_area_mapping = {\" \": 0, \"A\": 1, \"D\": 2, \"E\": 3, \"B\": 4, \"C\": 5}\n",
    "        self._setup_encoders()\n",
    "\n",
    "    def _setup_encoders(self):\n",
    "        self.NOTE_TYPES = ['Tap', 'Hold', 'Slide', 'Touch', 'TouchHold']\n",
    "        self.note_type_encoder = OneHotEncoder(sparse_output=False, dtype=np.float32, handle_unknown='ignore')\n",
    "        self.note_type_encoder.fit(np.array(self.NOTE_TYPES).reshape(-1, 1))\n",
    "        \n",
    "        self.positions = list(range(1, 9))\n",
    "        self.position_encoder = OneHotEncoder(sparse_output=False, dtype=np.float32, handle_unknown='ignore')\n",
    "        self.position_encoder.fit(np.array(self.positions).reshape(-1, 1))\n",
    "\n",
    "    def _extract_note_features(self, note, time):\n",
    "        \"\"\"原始的单个note特征提取方法\"\"\"\n",
    "        note_type_encoded = self.note_type_encoder.transform([[note['noteType']]])[0]\n",
    "        position_encoded = self.position_encoder.transform([[note['startPosition']]])[0]\n",
    "        \n",
    "        hold_time = note.get('holdTime', 0)\n",
    "        is_break = int(note['isBreak'])\n",
    "        is_ex = int(note['isEx'])\n",
    "        is_slide_break = int(note['isSlideBreak'])\n",
    "        slide_start_time = note['slideStartTime']\n",
    "        slide_end_time = slide_start_time + note['slideTime']\n",
    "        touch_area = self.touch_area_mapping[note['touchArea']]\n",
    "        \n",
    "        feature_vector = np.concatenate([\n",
    "            [time], note_type_encoded, position_encoded,\n",
    "            [hold_time], [is_break], [is_ex], [is_slide_break],\n",
    "            [slide_start_time], [slide_end_time], [touch_area]\n",
    "        ])\n",
    "        return feature_vector\n",
    "\n",
    "    def _extract_sequence_features_old(self, json_data):\n",
    "        \"\"\"原始的循环实现\"\"\"\n",
    "        note_groups = json_data.get('notes', [])\n",
    "        note_features_sequence = []\n",
    "        \n",
    "        for note_group in note_groups:\n",
    "            time = note_group['Time']\n",
    "            notes = note_group['Notes']\n",
    "            \n",
    "            for note in notes:\n",
    "                feature_vector = self._extract_note_features(note, time)\n",
    "                note_features_sequence.append(feature_vector)\n",
    "        \n",
    "        return note_features_sequence\n",
    "\n",
    "def performance_comparison_test():\n",
    "    \"\"\"对比原始方法和向量化方法的性能\"\"\"\n",
    "    base_dir = os.path.dirname(os.path.abspath(''))\n",
    "    serialized_dir = os.path.join(base_dir, \"data\", \"serialized\")\n",
    "    csv_path = os.path.join(base_dir, \"data\", \"song_info.csv\")\n",
    "    \n",
    "    # 创建两个数据集实例\n",
    "    dataset_new = MaichartDataset(serialized_dir, csv_path)\n",
    "    dataset_old = MaichartDatasetOld(serialized_dir, csv_path)\n",
    "    \n",
    "    # 选择测试样本\n",
    "    test_indices = list(range(min(10, len(dataset_new))))  # 测试前10个样本\n",
    "    \n",
    "    print(\"性能对比测试开始...\")\n",
    "    print(f\"测试样本数量: {len(test_indices)}\")\n",
    "    \n",
    "    # 测试原始方法\n",
    "    start_time = time.time()\n",
    "    old_results = []\n",
    "    for idx in test_indices:\n",
    "        try:\n",
    "            row = dataset_old.labels_data.iloc[idx]\n",
    "            json_filename = row['json_filename']\n",
    "            json_file_path = os.path.join(serialized_dir, json_filename)\n",
    "            \n",
    "            with open(json_file_path, 'r', encoding='utf-8') as f:\n",
    "                json_data = json.load(f)\n",
    "            \n",
    "            features = dataset_old._extract_sequence_features_old(json_data)\n",
    "            old_results.append(np.array(features))\n",
    "        except Exception as e:\n",
    "            print(f\"原始方法处理样本 {idx} 时出错: {e}\")\n",
    "            continue\n",
    "    \n",
    "    old_time = time.time() - start_time\n",
    "    \n",
    "    # 测试向量化方法\n",
    "    start_time = time.time()\n",
    "    new_results = []\n",
    "    for idx in test_indices:\n",
    "        try:\n",
    "            row = dataset_new.labels_data.iloc[idx]\n",
    "            json_filename = row['json_filename']\n",
    "            json_file_path = os.path.join(serialized_dir, json_filename)\n",
    "            \n",
    "            with open(json_file_path, 'r', encoding='utf-8') as f:\n",
    "                json_data = json.load(f)\n",
    "            \n",
    "            features = dataset_new._extract_sequence_features_vectorized(json_data)\n",
    "            new_results.append(features)\n",
    "        except Exception as e:\n",
    "            print(f\"向量化方法处理样本 {idx} 时出错: {e}\")\n",
    "            continue\n",
    "    \n",
    "    new_time = time.time() - start_time\n",
    "    \n",
    "    # 性能结果\n",
    "    print(f\"\\n性能对比结果:\")\n",
    "    print(f\"原始方法耗时: {old_time:.4f} 秒\")\n",
    "    print(f\"向量化方法耗时: {new_time:.4f} 秒\")\n",
    "    print(f\"加速比: {old_time/new_time:.2f}x\")\n",
    "    \n",
    "    # 验证结果正确性\n",
    "    print(f\"\\n正确性验证:\")\n",
    "    if len(old_results) == len(new_results):\n",
    "        all_close = True\n",
    "        for i, (old_feat, new_feat) in enumerate(zip(old_results, new_results)):\n",
    "            if not np.allclose(old_feat, new_feat, rtol=1e-5):\n",
    "                print(f\"样本 {i} 结果不一致!\")\n",
    "                print(f\"  原始方法形状: {old_feat.shape}\")\n",
    "                print(f\"  向量化方法形状: {new_feat.shape}\")\n",
    "                all_close = False\n",
    "        \n",
    "        if all_close:\n",
    "            print(\"✓ 所有测试样本的结果完全一致!\")\n",
    "        else:\n",
    "            print(\"✗ 发现结果不一致的样本\")\n",
    "    else:\n",
    "        print(f\"✗ 处理成功的样本数量不一致: 原始={len(old_results)}, 向量化={len(new_results)}\")\n",
    "\n",
    "# 运行性能测试\n",
    "performance_comparison_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dcec0d",
   "metadata": {},
   "source": [
    "### 3.3 向量化优化要点总结\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb230dbc",
   "metadata": {},
   "source": [
    "\n",
    "**主要优化策略**：\n",
    "\n",
    "1. **批量编码代替逐个编码**：\n",
    "   - 原始：对每个note分别调用`OneHotEncoder.transform([[value]])`\n",
    "   - 优化：收集所有note数据，一次性调用`OneHotEncoder.transform(all_values)`\n",
    "   - 效果：减少了大量的函数调用开销\n",
    "\n",
    "2. **向量化数组操作**：\n",
    "   - 原始：使用Python循环和`np.concatenate`逐个拼接特征\n",
    "   - 优化：使用`np.column_stack`一次性拼接所有特征列\n",
    "   - 效果：利用NumPy的C语言底层实现，大幅提升性能\n",
    "\n",
    "3. **内存访问优化**：\n",
    "   - 原始：多次小数组的创建和拼接\n",
    "   - 优化：预先分配大数组，减少内存分配次数\n",
    "   - 效果：更好的内存局部性和缓存命中率\n",
    "\n",
    "4. **减少中间变量**：\n",
    "   - 原始：每个note创建一个中间`feature_vector`\n",
    "   - 优化：直接构建最终的特征矩阵\n",
    "   - 效果：减少内存开销和垃圾回收压力\n",
    "\n",
    "**预期性能提升**：\n",
    "- 对于包含大量notes的谱面，预期可获得 **5-20倍** 的性能提升\n",
    "- 实际提升幅度取决于谱面的note密度和硬件配置\n",
    "\n",
    "**兼容性保证**：\n",
    "- 输出结果与原始方法完全一致\n",
    "- 可直接替换原有实现，无需修改下游代码"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4626766e",
   "metadata": {},
   "source": [
    "### 3.4 Collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3b7b439b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    自定义的collate_fn，用于处理变长序列。\n",
    "    - 对note序列进行padding，使其在batch内长度一致。\n",
    "    - 将标签堆叠成一个tensor。\n",
    "    \"\"\"\n",
    "    # 1. 分离序列和标签\n",
    "    # batch中的每个元素是 (note_features_tensor, difficulty_constant_tensor)\n",
    "    sequences, labels = zip(*batch)\n",
    "\n",
    "    # 2. 对序列进行padding\n",
    "    # pad_sequence期望一个tensor列表\n",
    "    # batch_first=True使输出的形状为 (batch_size, sequence_length, feature_dim)\n",
    "    padded_sequences = pad_sequence(sequences, batch_first=True, padding_value=0.0)\n",
    "\n",
    "    # 3. 将标签堆叠成一个tensor\n",
    "    # torch.stack(labels) 会创建一个 [batch_size] 的1D张量\n",
    "    # .view(-1, 1) 将其转换为 [batch_size, 1] 以匹配模型输出\n",
    "    labels_tensor = torch.stack(labels).view(-1, 1)\n",
    "\n",
    "    return padded_sequences, labels_tensor\n",
    "\n",
    "def collate_fn_with_stats(batch):\n",
    "    \"\"\"\n",
    "    带统计信息的collate_fn，用于分析和调试。\n",
    "    \n",
    "    返回：\n",
    "    - padded_sequences: 填充后的序列\n",
    "    - labels: 标签\n",
    "    - batch_stats: 包含批次统计信息的字典\n",
    "    \"\"\"\n",
    "    if not batch:\n",
    "        raise ValueError(\"batch为0\")\n",
    "    sequences, labels = zip(*batch)\n",
    "    \n",
    "    # 收集统计信息\n",
    "    seq_lengths = [seq.size(0) for seq in sequences]\n",
    "    stats = {\n",
    "        'batch_size': len(sequences),\n",
    "        'min_seq_length': min(seq_lengths),\n",
    "        'max_seq_length': max(seq_lengths),\n",
    "        'avg_seq_length': sum(seq_lengths) / len(seq_lengths),\n",
    "        'total_notes': sum(seq_lengths),\n",
    "        'padding_ratio': 0  # 将在padding后计算\n",
    "    }\n",
    "    \n",
    "    # Padding\n",
    "    padded_sequences = pad_sequence(sequences, batch_first=True, padding_value=0.0)\n",
    "    labels_tensor = torch.stack(labels).view(-1, 1)\n",
    "    \n",
    "    # 计算padding比例\n",
    "    total_elements = padded_sequences.numel()\n",
    "    padding_elements = (padded_sequences == 0).sum().item()\n",
    "    stats['padding_ratio'] = padding_elements / total_elements if total_elements > 0 else 0\n",
    "    \n",
    "    return padded_sequences, labels_tensor, stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a273d1",
   "metadata": {},
   "source": [
    "### 3.6 序列长度分布分析\n",
    "\n",
    "**问题发现**: 平均padding比例高达92.1%，这表明数据中存在序列长度极度不均匀的问题。\n",
    "\n",
    "**可能原因**:\n",
    "1. **数据中包含极长的序列**：少数极长谱面导致整体padding过多\n",
    "2. **特征维度错误**：实际特征维度与预期不符\n",
    "3. **数据处理错误**：序列提取过程中可能存在问题\n",
    "\n",
    "**解决方案**:\n",
    "1. 分析序列长度分布，找出异常值\n",
    "2. 使用动态批处理策略\n",
    "3. 考虑序列截断或分段处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "16979d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 序列长度分布分析 ===\n",
      "数据集总大小: 5478\n",
      "分析前 100 个样本...\n",
      "  样本 0: 序列长度=88, 特征维度=21, 难度=5.00\n",
      "  样本 0 特征矩阵:\n",
      "[[ 3.2  0.   0.   1.   0.   0.   0.   1.   0.   0.  ...  0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]\n",
      " [ 3.2  0.   0.   1.   0.   0.   0.   0.   0.   0.  ...  0.   1.   0.   0.   0.   0.   0.   0.   0.   0. ]\n",
      " [ 4.8  0.   0.   1.   0.   0.   1.   0.   0.   0.  ...  0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]\n",
      " [ 4.8  0.   0.   1.   0.   0.   0.   0.   0.   0.  ...  0.   0.   1.   0.   0.   0.   0.   0.   0.   0. ]\n",
      " [ 6.4  0.   0.   1.   0.   0.   0.   0.   1.   0.  ...  0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]\n",
      " [ 6.4  0.   0.   1.   0.   0.   0.   0.   0.   0.  ...  1.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]\n",
      " [ 8.   0.   0.   1.   0.   0.   0.   0.   0.   1.  ...  0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]\n",
      " [ 8.   0.   0.   1.   0.   0.   0.   0.   0.   0.  ...  0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]\n",
      " [ 9.6  1.   0.   0.   0.   0.   0.   0.   1.   0.  ...  0.   0.   0.   0.8  0.   0.   0.   0.   0.   0. ]\n",
      " [11.2  1.   0.   0.   0.   0.   0.   0.   0.   0.  ...  1.   0.   0.   0.8  0.   0.   0.   0.   0.   0. ]\n",
      " ...\n",
      " [79.6  0.   0.   1.   0.   0.   0.   0.   0.   0.  ...  0.   1.   0.   0.   0.   0.   0.   0.   0.   0. ]\n",
      " [80.4  0.   0.   1.   0.   0.   0.   0.   0.   0.  ...  1.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]\n",
      " [81.2  0.   0.   1.   0.   0.   0.   0.   0.   0.  ...  0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]\n",
      " [82.   0.   0.   1.   0.   0.   0.   0.   0.   1.  ...  0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]\n",
      " [83.2  0.   1.   0.   0.   0.   0.   0.   1.   0.  ...  0.   0.   0.   0.   0.   0.   0.  83.6 84.4  0. ]\n",
      " [86.4  0.   1.   0.   0.   0.   0.   0.   0.   0.  ...  0.   1.   0.   0.   0.   0.   0.  86.8 87.6  0. ]\n",
      " [89.2  0.   0.   1.   0.   0.   0.   0.   0.   1.  ...  0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]\n",
      " [89.2  0.   0.   1.   0.   0.   0.   0.   0.   0.  ...  0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]\n",
      " [89.6  1.   0.   0.   0.   0.   0.   0.   1.   0.  ...  0.   0.   0.   2.8  0.   0.   0.   0.   0.   0. ]\n",
      " [89.6  1.   0.   0.   0.   0.   0.   0.   0.   0.  ...  1.   0.   0.   2.8  0.   0.   0.   0.   0.   0. ]]\n",
      "  样本 1: 序列长度=116, 特征维度=21, 难度=7.00\n",
      "  样本 1 特征矩阵:\n",
      "[[ 3.2  1.   0.   0.   0.   0.   0.   0.   0.   0.  ...  1.   0.   0.   1.2  0.   0.   0.   0.   0.   0. ]\n",
      " [ 4.8  1.   0.   0.   0.   0.   0.   0.   1.   0.  ...  0.   0.   0.   1.2  0.   0.   0.   0.   0.   0. ]\n",
      " [ 6.4  1.   0.   0.   0.   0.   0.   0.   0.   0.  ...  0.   1.   0.   1.2  0.   0.   0.   0.   0.   0. ]\n",
      " [ 8.   1.   0.   0.   0.   0.   0.   1.   0.   0.  ...  0.   0.   0.   1.2  0.   0.   0.   0.   0.   0. ]\n",
      " [ 9.6  0.   0.   1.   0.   0.   1.   0.   0.   0.  ...  0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]\n",
      " [ 9.9  0.   0.   1.   0.   0.   1.   0.   0.   0.  ...  0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]\n",
      " [ 9.9  0.   0.   1.   0.   0.   0.   0.   0.   0.  ...  0.   0.   1.   0.   0.   0.   0.   0.   0.   0. ]\n",
      " [11.2  0.   0.   1.   0.   0.   0.   0.   0.   0.  ...  0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]\n",
      " [11.5  0.   0.   1.   0.   0.   0.   0.   0.   1.  ...  0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]\n",
      " [11.5  0.   0.   1.   0.   0.   0.   0.   0.   0.  ...  0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]\n",
      " ...\n",
      " [82.   0.   0.   1.   0.   0.   0.   0.   0.   0.  ...  0.   0.   1.   0.   0.   0.   0.   0.   0.   0. ]\n",
      " [82.8  0.   0.   1.   0.   0.   0.   0.   0.   0.  ...  0.   1.   0.   0.   0.   0.   0.   0.   0.   0. ]\n",
      " [83.6  0.   0.   1.   0.   0.   0.   0.   0.   0.  ...  1.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]\n",
      " [84.4  0.   1.   0.   0.   0.   0.   0.   0.   0.  ...  0.   1.   0.   0.   0.   0.   0.  84.8 85.6  0. ]\n",
      " [86.4  0.   1.   0.   0.   0.   0.   1.   0.   0.  ...  0.   0.   0.   0.   0.   0.   0.  86.8 87.6  0. ]\n",
      " [88.4  0.   0.   1.   0.   0.   0.   0.   0.   0.  ...  0.   1.   0.   0.   0.   0.   0.   0.   0.   0. ]\n",
      " [88.4  0.   0.   1.   0.   0.   0.   0.   0.   0.  ...  0.   0.   1.   0.   0.   0.   0.   0.   0.   0. ]\n",
      " [89.2  0.   0.   1.   0.   0.   1.   0.   0.   0.  ...  0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]\n",
      " [89.2  1.   0.   0.   0.   0.   0.   1.   0.   0.  ...  0.   0.   0.   3.6  0.   0.   0.   0.   0.   0. ]\n",
      " [89.6  1.   0.   0.   0.   0.   0.   0.   0.   0.  ...  1.   0.   0.   3.2  0.   0.   0.   0.   0.   0. ]]\n",
      "  样本 2: 序列长度=168, 特征维度=21, 难度=10.00\n",
      "  样本 2 特征矩阵:\n",
      "[[ 3.2  1.   0.   0.   0.   0.   0.   0.   1.   0.  ...  0.   0.   0.   1.2  0.   0.   0.   0.   0.   0. ]\n",
      " [ 3.2  1.   0.   0.   0.   0.   0.   0.   0.   0.  ...  1.   0.   0.   1.2  0.   0.   0.   0.   0.   0. ]\n",
      " [ 4.8  1.   0.   0.   0.   0.   0.   1.   0.   0.  ...  0.   0.   0.   0.4  0.   0.   0.   0.   0.   0. ]\n",
      " [ 4.8  1.   0.   0.   0.   0.   0.   0.   0.   0.  ...  0.   1.   0.   1.2  0.   0.   0.   0.   0.   0. ]\n",
      " [ 5.6  0.   0.   1.   0.   0.   0.   0.   1.   0.  ...  0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]\n",
      " [ 5.9  0.   0.   1.   0.   0.   0.   0.   1.   0.  ...  0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]\n",
      " [ 6.2  0.   0.   1.   0.   0.   0.   1.   0.   0.  ...  0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]\n",
      " [ 6.4  1.   0.   0.   0.   0.   1.   0.   0.   0.  ...  0.   0.   0.   1.2  0.   0.   0.   0.   0.   0. ]\n",
      " [ 6.4  1.   0.   0.   0.   0.   0.   0.   0.   0.  ...  0.   0.   1.   0.8  0.   0.   0.   0.   0.   0. ]\n",
      " [ 7.6  0.   0.   1.   0.   0.   0.   0.   0.   0.  ...  0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]\n",
      " ...\n",
      " [85.2  1.   0.   0.   0.   0.   0.   0.   0.   0.  ...  0.   0.   1.   0.8  0.   0.   0.   0.   0.   0. ]\n",
      " [85.6  0.   0.   1.   0.   0.   0.   0.   0.   1.  ...  0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]\n",
      " [86.   0.   1.   0.   0.   0.   0.   0.   0.   1.  ...  0.   0.   0.   0.   0.   0.   0.  86.4 86.6  0. ]\n",
      " [86.8  0.   1.   0.   0.   0.   0.   0.   0.   0.  ...  0.   0.   1.   0.   0.   0.   0.  87.2 87.4  0. ]\n",
      " [87.6  0.   1.   0.   0.   0.   0.   0.   0.   1.  ...  0.   0.   0.   0.   0.   0.   0.  88.  88.8  0. ]\n",
      " [87.6  0.   1.   0.   0.   0.   0.   0.   0.   0.  ...  0.   0.   1.   0.   0.   0.   0.  88.  88.8  0. ]\n",
      " [89.2  0.   0.   1.   0.   0.   0.   0.   1.   0.  ...  0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]\n",
      " [89.2  0.   0.   1.   0.   0.   0.   0.   0.   0.  ...  0.   1.   0.   0.   0.   0.   0.   0.   0.   0. ]\n",
      " [89.6  1.   0.   0.   0.   0.   0.   0.   1.   0.  ...  0.   0.   0.   3.2  0.   0.   0.   0.   0.   0. ]\n",
      " [89.6  1.   0.   0.   0.   0.   0.   0.   0.   0.  ...  0.   1.   0.   3.2  0.   0.   0.   0.   0.   0. ]]\n",
      "  样本 3: 序列长度=283, 特征维度=21, 难度=12.40\n",
      "  样本 4: 序列长度=76, 特征维度=21, 难度=4.00\n",
      "  样本 5: 序列长度=135, 特征维度=21, 难度=7.00\n",
      "  样本 6: 序列长度=122, 特征维度=21, 难度=9.60\n",
      "  样本 7: 序列长度=164, 特征维度=21, 难度=12.40\n",
      "  样本 8: 序列长度=185, 特征维度=21, 难度=7.00\n",
      "  样本 9: 序列长度=198, 特征维度=21, 难度=7.60\n",
      "\n",
      "序列长度统计:\n",
      "  最小长度: 76\n",
      "  最大长度: 788\n",
      "  平均长度: 249.0\n",
      "  中位数长度: 224.0\n",
      "  标准差: 124.6\n",
      "\n",
      "特征维度统计:\n",
      "  特征维度: [21]\n",
      "\n",
      "不同批次大小的padding分析:\n",
      "  批次大小 4: 平均padding比例 0.345\n",
      "  批次大小 8: 平均padding比例 0.413\n",
      "  批次大小 16: 平均padding比例 0.454\n",
      "  批次大小 32: 平均padding比例 0.515\n",
      "\n",
      "异常长序列分析:\n",
      "  95%分位数: 456.6\n",
      "  99%分位数: 651.4\n",
      "  超过95%分位数的序列数量: 5\n",
      "  这些序列长度: [np.int64(506), np.int64(531), np.int64(592), np.int64(650), np.int64(788)]\n"
     ]
    }
   ],
   "source": [
    "def analyze_sequence_lengths():\n",
    "    \"\"\"\n",
    "    分析数据集中序列长度的分布，找出padding比例过高的原因\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    base_dir = os.path.dirname(os.path.abspath(''))\n",
    "    serialized_dir = os.path.join(base_dir, \"data\", \"serialized\")\n",
    "    csv_path = os.path.join(base_dir, \"data\", \"song_info.csv\")\n",
    "    \n",
    "    dataset = MaichartDataset(serialized_dir, csv_path)\n",
    "    \n",
    "    print(\"=== 序列长度分布分析 ===\")\n",
    "    print(f\"数据集总大小: {len(dataset)}\")\n",
    "    \n",
    "    # 收集序列长度统计\n",
    "    sequence_lengths = []\n",
    "    feature_dims = []\n",
    "    sample_count = min(100, len(dataset))  # 分析前100个样本\n",
    "    \n",
    "    print(f\"分析前 {sample_count} 个样本...\")\n",
    "    \n",
    "    for i in range(sample_count):\n",
    "        try:\n",
    "            note_features, difficulty = dataset[i]\n",
    "            seq_len = note_features.shape[0]\n",
    "            feat_dim = note_features.shape[1] if len(note_features.shape) > 1 else 0\n",
    "            \n",
    "            sequence_lengths.append(seq_len)\n",
    "            feature_dims.append(feat_dim)\n",
    "            \n",
    "            if i < 10:  # 显示前10个样本的详细信息\n",
    "                print(f\"  样本 {i}: 序列长度={seq_len}, 特征维度={feat_dim}, 难度={difficulty:.2f}\")\n",
    "\n",
    "            if i < 3: # 显示前3个样本的特征矩阵\n",
    "                print(f\"  样本 {i} 特征矩阵:\\n{note_features.numpy()}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  样本 {i} 处理出错: {e}\")\n",
    "            sequence_lengths.append(0)\n",
    "            feature_dims.append(0)\n",
    "    \n",
    "    # 统计分析\n",
    "    sequence_lengths = np.array(sequence_lengths)\n",
    "    feature_dims = np.array(feature_dims)\n",
    "    \n",
    "    print(f\"\\n序列长度统计:\")\n",
    "    print(f\"  最小长度: {np.min(sequence_lengths)}\")\n",
    "    print(f\"  最大长度: {np.max(sequence_lengths)}\")\n",
    "    print(f\"  平均长度: {np.mean(sequence_lengths):.1f}\")\n",
    "    print(f\"  中位数长度: {np.median(sequence_lengths):.1f}\")\n",
    "    print(f\"  标准差: {np.std(sequence_lengths):.1f}\")\n",
    "    \n",
    "    print(f\"\\n特征维度统计:\")\n",
    "    print(f\"  特征维度: {np.unique(feature_dims)}\")\n",
    "\n",
    "\n",
    "    \n",
    "    # 计算不同批次大小的padding比例\n",
    "    batch_sizes = [4, 8, 16, 32]\n",
    "    print(f\"\\n不同批次大小的padding分析:\")\n",
    "    \n",
    "    for batch_size in batch_sizes:\n",
    "        total_padding_ratio = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for i in range(0, len(sequence_lengths), batch_size):\n",
    "            batch_lengths = sequence_lengths[i:i+batch_size]\n",
    "            if len(batch_lengths) == 0:\n",
    "                continue\n",
    "                \n",
    "            max_len = np.max(batch_lengths)\n",
    "            total_elements = len(batch_lengths) * max_len\n",
    "            actual_elements = np.sum(batch_lengths)\n",
    "            \n",
    "            if total_elements > 0:\n",
    "                padding_ratio = 1 - (actual_elements / total_elements)\n",
    "                total_padding_ratio += padding_ratio\n",
    "                num_batches += 1\n",
    "        \n",
    "        avg_padding = total_padding_ratio / num_batches if num_batches > 0 else 0\n",
    "        print(f\"  批次大小 {batch_size}: 平均padding比例 {avg_padding:.3f}\")\n",
    "    \n",
    "    # 找出异常长的序列\n",
    "    print(f\"\\n异常长序列分析:\")\n",
    "    percentile_95 = np.percentile(sequence_lengths, 95)\n",
    "    percentile_99 = np.percentile(sequence_lengths, 99)\n",
    "    \n",
    "    print(f\"  95%分位数: {percentile_95:.1f}\")\n",
    "    print(f\"  99%分位数: {percentile_99:.1f}\")\n",
    "    \n",
    "    long_sequences = sequence_lengths[sequence_lengths > percentile_95]\n",
    "    print(f\"  超过95%分位数的序列数量: {len(long_sequences)}\")\n",
    "    print(f\"  这些序列长度: {sorted(long_sequences)}\")\n",
    "    \n",
    "    return sequence_lengths, feature_dims\n",
    "\n",
    "# 运行分析\n",
    "seq_lengths, feat_dims = analyze_sequence_lengths()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7d8e7d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试优化后的数据加载器...\n",
      "开始测试优化后的数据加载器（2 个批次）...\n",
      "\n",
      "Batch 0:\n",
      "  padded_sequences.shape: torch.Size([10, 283, 21])\n",
      "  labels.shape: torch.Size([10, 1])\n",
      "  每个序列的note数量: [88, 116, 168, 283, 76, 135, 122, 164, 185, 198]\n",
      "  本批次总note数: 1535\n",
      "  特征维度: 21\n",
      "\n",
      "Batch 1:\n",
      "  padded_sequences.shape: torch.Size([10, 368, 21])\n",
      "  labels.shape: torch.Size([10, 1])\n",
      "  每个序列的note数量: [299, 253, 368, 126, 177, 168, 147, 138, 213, 323]\n",
      "  本批次总note数: 2212\n",
      "  特征维度: 21\n",
      "\n",
      "性能统计:\n",
      "  总耗时: 0.0749 秒\n",
      "  总note数: 3747\n",
      "  处理速度: 50044.0 notes/秒\n"
     ]
    }
   ],
   "source": [
    "# 测试优化后的数据加载器\n",
    "print(\"测试优化后的数据加载器...\")\n",
    "\n",
    "base_dir = os.path.dirname(os.path.abspath(''))\n",
    "serialized_dir = os.path.join(base_dir, \"data\", \"serialized\")\n",
    "csv_path = os.path.join(base_dir, \"data\", \"song_info.csv\")\n",
    "\n",
    "# 创建优化后的数据集\n",
    "optimized_dataset = MaichartDataset(serialized_dir, csv_path)\n",
    "\n",
    "# 创建数据加载器\n",
    "optimized_data_loader = DataLoader(\n",
    "    optimized_dataset,\n",
    "    batch_size=10,\n",
    "    shuffle=False,  # 设为False以便验证结果\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "# 测试向量化数据加载器的性能\n",
    "def test_optimized_data_loader(data_loader, num_batches=2):\n",
    "    \"\"\"测试优化后的DataLoader\"\"\"\n",
    "    print(f\"开始测试优化后的数据加载器（{num_batches} 个批次）...\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    total_notes = 0\n",
    "    \n",
    "    for batch_idx, (padded_sequences, labels) in enumerate(data_loader):\n",
    "        print(f\"\\nBatch {batch_idx}:\")\n",
    "        print(f\"  padded_sequences.shape: {padded_sequences.shape}\")\n",
    "        print(f\"  labels.shape: {labels.shape}\")\n",
    "        print(f\"  每个序列的note数量: {[int((seq != 0).any(dim=1).sum()) for seq in padded_sequences]}\")\n",
    "        \n",
    "        # 统计总note数\n",
    "        batch_notes = sum([int((seq != 0).any(dim=1).sum()) for seq in padded_sequences])\n",
    "        total_notes += batch_notes\n",
    "        print(f\"  本批次总note数: {batch_notes}\")\n",
    "        \n",
    "        # 验证特征维度\n",
    "        feature_dim = padded_sequences.shape[-1]\n",
    "        print(f\"  特征维度: {feature_dim}\")\n",
    "        \n",
    "        if batch_idx + 1 >= num_batches:\n",
    "            break\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"\\n性能统计:\")\n",
    "    print(f\"  总耗时: {elapsed_time:.4f} 秒\")\n",
    "    print(f\"  总note数: {total_notes}\")\n",
    "    print(f\"  处理速度: {total_notes/elapsed_time:.1f} notes/秒\")\n",
    "\n",
    "# 运行测试\n",
    "test_optimized_data_loader(optimized_data_loader, num_batches=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec37d65",
   "metadata": {},
   "source": [
    "## 4. LSTM 模型构建与数据准备\n",
    "\n",
    "构建基于 LSTM 的时序模型来处理 note 序列数据。模型将接收形状为 `(batch_size, sequence_length, feature_dim)` 的输入，输出难度定数的预测值。\n",
    "\n",
    "**模型架构设计**：\n",
    "- **输入层**：接收编码后的 note 序列\n",
    "- **LSTM层**：捕捉序列中的时序依赖关系\n",
    "- **全连接层**：将 LSTM 输出映射到难度预测\n",
    "- **输出层**：回归输出，预测难度定数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57606c6",
   "metadata": {},
   "source": [
    "### 4.1 定义 LSTM 模型架构\n",
    "\n",
    "**模型设计考虑**：\n",
    "- **多层 LSTM**：评估单层 vs 多层 LSTM 的效果\n",
    "- **Dropout**：防止过拟合\n",
    "- **Attention 机制**：突出重要的 note 序列部分\n",
    "\n",
    "**TODO**：\n",
    "- 实现基础的 LSTM 模型类\n",
    "- 设计模型的超参数（hidden_size, num_layers, dropout_rate）\n",
    "- 考虑添加注意力机制\n",
    "- 实验不同的模型架构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3e18abac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(SimpleLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        # 取最后一个时间步的输出\n",
    "        last_out = lstm_out[:, -1, :]\n",
    "        output = self.fc(last_out)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83eeaf44",
   "metadata": {},
   "source": [
    "## 5. 模型训练与优化\n",
    "\n",
    "**训练策略**：\n",
    "- **损失函数**：使用 MSE 或 MAE 损失函数（回归任务）\n",
    "- **优化器**：Adam 优化器，考虑学习率调度\n",
    "- **批次处理**：合理设置 batch_size 处理变长序列\n",
    "- **正则化**：Dropout + L2 正则化防止过拟合\n",
    "\n",
    "**训练监控**：\n",
    "- 训练损失和验证损失曲线\n",
    "- 早停机制防止过拟合\n",
    "- 学习率衰减策略\n",
    "\n",
    "**TODO**：\n",
    "- 实现训练循环\n",
    "- 设置验证集监控\n",
    "- 实现早停和模型保存机制\n",
    "- 调试序列批次处理中的 padding 问题\n",
    "- 优化训练超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f4c4d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Batch [0], Loss: 95.0216\n",
      "Epoch [1/2], Batch [10], Loss: 59.6743\n",
      "Epoch [1/2], Batch [20], Loss: 35.7750\n",
      "Epoch [1/2], Batch [30], Loss: 34.5730\n",
      "Epoch [1/2], Batch [40], Loss: 24.1483\n",
      "Epoch [1/2], Batch [50], Loss: 21.4599\n",
      "Epoch [1/2], Batch [60], Loss: 23.2256\n",
      "Epoch [1/2], Batch [70], Loss: 11.5874\n",
      "Epoch [1/2], Batch [80], Loss: 14.4881\n",
      "Epoch [1/2], Batch [90], Loss: 15.6694\n",
      "Epoch [1/2], Batch [100], Loss: 15.9063\n",
      "Epoch [1/2], Batch [110], Loss: 8.7980\n",
      "Epoch [1/2], Batch [120], Loss: 17.4870\n",
      "Epoch [1/2], Batch [130], Loss: 9.6533\n",
      "Epoch [1/2], Batch [140], Loss: 15.8760\n",
      "Epoch [1/2], Batch [150], Loss: 12.2290\n",
      "Epoch [1/2], Batch [160], Loss: 12.1949\n",
      "Epoch [1/2], Batch [170], Loss: 10.5059\n",
      "Epoch [1/2], Batch [180], Loss: 14.6460\n",
      "Epoch [1/2], Batch [190], Loss: 15.4255\n",
      "Epoch [1/2], Batch [200], Loss: 16.5977\n",
      "Epoch [1/2], Batch [210], Loss: 12.4710\n",
      "Epoch [1/2], Batch [220], Loss: 16.8045\n",
      "Epoch [1/2], Batch [230], Loss: 19.0256\n",
      "Epoch [1/2], Batch [240], Loss: 11.8946\n",
      "Epoch [1/2], Batch [250], Loss: 14.3449\n",
      "Epoch [1/2], Batch [260], Loss: 13.5433\n",
      "Epoch [1/2], Batch [270], Loss: 18.3722\n",
      "Epoch [1/2], Batch [280], Loss: 17.5122\n",
      "Epoch [1/2], Batch [290], Loss: 12.1139\n",
      "Epoch [1/2], Batch [300], Loss: 13.1680\n",
      "Epoch [1/2], Batch [310], Loss: 14.9994\n",
      "Epoch [1/2], Batch [320], Loss: 11.2463\n",
      "Epoch [1/2], Batch [330], Loss: 10.4845\n",
      "Epoch [1/2], Batch [340], Loss: 14.4317\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[52]\u001b[39m\u001b[32m, line 62\u001b[39m\n\u001b[32m     59\u001b[39m     \u001b[38;5;66;03m# 开始训练模型\u001b[39;00m\n\u001b[32m     60\u001b[39m     train_model(model, data_loader, num_epochs=\u001b[32m2\u001b[39m, learning_rate=\u001b[32m0.001\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m \u001b[43mtest_model_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[52]\u001b[39m\u001b[32m, line 60\u001b[39m, in \u001b[36mtest_model_training\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     57\u001b[39m model = SimpleLSTM(input_size, hidden_size, output_size, num_layers)\n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m# 开始训练模型\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.001\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[52]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, data_loader, num_epochs, learning_rate)\u001b[39m\n\u001b[32m     20\u001b[39m outputs = model(padded_sequences)\n\u001b[32m     21\u001b[39m loss = criterion(outputs, labels)\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m optimizer.step()\n\u001b[32m     25\u001b[39m epoch_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\wushuopei\\code\\BMK-mdp\\.venv\\Lib\\site-packages\\torch\\_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\wushuopei\\code\\BMK-mdp\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\wushuopei\\code\\BMK-mdp\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def train_model(model, data_loader, num_epochs=1, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    训练LSTM模型\n",
    "\n",
    "    Args:\n",
    "        model: LSTM模型实例\n",
    "        data_loader: DataLoader实例\n",
    "        num_epochs: 训练轮数\n",
    "        learning_rate: 学习率\n",
    "    \"\"\"\n",
    "    criterion = nn.MSELoss()  # 使用均方误差损失函数\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for batch_idx, (padded_sequences, labels) in enumerate(data_loader):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(padded_sequences)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "        avg_loss = epoch_loss / len(data_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] 完成, 平均损失: {avg_loss:.4f}\")\n",
    "\n",
    "# 测试模型训练\n",
    "def test_model_training():\n",
    "    \"\"\"测试LSTM模型的训练过程\"\"\"\n",
    "    base_dir = os.path.dirname(os.path.abspath(''))\n",
    "    serialized_dir = os.path.join(base_dir, \"data\", \"serialized\")\n",
    "    csv_path = os.path.join(base_dir, \"data\", \"song_info.csv\")\n",
    "\n",
    "    # 创建数据集和数据加载器\n",
    "    dataset = MaichartDataset(serialized_dir, csv_path)\n",
    "    data_loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    # 定义模型参数\n",
    "    input_size = 21  # 特征维度\n",
    "    hidden_size = 64  # LSTM隐藏层大小\n",
    "    output_size = 1  # 输出维度（难度定数）\n",
    "    num_layers = 2  # LSTM层数\n",
    "\n",
    "    # 创建模型实例\n",
    "    model = SimpleLSTM(input_size, hidden_size, output_size, num_layers)\n",
    "\n",
    "    # 开始训练模型\n",
    "    train_model(model, data_loader, num_epochs=1, learning_rate=0.001)\n",
    "\n",
    "test_model_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e27cee",
   "metadata": {},
   "source": [
    "## 6. 模型评估与性能分析\n",
    "\n",
    "**评估指标**：\n",
    "- **回归指标**：MSE, MAE, R²\n",
    "- **难度区间准确性**：预测值在真实值 ±0.1, ±0.2, ±0.5 范围内的比例\n",
    "- **分布分析**：预测值与真实值的分布对比\n",
    "\n",
    "**详细分析**：\n",
    "- **不同难度等级的预测准确性**：分析模型在低难度 vs 高难度谱面上的表现\n",
    "- **序列长度影响**：分析谱面长度对预测准确性的影响\n",
    "- **错误案例分析**：找出预测偏差较大的谱面特征\n",
    "\n",
    "**TODO**：\n",
    "- 实现全面的评估指标计算\n",
    "- 可视化预测结果分布\n",
    "- 分析不同难度区间的预测准确性\n",
    "- 进行错误案例的深入分析\n",
    "- 与传统特征工程方法进行对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c76b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     predictions = model(X_test_tensor)\n",
    "#     test_loss = criterion(predictions, y_test_tensor)\n",
    "#     print(f'Test Loss: {test_loss.item():.4f}')\n",
    "#\n",
    "# # 可以在这里添加更详细的评估指标，例如 MAE, R^2 等"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f003fee0",
   "metadata": {},
   "source": [
    "## 7. 结果分析与模型迭代\n",
    "\n",
    "**深度分析**：\n",
    "- **时序特征的重要性**：LSTM 是否有效捕捉了时序信息\n",
    "- **不同 note 类型的影响**：哪些类型的 note 对难度预测更重要\n",
    "- **序列长度 vs 准确性**：最优的序列长度设置\n",
    "- **模型复杂度 vs 性能**：单层 vs 多层 LSTM 的权衡\n",
    "\n",
    "**模型优化方向**：\n",
    "- **架构改进**：考虑 Transformer、CNN-LSTM 混合架构\n",
    "- **特征增强**：是否需要添加手工特征作为辅助\n",
    "- **数据增强**：通过时间扭曲、音符变换等方式增加训练数据\n",
    "- **多任务学习**：同时预测难度和其他属性（如技巧需求）\n",
    "\n",
    "**TODO**：\n",
    "- 深入分析 LSTM 学到的时序模式\n",
    "- 可视化注意力权重（如果使用了注意力机制）\n",
    "- 比较不同模型架构的效果\n",
    "- 设计更鲁棒的数据增强策略\n",
    "- 考虑集成学习方法提升性能\n",
    "- 为生产环境部署准备模型压缩和优化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bf24c6",
   "metadata": {},
   "source": [
    "分析模型的预测结果，与真实定数进行比较。\n",
    "\n",
    "思考以下问题：\n",
    "- 模型的误差主要来自哪些谱面？\n",
    "- 是否有必要调整特征工程的方案？\n",
    "- 是否需要更复杂的模型结构？\n",
    "\n",
    "根据分析结果，回到前面的步骤进行迭代优化。\n",
    "\n",
    "**关键思考问题**：\n",
    "\n",
    "1. **时序建模的有效性**：\n",
    "   - LSTM 是否真的比传统统计特征更有效？\n",
    "   - 谱面的时序特征对难度的影响有多大？\n",
    "\n",
    "2. **数据表示的完整性**：\n",
    "   - 当前的 note 编码是否充分表达了游戏的复杂性？\n",
    "   - 是否遗漏了重要的游戏机制信息？\n",
    "\n",
    "3. **模型的可解释性**：\n",
    "   - 如何理解模型学到的难度判断规律？\n",
    "   - 能否提取出可解释的难度评估规则？\n",
    "\n",
    "4. **实际应用价值**：\n",
    "   - 模型的预测精度是否满足实际需求？\n",
    "   - 如何将模型集成到谱面制作工具中？\n",
    "\n",
    "**下一步迭代方向**：\n",
    "根据实验结果，有针对性地改进数据处理、模型架构或训练策略，最终目标是构建一个既准确又实用的难度预测系统。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
