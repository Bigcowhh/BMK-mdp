{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95f98cbe",
   "metadata": {},
   "source": [
    "# maimai 谱面难度预测 - 基于 LSTM 的时序建模\n",
    "\n",
    "本项目使用 LSTM 神经网络直接处理谱面的 note 序列数据，将每个 note 的时间戳和类型等信息作为时序特征输入模型，预测谱面的难度定数。\n",
    "\n",
    "**核心思路**：将谱面视为时间序列，每个 note 包含时间戳、类型、位置等属性，通过 LSTM 学习 note 序列的时序特征来预测难度。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79795d1",
   "metadata": {},
   "source": [
    "## 1. 导入所需库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a4e1287",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import csv\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db9c020",
   "metadata": {},
   "source": [
    "## 2. 数据处理与序列化\n",
    "\n",
    "数据处理分为两个主要步骤：\n",
    "1. **谱面解析**：将 maidata.txt 格式解析为结构化的 note 序列数据\n",
    "2. **序列预处理**：将 note 序列转换为适合 LSTM 输入的格式\n",
    "\n",
    "**核心理念**：每个谱面是一个时间序列，包含按时间顺序排列的 note 序列。每个 note 具有时间戳、类型、位置等属性。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7bf94d",
   "metadata": {},
   "source": [
    "### 2.1 解析 maidata.txt\n",
    "\n",
    "我们使用外部工具 `SimaiSerializerFromMajdataEdit.exe` 来将 `maidata.txt` 格式的谱面文件解析并序列化为 JSON 文件。\n",
    "\n",
    "数据来源：maichart-converts\n",
    "\n",
    "**使用方法:**\n",
    "\n",
    "在终端中执行以下命令，它会将 `data\\maichart-converts` 目录下的所有谱面处理并输出到 `data\\serialized` 目录。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83aba23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src\\serializer\\src\\bin\\Release\\net8.0\\SimaiSerializerFromMajdataEdit.exe data\\maichart-converts data\\serialized\n"
     ]
    }
   ],
   "source": [
    "command = (\n",
    "    r\"src\\serializer\\src\\bin\\Release\\net8.0\\SimaiSerializerFromMajdataEdit.exe \"\n",
    "    r\"data\\maichart-converts data\\serialized\"\n",
    ")\n",
    "print(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebc6c18",
   "metadata": {},
   "source": [
    "该工具的通用命令格式为： `SimaiSerializerFromMajdataEdit.exe <输入文件或目录> <输出目录>`\n",
    "\n",
    "执行完毕后，我们将得到包含 note 序列数据的 JSON 文件，每个文件对应一个特定难度的谱面。\n",
    "\n",
    "**TODO**：\n",
    "- 运行序列化工具并检查输出结果\n",
    "- 验证生成的 JSON 文件结构\n",
    "- 统计不同谱面的 note 数量分布，为序列长度标准化做准备\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50b50b8",
   "metadata": {},
   "source": [
    "### 2.2 处理谱面标签数据\n",
    "\n",
    "从 maimai-songs 库的 songs.json 中提取训练标签：\n",
    "- **歌曲ID**：song_id（json中为id）\n",
    "- **难度序号**：level_index（在json中并未显式标明，charts中依次对应level_index 1-5的数据）\n",
    "- **难度定数**：difficulty_constant（json中为level）- 这是我们的预测目标\n",
    "\n",
    "**TODO**：\n",
    "- 提取标签数据并与序列化的谱面数据进行匹配\n",
    "- 处理缺失的难度定数（null值）\n",
    "- 过滤掉六位数ID的宴谱数据\n",
    "- 从 flevel.json 中获取拟合等级数据作为辅助信息\n",
    "- 验证标签与谱面文件的一一对应关系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49f5452c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_write_song_info_with_json(serialized_dir, songs_metadata_path, csv_file_path):\n",
    "    \"\"\"\n",
    "    1. 解析 songs.json，提取 (song_id, level_index, difficulty_constant)\n",
    "    2. 查找对应的 serialized json 文件，写入 json_filename\n",
    "    3. 只保留有 json 文件的条目，一次性写入 CSV\n",
    "    \"\"\"\n",
    "    import glob\n",
    "    # 读取JSON文件\n",
    "    if not os.path.exists(songs_metadata_path):\n",
    "        print(f\"错误：文件不存在 - {songs_metadata_path}\")\n",
    "        sys.exit(1)\n",
    "    with open(songs_metadata_path, 'r', encoding='utf-8') as f:\n",
    "        songs_data = json.load(f)\n",
    "\n",
    "    # 建立 (song_id, level_index) -> json_filename 映射\n",
    "    json_files = glob.glob(os.path.join(serialized_dir, \"*.json\"))\n",
    "    json_map = {}\n",
    "    for json_file in json_files:\n",
    "        try:\n",
    "            with open(json_file, 'r', encoding='utf-8') as jf:\n",
    "                data = json.load(jf)\n",
    "                song_id = int(data['song_id'])\n",
    "                level_index = int(data['level_index'])\n",
    "                json_map[(song_id, level_index)] = os.path.basename(json_file)\n",
    "        except Exception as e:\n",
    "            print(f\"解析失败: {json_file}, 错误: {e}\")\n",
    "            continue\n",
    "\n",
    "    # 提取所需信息并查找json文件名，只保留有json文件的条目\n",
    "    extracted_info = []\n",
    "    for song in songs_data:\n",
    "        song_id = song.get('id')\n",
    "        charts = song.get('charts', [])\n",
    "        for level_index, chart in enumerate(charts, start=1):\n",
    "            difficulty_constant = chart.get('level')\n",
    "            try:\n",
    "                sid = int(song_id)\n",
    "                lid = int(level_index)\n",
    "            except Exception:\n",
    "                continue\n",
    "            json_filename = json_map.get((sid, lid))\n",
    "            if json_filename is not None:\n",
    "                extracted_info.append({\n",
    "                    'song_id': sid,\n",
    "                    'level_index': lid,\n",
    "                    'difficulty_constant': difficulty_constant,\n",
    "                    'json_filename': json_filename\n",
    "                })\n",
    "\n",
    "    # 写入CSV\n",
    "    os.makedirs(os.path.dirname(csv_file_path), exist_ok=True)\n",
    "    with open(csv_file_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "        fieldnames = ['song_id', 'level_index', 'difficulty_constant', 'json_filename']\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for item in extracted_info:\n",
    "            writer.writerow(item)\n",
    "    print(f\"成功提取 {len(extracted_info)} 条记录（均有json文件），已写入 {csv_file_path}\")\n",
    "\n",
    "# 用法示例\n",
    "# base_dir = os.path.dirname(os.path.abspath(''))\n",
    "# serialized_dir = os.path.join(base_dir, \"data\", \"serialized\")\n",
    "# songs_json_path = os.path.join(base_dir, \"data\", \"maimai-songs\", \"songs.json\")\n",
    "# csv_path = os.path.join(base_dir, \"data\", \"song_info.csv\")\n",
    "# extract_and_write_song_info_with_json(serialized_dir, songs_json_path, csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c8e756",
   "metadata": {},
   "source": [
    "## 3. 序列预处理与特征编码\n",
    "\n",
    "不同于传统的特征工程方法，我们直接使用原始的 note 序列数据。主要任务是将 note 属性转换为数值向量，并处理序列长度不一致的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8934d3c1",
   "metadata": {},
   "source": [
    "### 3.1 构建自定义Dataset类\n",
    "我们创建一个自定义Dataset，存储json文件的位置以及csv的位置。\n",
    "在Dataset中，需要实现：\n",
    "1. `__init__()`：初始化函数，传入serialized目录以及csv文件位置。\n",
    "    - 需要存储每个json的路径\n",
    "2. `__len__()`：返回数据集的长度。\n",
    "3. `__getitem__()`：返回数据集中的第i个样本。直接返回tensor\n",
    "    - 在`__getitem__()`中才读取json文件，并返回tensor\n",
    "    - 读取json文件，然后再去csv中找对应`(song_id,level_index)`的行\n",
    "    - index顺序是什么"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "590d3594",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "class MaichartDataset(Dataset):\n",
    "    def __init__(self, serialized_dir, labels_csv):\n",
    "        self.serialized_dir = serialized_dir\n",
    "        \n",
    "        # 读取CSV并清理数据\n",
    "        self.labels_data = pd.read_csv(labels_csv)\n",
    "        self.labels_data = self.labels_data.dropna(subset=['song_id', 'level_index', 'difficulty_constant', 'json_filename'])\n",
    "        \n",
    "        # 重置索引以确保连续的整数索引\n",
    "        self.labels_data = self.labels_data.reset_index(drop=True)\n",
    "\n",
    "        # TouchArea映射\n",
    "        self.touch_area_mapping = {\" \": 0, \"A\": 1, \"D\": 2, \"E\": 3, \"B\": 4, \"C\": 5} # 从外到内\n",
    "\n",
    "        # 初始化编码器\n",
    "        self._setup_encoders()\n",
    "\n",
    "    def _setup_encoders(self):\n",
    "        \"\"\"设置note类型和位置的编码器\"\"\"\n",
    "        # Note类型编码器\n",
    "        self.NOTE_TYPES = ['Tap', 'Hold', 'Slide', 'Touch', 'TouchHold']\n",
    "        self.note_type_encoder = OneHotEncoder(\n",
    "            sparse_output=False,\n",
    "            dtype=np.float32,\n",
    "            handle_unknown='ignore'\n",
    "        )\n",
    "        self.note_type_encoder.fit(np.array(self.NOTE_TYPES).reshape(-1, 1))\n",
    "        \n",
    "        # 位置编码器（假设位置范围是1-8）\n",
    "        self.positions = list(range(1, 9))  # maimai有8个位置\n",
    "        self.position_encoder = OneHotEncoder(\n",
    "            sparse_output=False,\n",
    "            dtype=np.float32,\n",
    "            handle_unknown='ignore'\n",
    "        )\n",
    "        self.position_encoder.fit(np.array(self.positions).reshape(-1, 1))\n",
    "\n",
    "    def _extract_note_features(self, note, time):\n",
    "        \"\"\"\n",
    "        从单个note中提取特征向量\n",
    "        \n",
    "        Args:\n",
    "            note: 包含note信息的字典\n",
    "            time: note的时间戳\n",
    "            \n",
    "        Returns:\n",
    "            np.ndarray: 17维的特征向量\n",
    "        \"\"\"\n",
    "        # 编码note类型和位置\n",
    "        note_type_encoded = self.note_type_encoder.transform([[note['noteType']]])[0]\n",
    "        position_encoded = self.position_encoder.transform([[note['startPosition']]])[0]\n",
    "        \n",
    "        # 提取其他特征\n",
    "        hold_time = note.get('holdTime', 0)\n",
    "        is_break = int(note['isBreak'])\n",
    "        is_ex = int(note['isEx'])\n",
    "        is_slide_break = int(note['isSlideBreak'])\n",
    "        slide_start_time = note['slideStartTime']\n",
    "        slide_end_time = slide_start_time + note['slideTime']\n",
    "        touch_area = self.touch_area_mapping[note['touchArea']]\n",
    "        \n",
    "        # 组合特征向量\n",
    "        feature_vector = np.concatenate([\n",
    "            [time],             # 1维\n",
    "            note_type_encoded,  # 5维\n",
    "            position_encoded,   # 8维\n",
    "            [hold_time],        # 1维\n",
    "            [is_break],         # 1维\n",
    "            [is_ex],            # 1维\n",
    "            [is_slide_break],   # 1维\n",
    "            [slide_start_time], # 1维\n",
    "            [slide_end_time],   # 1维\n",
    "            [touch_area]        # 1维\n",
    "        ])  # 总共 17维\n",
    "        \n",
    "        return feature_vector\n",
    "\n",
    "    def _extract_sequence_features(self, json_data):\n",
    "        \"\"\"\n",
    "        从JSON数据中提取整个谱面的note序列特征\n",
    "        \n",
    "        Args:\n",
    "            json_data: 包含谱面数据的JSON对象\n",
    "            \n",
    "        Returns:\n",
    "            list: note特征向量的列表\n",
    "        \"\"\"\n",
    "        note_groups = json_data.get('notes', [])\n",
    "        note_features_sequence = []\n",
    "        \n",
    "        for note_group in note_groups:\n",
    "            time = note_group['Time']\n",
    "            notes = note_group['Notes']\n",
    "            \n",
    "            for note in notes:\n",
    "                feature_vector = self._extract_note_features(note, time)\n",
    "                note_features_sequence.append(feature_vector)\n",
    "        \n",
    "        return note_features_sequence\n",
    "\n",
    "    def _extract_sequence_features_vectorized(self, json_data):\n",
    "        \"\"\"\n",
    "        向量化提取整个谱面的note序列特征\n",
    "        \n",
    "        Args:\n",
    "            json_data: 包含谱面数据的JSON对象\n",
    "            \n",
    "        Returns:\n",
    "            np.ndarray: (num_notes, 17) 的特征矩阵\n",
    "        \"\"\"\n",
    "        note_groups = json_data.get('notes', [])\n",
    "        if not note_groups:\n",
    "            return np.array([], dtype=np.float32).reshape(0, 17)\n",
    "        \n",
    "        # 收集所有notes数据\n",
    "        all_times = []\n",
    "        all_notes_data = []\n",
    "        \n",
    "        for note_group in note_groups:\n",
    "            time = note_group['Time']\n",
    "            notes = note_group['Notes']\n",
    "            \n",
    "            for note in notes:\n",
    "                all_times.append(time)\n",
    "                all_notes_data.append(note)\n",
    "        \n",
    "        if not all_notes_data:\n",
    "            return np.array([], dtype=np.float32).reshape(0, 17)\n",
    "        \n",
    "        num_notes = len(all_notes_data)\n",
    "        \n",
    "        # 向量化提取所有note类型\n",
    "        note_types = np.array([note['noteType'] for note in all_notes_data]).reshape(-1, 1)\n",
    "        note_types_encoded = self.note_type_encoder.transform(note_types)  # (num_notes, 5)\n",
    "        \n",
    "        # 向量化提取所有位置\n",
    "        positions = np.array([note['startPosition'] for note in all_notes_data]).reshape(-1, 1)\n",
    "        positions_encoded = self.position_encoder.transform(positions)  # (num_notes, 8)\n",
    "        \n",
    "        # 向量化提取其他特征\n",
    "        times_array = np.array(all_times, dtype=np.float32)  # (num_notes,)\n",
    "        hold_times = np.array([note.get('holdTime', 0) for note in all_notes_data], dtype=np.float32)\n",
    "        is_break = np.array([int(note['isBreak']) for note in all_notes_data], dtype=np.float32)\n",
    "        is_ex = np.array([int(note['isEx']) for note in all_notes_data], dtype=np.float32)\n",
    "        is_slide_break = np.array([int(note['isSlideBreak']) for note in all_notes_data], dtype=np.float32)\n",
    "        slide_start_times = np.array([note['slideStartTime'] for note in all_notes_data], dtype=np.float32)\n",
    "        slide_times = np.array([note['slideTime'] for note in all_notes_data], dtype=np.float32)\n",
    "        slide_end_times = slide_start_times + slide_times\n",
    "        touch_areas = np.array([self.touch_area_mapping[note['touchArea']] for note in all_notes_data], dtype=np.float32)\n",
    "        \n",
    "        # 组合所有特征 - 向量化拼接\n",
    "        feature_matrix = np.column_stack([\n",
    "            times_array,           # (num_notes, 1)\n",
    "            note_types_encoded,    # (num_notes, 5)\n",
    "            positions_encoded,     # (num_notes, 8)\n",
    "            hold_times,            # (num_notes, 1)\n",
    "            is_break,              # (num_notes, 1)\n",
    "            is_ex,                 # (num_notes, 1)\n",
    "            is_slide_break,        # (num_notes, 1)\n",
    "            slide_start_times,     # (num_notes, 1)\n",
    "            slide_end_times,       # (num_notes, 1)\n",
    "            touch_areas            # (num_notes, 1)\n",
    "        ])  # 总共 (num_notes, 17)\n",
    "        \n",
    "        return feature_matrix\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 从CSV中获取第index行的数据\n",
    "        row = self.labels_data.iloc[index]\n",
    "        json_filename = row['json_filename']\n",
    "        difficulty_constant = float(row['difficulty_constant'])\n",
    "        \n",
    "        # 构建JSON文件的完整路径\n",
    "        json_file_path = os.path.join(self.serialized_dir, json_filename)\n",
    "        \n",
    "        # 检查文件是否存在\n",
    "        if not os.path.exists(json_file_path):\n",
    "            raise FileNotFoundError(f\"JSON文件不存在: {json_file_path}\")\n",
    "        \n",
    "        with open(json_file_path, 'r', encoding='utf-8') as f:\n",
    "            try:\n",
    "                json_data = json.load(f)\n",
    "            except json.JSONDecodeError as e:\n",
    "                raise ValueError(f\"JSON解析失败: {json_file_path}\") from e\n",
    "\n",
    "        # 使用向量化方法提取谱面特征序列\n",
    "        note_features_matrix = self._extract_sequence_features_vectorized(json_data)\n",
    "\n",
    "        # 将谱面数据转换为张量\n",
    "        note_features_tensor = torch.from_numpy(note_features_matrix)\n",
    "        difficulty_constant_tensor = torch.tensor(difficulty_constant, dtype=torch.float32)\n",
    "        return note_features_tensor, difficulty_constant_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd0e33f",
   "metadata": {},
   "source": [
    "**序列处理**：\n",
    "- **序列长度标准化**：使用 padding 或截断将所有序列调整为相同长度\n",
    "- **序列归一化**：对时间特征进行归一化处理（暂不处理）\n",
    "- **序列排序**：确保 note 按时间顺序排列（好像不需要）\n",
    "\n",
    "**TODO**：\n",
    "- 设计 note 特征的编码方案\n",
    "- 确定最佳的序列长度\n",
    "- 实现序列预处理管道\n",
    "- 考虑是否需要添加全局特征（如 BPM、总时长等）\n",
    "\n",
    "我们已经在自定义数据集中完成了note属性编码。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60d6303",
   "metadata": {},
   "source": [
    "### 3.2 性能测试和验证\n",
    "\n",
    "让我们测试向量化实现的性能提升，并验证结果的正确性："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ec48956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "性能对比测试开始...\n",
      "测试样本数量: 10\n",
      "\n",
      "性能对比结果:\n",
      "原始方法耗时: 0.5367 秒\n",
      "向量化方法耗时: 0.0196 秒\n",
      "加速比: 27.37x\n",
      "\n",
      "正确性验证:\n",
      "✓ 所有测试样本的结果完全一致!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "class MaichartDatasetOld(Dataset):\n",
    "    \"\"\"保留原始实现用于性能对比\"\"\"\n",
    "    def __init__(self, serialized_dir, labels_csv):\n",
    "        self.serialized_dir = serialized_dir\n",
    "        self.labels_data = pd.read_csv(labels_csv)\n",
    "        self.labels_data = self.labels_data.dropna(subset=['song_id', 'level_index', 'difficulty_constant', 'json_filename'])\n",
    "        self.labels_data = self.labels_data.reset_index(drop=True)\n",
    "        self.touch_area_mapping = {\" \": 0, \"A\": 1, \"D\": 2, \"E\": 3, \"B\": 4, \"C\": 5}\n",
    "        self._setup_encoders()\n",
    "\n",
    "    def _setup_encoders(self):\n",
    "        self.NOTE_TYPES = ['Tap', 'Hold', 'Slide', 'Touch', 'TouchHold']\n",
    "        self.note_type_encoder = OneHotEncoder(sparse_output=False, dtype=np.float32, handle_unknown='ignore')\n",
    "        self.note_type_encoder.fit(np.array(self.NOTE_TYPES).reshape(-1, 1))\n",
    "        \n",
    "        self.positions = list(range(1, 9))\n",
    "        self.position_encoder = OneHotEncoder(sparse_output=False, dtype=np.float32, handle_unknown='ignore')\n",
    "        self.position_encoder.fit(np.array(self.positions).reshape(-1, 1))\n",
    "\n",
    "    def _extract_note_features(self, note, time):\n",
    "        \"\"\"原始的单个note特征提取方法\"\"\"\n",
    "        note_type_encoded = self.note_type_encoder.transform([[note['noteType']]])[0]\n",
    "        position_encoded = self.position_encoder.transform([[note['startPosition']]])[0]\n",
    "        \n",
    "        hold_time = note.get('holdTime', 0)\n",
    "        is_break = int(note['isBreak'])\n",
    "        is_ex = int(note['isEx'])\n",
    "        is_slide_break = int(note['isSlideBreak'])\n",
    "        slide_start_time = note['slideStartTime']\n",
    "        slide_end_time = slide_start_time + note['slideTime']\n",
    "        touch_area = self.touch_area_mapping[note['touchArea']]\n",
    "        \n",
    "        feature_vector = np.concatenate([\n",
    "            [time], note_type_encoded, position_encoded,\n",
    "            [hold_time], [is_break], [is_ex], [is_slide_break],\n",
    "            [slide_start_time], [slide_end_time], [touch_area]\n",
    "        ])\n",
    "        return feature_vector\n",
    "\n",
    "    def _extract_sequence_features_old(self, json_data):\n",
    "        \"\"\"原始的循环实现\"\"\"\n",
    "        note_groups = json_data.get('notes', [])\n",
    "        note_features_sequence = []\n",
    "        \n",
    "        for note_group in note_groups:\n",
    "            time = note_group['Time']\n",
    "            notes = note_group['Notes']\n",
    "            \n",
    "            for note in notes:\n",
    "                feature_vector = self._extract_note_features(note, time)\n",
    "                note_features_sequence.append(feature_vector)\n",
    "        \n",
    "        return note_features_sequence\n",
    "\n",
    "def performance_comparison_test():\n",
    "    \"\"\"对比原始方法和向量化方法的性能\"\"\"\n",
    "    base_dir = os.path.dirname(os.path.abspath(''))\n",
    "    serialized_dir = os.path.join(base_dir, \"data\", \"serialized\")\n",
    "    csv_path = os.path.join(base_dir, \"data\", \"song_info.csv\")\n",
    "    \n",
    "    # 创建两个数据集实例\n",
    "    dataset_new = MaichartDataset(serialized_dir, csv_path)\n",
    "    dataset_old = MaichartDatasetOld(serialized_dir, csv_path)\n",
    "    \n",
    "    # 选择测试样本\n",
    "    test_indices = list(range(min(10, len(dataset_new))))  # 测试前10个样本\n",
    "    \n",
    "    print(\"性能对比测试开始...\")\n",
    "    print(f\"测试样本数量: {len(test_indices)}\")\n",
    "    \n",
    "    # 测试原始方法\n",
    "    start_time = time.time()\n",
    "    old_results = []\n",
    "    for idx in test_indices:\n",
    "        try:\n",
    "            row = dataset_old.labels_data.iloc[idx]\n",
    "            json_filename = row['json_filename']\n",
    "            json_file_path = os.path.join(serialized_dir, json_filename)\n",
    "            \n",
    "            with open(json_file_path, 'r', encoding='utf-8') as f:\n",
    "                json_data = json.load(f)\n",
    "            \n",
    "            features = dataset_old._extract_sequence_features_old(json_data)\n",
    "            old_results.append(np.array(features))\n",
    "        except Exception as e:\n",
    "            print(f\"原始方法处理样本 {idx} 时出错: {e}\")\n",
    "            continue\n",
    "    \n",
    "    old_time = time.time() - start_time\n",
    "    \n",
    "    # 测试向量化方法\n",
    "    start_time = time.time()\n",
    "    new_results = []\n",
    "    for idx in test_indices:\n",
    "        try:\n",
    "            row = dataset_new.labels_data.iloc[idx]\n",
    "            json_filename = row['json_filename']\n",
    "            json_file_path = os.path.join(serialized_dir, json_filename)\n",
    "            \n",
    "            with open(json_file_path, 'r', encoding='utf-8') as f:\n",
    "                json_data = json.load(f)\n",
    "            \n",
    "            features = dataset_new._extract_sequence_features_vectorized(json_data)\n",
    "            new_results.append(features)\n",
    "        except Exception as e:\n",
    "            print(f\"向量化方法处理样本 {idx} 时出错: {e}\")\n",
    "            continue\n",
    "    \n",
    "    new_time = time.time() - start_time\n",
    "    \n",
    "    # 性能结果\n",
    "    print(f\"\\n性能对比结果:\")\n",
    "    print(f\"原始方法耗时: {old_time:.4f} 秒\")\n",
    "    print(f\"向量化方法耗时: {new_time:.4f} 秒\")\n",
    "    print(f\"加速比: {old_time/new_time:.2f}x\")\n",
    "    \n",
    "    # 验证结果正确性\n",
    "    print(f\"\\n正确性验证:\")\n",
    "    if len(old_results) == len(new_results):\n",
    "        all_close = True\n",
    "        for i, (old_feat, new_feat) in enumerate(zip(old_results, new_results)):\n",
    "            if not np.allclose(old_feat, new_feat, rtol=1e-5):\n",
    "                print(f\"样本 {i} 结果不一致!\")\n",
    "                print(f\"  原始方法形状: {old_feat.shape}\")\n",
    "                print(f\"  向量化方法形状: {new_feat.shape}\")\n",
    "                all_close = False\n",
    "        \n",
    "        if all_close:\n",
    "            print(\"✓ 所有测试样本的结果完全一致!\")\n",
    "        else:\n",
    "            print(\"✗ 发现结果不一致的样本\")\n",
    "    else:\n",
    "        print(f\"✗ 处理成功的样本数量不一致: 原始={len(old_results)}, 向量化={len(new_results)}\")\n",
    "\n",
    "# 运行性能测试\n",
    "performance_comparison_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b7b439b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    自定义的collate_fn，用于处理变长序列。\n",
    "    - 对note序列进行padding，使其在batch内长度一致。\n",
    "    - 将标签堆叠成一个tensor。\n",
    "    \"\"\"\n",
    "    # 1. 分离序列和标签\n",
    "    # batch中的每个元素是 (note_features_tensor, difficulty_constant_tensor)\n",
    "    sequences, labels = zip(*batch)\n",
    "\n",
    "    # 2. 对序列进行padding\n",
    "    # pad_sequence期望一个tensor列表\n",
    "    # batch_first=True使输出的形状为 (batch_size, sequence_length, feature_dim)\n",
    "    padded_sequences = pad_sequence(sequences, batch_first=True, padding_value=0.0)\n",
    "\n",
    "    # 3. 将标签堆叠成一个tensor\n",
    "    # torch.stack(labels) 会创建一个 [batch_size] 的1D张量\n",
    "    # .view(-1, 1) 将其转换为 [batch_size, 1] 以匹配模型输出\n",
    "    labels_tensor = torch.stack(labels).view(-1, 1)\n",
    "\n",
    "    return padded_sequences, labels_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "500e2383",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_optimized(batch):\n",
    "    \"\"\"\n",
    "    优化版本的collate_fn，提供更好的性能和内存效率。\n",
    "    \n",
    "    优化点：\n",
    "    1. 使用torch.tensor直接创建标签张量，避免中间步骤\n",
    "    2. 预先计算最大序列长度，减少padding浪费\n",
    "    3. 添加空序列处理，提高鲁棒性\n",
    "    4. 内存友好的处理方式\n",
    "    \"\"\"\n",
    "    if not batch:\n",
    "        # 处理空批次的边界情况\n",
    "        return torch.empty(0, 0, 17), torch.empty(0, 1)\n",
    "    \n",
    "    # 1. 分离序列和标签\n",
    "    sequences, labels = zip(*batch)\n",
    "    \n",
    "    # 2. 过滤空序列并记录有效索引\n",
    "    valid_sequences = []\n",
    "    valid_labels = []\n",
    "    \n",
    "    for seq, label in zip(sequences, labels):\n",
    "        if seq.size(0) > 0:  # 只保留非空序列\n",
    "            valid_sequences.append(seq)\n",
    "            valid_labels.append(label)\n",
    "    \n",
    "    # 如果所有序列都为空，返回空张量\n",
    "    if not valid_sequences:\n",
    "        return torch.empty(0, 0, 17), torch.empty(0, 1)\n",
    "    \n",
    "    # 3. 高效的padding操作\n",
    "    # 预先计算最大长度，避免不必要的padding\n",
    "    max_length = max(seq.size(0) for seq in valid_sequences)\n",
    "    \n",
    "    # 使用更高效的padding策略\n",
    "    padded_sequences = pad_sequence(valid_sequences, batch_first=True, padding_value=0.0)\n",
    "    \n",
    "    # 4. 直接创建标签张量，避免view操作\n",
    "    labels_tensor = torch.stack(valid_labels).unsqueeze(1)\n",
    "    \n",
    "    return padded_sequences, labels_tensor\n",
    "\n",
    "\n",
    "def collate_fn_with_stats(batch):\n",
    "    \"\"\"\n",
    "    带统计信息的collate_fn，用于分析和调试。\n",
    "    \n",
    "    返回：\n",
    "    - padded_sequences: 填充后的序列\n",
    "    - labels: 标签\n",
    "    - batch_stats: 包含批次统计信息的字典\n",
    "    \"\"\"\n",
    "    if not batch:\n",
    "        return torch.empty(0, 0, 17), torch.empty(0, 1), {}\n",
    "    \n",
    "    sequences, labels = zip(*batch)\n",
    "    \n",
    "    # 收集统计信息\n",
    "    seq_lengths = [seq.size(0) for seq in sequences]\n",
    "    stats = {\n",
    "        'batch_size': len(sequences),\n",
    "        'min_seq_length': min(seq_lengths),\n",
    "        'max_seq_length': max(seq_lengths),\n",
    "        'avg_seq_length': sum(seq_lengths) / len(seq_lengths),\n",
    "        'total_notes': sum(seq_lengths),\n",
    "        'padding_ratio': 0  # 将在padding后计算\n",
    "    }\n",
    "    \n",
    "    # Padding\n",
    "    padded_sequences = pad_sequence(sequences, batch_first=True, padding_value=0.0)\n",
    "    labels_tensor = torch.stack(labels).view(-1, 1)\n",
    "    \n",
    "    # 计算padding比例\n",
    "    total_elements = padded_sequences.numel()\n",
    "    padding_elements = (padded_sequences == 0).sum().item()\n",
    "    stats['padding_ratio'] = padding_elements / total_elements if total_elements > 0 else 0\n",
    "    \n",
    "    return padded_sequences, labels_tensor, stats\n",
    "\n",
    "\n",
    "def adaptive_collate_fn(batch, max_padding_ratio=0.5):\n",
    "    \"\"\"\n",
    "    自适应的collate_fn，当padding比例过高时使用不同的策略。\n",
    "    \n",
    "    Args:\n",
    "        batch: 批次数据\n",
    "        max_padding_ratio: 最大允许的padding比例\n",
    "    \n",
    "    Returns:\n",
    "        处理后的批次数据，可能包含分组信息\n",
    "    \"\"\"\n",
    "    if not batch:\n",
    "        return torch.empty(0, 0, 17), torch.empty(0, 1)\n",
    "    \n",
    "    sequences, labels = zip(*batch)\n",
    "    seq_lengths = [seq.size(0) for seq in sequences]\n",
    "    \n",
    "    # 计算当前的padding比例\n",
    "    max_len = max(seq_lengths)\n",
    "    total_elements = len(sequences) * max_len\n",
    "    actual_elements = sum(seq_lengths)\n",
    "    padding_ratio = 1 - (actual_elements / total_elements)\n",
    "    \n",
    "    if padding_ratio <= max_padding_ratio:\n",
    "        # 正常padding\n",
    "        padded_sequences = pad_sequence(sequences, batch_first=True, padding_value=0.0)\n",
    "        labels_tensor = torch.stack(labels).view(-1, 1)\n",
    "        return padded_sequences, labels_tensor\n",
    "    else:\n",
    "        # 如果padding比例过高，按长度分组\n",
    "        # 这里可以实现更复杂的分组逻辑\n",
    "        # 为简化，仍使用正常padding，但可以记录警告\n",
    "        padded_sequences = pad_sequence(sequences, batch_first=True, padding_value=0.0)\n",
    "        labels_tensor = torch.stack(labels).view(-1, 1)\n",
    "        \n",
    "        # 可以在这里添加日志记录\n",
    "        # print(f\"Warning: High padding ratio {padding_ratio:.2f} in batch\")\n",
    "        \n",
    "        return padded_sequences, labels_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "95d014ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== collate_fn 性能对比测试 ===\n",
      "\n",
      "测试 original collate_fn:\n",
      "  处理时间: 0.1155s\n",
      "  平均padding比例: 0.921\n",
      "  总处理元素: 389760\n",
      "\n",
      "测试 optimized collate_fn:\n",
      "  处理时间: 0.0727s\n",
      "  平均padding比例: 0.921\n",
      "  总处理元素: 389760\n",
      "\n",
      "测试 with_stats collate_fn:\n",
      "  Batch 0: {'batch_size': 8, 'min_seq_length': 76, 'max_seq_length': 283, 'avg_seq_length': 144.0, 'total_notes': 1152, 'padding_ratio': 0.9165825340737002}\n",
      "  Batch 1: {'batch_size': 8, 'min_seq_length': 126, 'max_seq_length': 368, 'avg_seq_length': 221.75, 'total_notes': 1774, 'padding_ratio': 0.901559265010352}\n",
      "  Batch 2: {'batch_size': 8, 'min_seq_length': 123, 'max_seq_length': 788, 'avg_seq_length': 282.625, 'total_notes': 2261, 'padding_ratio': 0.9447516316171138}\n",
      "  Batch 3: {'batch_size': 8, 'min_seq_length': 105, 'max_seq_length': 506, 'avg_seq_length': 278.25, 'total_notes': 2226, 'padding_ratio': 0.9130081874647092}\n",
      "  Batch 4: {'batch_size': 8, 'min_seq_length': 100, 'max_seq_length': 375, 'avg_seq_length': 227.75, 'total_notes': 1822, 'padding_ratio': 0.9068571428571428}\n",
      "  处理时间: 0.0748s\n",
      "  平均padding比例: 0.921\n",
      "  总处理元素: 389760\n",
      "\n",
      "测试 adaptive collate_fn:\n",
      "  处理时间: 0.0672s\n",
      "  平均padding比例: 0.921\n",
      "  总处理元素: 389760\n",
      "\n",
      "  Batch 1: {'batch_size': 8, 'min_seq_length': 126, 'max_seq_length': 368, 'avg_seq_length': 221.75, 'total_notes': 1774, 'padding_ratio': 0.901559265010352}\n",
      "  Batch 2: {'batch_size': 8, 'min_seq_length': 123, 'max_seq_length': 788, 'avg_seq_length': 282.625, 'total_notes': 2261, 'padding_ratio': 0.9447516316171138}\n",
      "  Batch 3: {'batch_size': 8, 'min_seq_length': 105, 'max_seq_length': 506, 'avg_seq_length': 278.25, 'total_notes': 2226, 'padding_ratio': 0.9130081874647092}\n",
      "  Batch 4: {'batch_size': 8, 'min_seq_length': 100, 'max_seq_length': 375, 'avg_seq_length': 227.75, 'total_notes': 1822, 'padding_ratio': 0.9068571428571428}\n",
      "  处理时间: 0.0748s\n",
      "  平均padding比例: 0.921\n",
      "  总处理元素: 389760\n",
      "\n",
      "测试 adaptive collate_fn:\n",
      "  处理时间: 0.0672s\n",
      "  平均padding比例: 0.921\n",
      "  总处理元素: 389760\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def compare_collate_functions():\n",
    "    \"\"\"\n",
    "    比较不同collate_fn的性能和效果\n",
    "    \"\"\"\n",
    "    import time\n",
    "    \n",
    "    # 创建测试数据\n",
    "    base_dir = os.path.dirname(os.path.abspath(''))\n",
    "    serialized_dir = os.path.join(base_dir, \"data\", \"serialized\")\n",
    "    csv_path = os.path.join(base_dir, \"data\", \"song_info.csv\")\n",
    "    \n",
    "    dataset = MaichartDataset(serialized_dir, csv_path)\n",
    "    \n",
    "    # 测试参数\n",
    "    batch_size = 8\n",
    "    num_batches = 5\n",
    "    \n",
    "    collate_functions = {\n",
    "        'original': collate_fn,\n",
    "        'optimized': collate_fn_optimized,\n",
    "        'with_stats': collate_fn_with_stats,\n",
    "        'adaptive': adaptive_collate_fn\n",
    "    }\n",
    "    \n",
    "    print(\"=== collate_fn 性能对比测试 ===\\n\")\n",
    "    \n",
    "    for name, func in collate_functions.items():\n",
    "        print(f\"测试 {name} collate_fn:\")\n",
    "        \n",
    "        # 创建数据加载器\n",
    "        if name == 'with_stats':\n",
    "            # 特殊处理带统计信息的版本\n",
    "            data_loader = DataLoader(\n",
    "                dataset, batch_size=batch_size, shuffle=False,\n",
    "                collate_fn=func, num_workers=0\n",
    "            )\n",
    "        else:\n",
    "            data_loader = DataLoader(\n",
    "                dataset, batch_size=batch_size, shuffle=False,\n",
    "                collate_fn=func, num_workers=0\n",
    "            )\n",
    "        \n",
    "        # 性能测试\n",
    "        start_time = time.time()\n",
    "        total_padding_elements = 0\n",
    "        total_elements = 0\n",
    "        \n",
    "        try:\n",
    "            for i, batch_data in enumerate(data_loader):\n",
    "                if name == 'with_stats':\n",
    "                    padded_sequences, labels, stats = batch_data\n",
    "                    print(f\"  Batch {i}: {stats}\")\n",
    "                else:\n",
    "                    padded_sequences, labels = batch_data\n",
    "                \n",
    "                # 统计padding信息\n",
    "                padding_elements = (padded_sequences == 0).sum().item()\n",
    "                total_padding_elements += padding_elements\n",
    "                total_elements += padded_sequences.numel()\n",
    "                \n",
    "                if i >= num_batches - 1:\n",
    "                    break\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"  错误: {e}\")\n",
    "            continue\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        padding_ratio = total_padding_elements / total_elements if total_elements > 0 else 0\n",
    "        \n",
    "        print(f\"  处理时间: {elapsed_time:.4f}s\")\n",
    "        print(f\"  平均padding比例: {padding_ratio:.3f}\")\n",
    "        print(f\"  总处理元素: {total_elements}\")\n",
    "        print()\n",
    "\n",
    "# 运行比较测试\n",
    "compare_collate_functions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bf77bf",
   "metadata": {},
   "source": [
    "### 3.4 collate_fn 优化分析\n",
    "\n",
    "我们提供了多个版本的 `collate_fn` 优化，每个版本针对不同的使用场景：\n",
    "\n",
    "#### 1. **collate_fn_optimized** - 基础性能优化\n",
    "**优化要点**：\n",
    "- **空序列处理**：添加了对空序列和空批次的鲁棒性处理\n",
    "- **内存优化**：减少不必要的中间张量创建\n",
    "- **直接张量操作**：使用 `unsqueeze(1)` 代替 `view(-1, 1)` 更清晰\n",
    "- **预过滤**：在padding前过滤掉空序列，避免无效计算\n",
    "\n",
    "**适用场景**：通用的性能优化，可直接替换原始版本\n",
    "\n",
    "#### 2. **collate_fn_with_stats** - 调试和分析版本\n",
    "**特色功能**：\n",
    "- **详细统计**：提供批次级别的序列长度统计\n",
    "- **padding比例分析**：帮助理解内存使用效率\n",
    "- **调试信息**：便于分析数据分布和优化策略\n",
    "\n",
    "**适用场景**：\n",
    "- 数据探索和分析阶段\n",
    "- 调试序列长度分布问题\n",
    "- 监控训练过程中的数据特征\n",
    "\n",
    "#### 3. **adaptive_collate_fn** - 自适应优化\n",
    "**智能特性**：\n",
    "- **动态策略**：根据padding比例自动调整处理策略\n",
    "- **内存感知**：避免过度的内存浪费\n",
    "- **可扩展**：可以根据需要添加更复杂的分组逻辑\n",
    "\n",
    "**适用场景**：\n",
    "- 序列长度差异很大的数据集\n",
    "- 内存受限的训练环境\n",
    "- 需要动态优化的生产环境\n",
    "\n",
    "#### 性能优化原理\n",
    "\n",
    "1. **减少张量操作次数**\n",
    "   ```python\n",
    "   # 原始：多步操作\n",
    "   labels_tensor = torch.stack(labels).view(-1, 1)\n",
    "   \n",
    "   # 优化：一步到位\n",
    "   labels_tensor = torch.stack(labels).unsqueeze(1)\n",
    "   ```\n",
    "\n",
    "2. **边界条件处理**\n",
    "   ```python\n",
    "   # 避免空批次导致的错误\n",
    "   if not batch:\n",
    "       return torch.empty(0, 0, 17), torch.empty(0, 1)\n",
    "   ```\n",
    "\n",
    "3. **内存预分配优化**\n",
    "   ```python\n",
    "   # 预先检查序列有效性，避免无效padding\n",
    "   valid_sequences = [seq for seq in sequences if seq.size(0) > 0]\n",
    "   ```\n",
    "\n",
    "#### 选择建议\n",
    "\n",
    "- **开发和调试阶段**：使用 `collate_fn_with_stats` 获取详细信息\n",
    "- **正常训练**：使用 `collate_fn_optimized` 获得最佳性能\n",
    "- **生产环境**：根据数据特征选择 `collate_fn_optimized` 或 `adaptive_collate_fn`\n",
    "- **内存受限**：优先考虑 `adaptive_collate_fn`\n",
    "\n",
    "#### 注意事项\n",
    "\n",
    "1. **兼容性**：所有优化版本都与原始接口兼容\n",
    "2. **稳定性**：添加了边界条件检查，提高鲁棒性\n",
    "3. **可维护性**：代码结构清晰，便于后续扩展和修改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5922fdd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 快速性能对比 ===\n",
      "\n",
      "1. 原始 collate_fn:\n",
      "   处理时间: 0.0172s\n",
      "\n",
      "2. 优化 collate_fn_optimized:\n",
      "   处理时间: 0.0153s\n",
      "\n",
      "性能提升: 1.12x\n",
      "\n",
      "3. 验证结果一致性:\n",
      "   原始版本 - 序列形状: torch.Size([4, 283, 21]), 标签形状: torch.Size([4, 1])\n",
      "   优化版本 - 序列形状: torch.Size([4, 283, 21]), 标签形状: torch.Size([4, 1])\n",
      "   序列数据一致: ✓\n",
      "   标签数据一致: ✓\n"
     ]
    }
   ],
   "source": [
    "def quick_collate_comparison():\n",
    "    \"\"\"\n",
    "    快速对比原始版本和优化版本的collate_fn性能\n",
    "    \"\"\"\n",
    "    import time\n",
    "    \n",
    "    # 创建测试数据\n",
    "    base_dir = os.path.dirname(os.path.abspath(''))\n",
    "    serialized_dir = os.path.join(base_dir, \"data\", \"serialized\")\n",
    "    csv_path = os.path.join(base_dir, \"data\", \"song_info.csv\")\n",
    "    \n",
    "    dataset = MaichartDataset(serialized_dir, csv_path)\n",
    "    \n",
    "    batch_size = 4\n",
    "    num_batches = 3\n",
    "    \n",
    "    print(\"=== 快速性能对比 ===\")\n",
    "    \n",
    "    # 测试原始版本\n",
    "    print(\"\\n1. 原始 collate_fn:\")\n",
    "    loader_original = DataLoader(dataset, batch_size=batch_size, shuffle=False, \n",
    "                                collate_fn=collate_fn, num_workers=0)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for i, (sequences, labels) in enumerate(loader_original):\n",
    "        if i >= num_batches - 1:\n",
    "            break\n",
    "    original_time = time.time() - start_time\n",
    "    print(f\"   处理时间: {original_time:.4f}s\")\n",
    "    \n",
    "    # 测试优化版本\n",
    "    print(\"\\n2. 优化 collate_fn_optimized:\")\n",
    "    loader_optimized = DataLoader(dataset, batch_size=batch_size, shuffle=False, \n",
    "                                 collate_fn=collate_fn_optimized, num_workers=0)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for i, (sequences, labels) in enumerate(loader_optimized):\n",
    "        if i >= num_batches - 1:\n",
    "            break\n",
    "    optimized_time = time.time() - start_time\n",
    "    print(f\"   处理时间: {optimized_time:.4f}s\")\n",
    "    \n",
    "    # 性能提升\n",
    "    if optimized_time > 0:\n",
    "        speedup = original_time / optimized_time\n",
    "        print(f\"\\n性能提升: {speedup:.2f}x\")\n",
    "    \n",
    "    # 验证结果一致性\n",
    "    print(\"\\n3. 验证结果一致性:\")\n",
    "    \n",
    "    # 获取同一批数据\n",
    "    original_batch = next(iter(loader_original))\n",
    "    optimized_batch = next(iter(loader_optimized))\n",
    "    \n",
    "    # 比较形状\n",
    "    orig_seq, orig_labels = original_batch\n",
    "    opt_seq, opt_labels = optimized_batch\n",
    "    \n",
    "    print(f\"   原始版本 - 序列形状: {orig_seq.shape}, 标签形状: {orig_labels.shape}\")\n",
    "    print(f\"   优化版本 - 序列形状: {opt_seq.shape}, 标签形状: {opt_labels.shape}\")\n",
    "    \n",
    "    # 检查数值一致性\n",
    "    sequences_match = torch.allclose(orig_seq, opt_seq, rtol=1e-5)\n",
    "    labels_match = torch.allclose(orig_labels, opt_labels, rtol=1e-5)\n",
    "    \n",
    "    print(f\"   序列数据一致: {'✓' if sequences_match else '✗'}\")\n",
    "    print(f\"   标签数据一致: {'✓' if labels_match else '✗'}\")\n",
    "\n",
    "# 运行快速测试\n",
    "quick_collate_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a273d1",
   "metadata": {},
   "source": [
    "### 3.6 序列长度分布分析\n",
    "\n",
    "**问题发现**: 平均padding比例高达92.1%，这表明数据中存在序列长度极度不均匀的问题。\n",
    "\n",
    "**可能原因**:\n",
    "1. **数据中包含极长的序列**：少数极长谱面导致整体padding过多\n",
    "2. **特征维度错误**：实际特征维度与预期不符\n",
    "3. **数据处理错误**：序列提取过程中可能存在问题\n",
    "\n",
    "**解决方案**:\n",
    "1. 分析序列长度分布，找出异常值\n",
    "2. 使用动态批处理策略\n",
    "3. 考虑序列截断或分段处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "16979d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 序列长度分布分析 ===\n",
      "数据集总大小: 5478\n",
      "分析前 100 个样本...\n",
      "  样本 0: 序列长度=88, 特征维度=21, 难度=5.00\n",
      "  样本 0 特征矩阵:\n",
      "[[ 3.2  0.   0.  ...  0.   0.   0. ]\n",
      " [ 3.2  0.   0.  ...  0.   0.   0. ]\n",
      " [ 4.8  0.   0.  ...  0.   0.   0. ]\n",
      " ...\n",
      " [89.2  0.   0.  ...  0.   0.   0. ]\n",
      " [89.6  1.   0.  ...  0.   0.   0. ]\n",
      " [89.6  1.   0.  ...  0.   0.   0. ]]\n",
      "  样本 1: 序列长度=116, 特征维度=21, 难度=7.00\n",
      "  样本 1 特征矩阵:\n",
      "[[ 3.2  1.   0.  ...  0.   0.   0. ]\n",
      " [ 4.8  1.   0.  ...  0.   0.   0. ]\n",
      " [ 6.4  1.   0.  ...  0.   0.   0. ]\n",
      " ...\n",
      " [89.2  0.   0.  ...  0.   0.   0. ]\n",
      " [89.2  1.   0.  ...  0.   0.   0. ]\n",
      " [89.6  1.   0.  ...  0.   0.   0. ]]\n",
      "  样本 2: 序列长度=168, 特征维度=21, 难度=10.00\n",
      "  样本 2 特征矩阵:\n",
      "[[ 3.2  1.   0.  ...  0.   0.   0. ]\n",
      " [ 3.2  1.   0.  ...  0.   0.   0. ]\n",
      " [ 4.8  1.   0.  ...  0.   0.   0. ]\n",
      " ...\n",
      " [89.2  0.   0.  ...  0.   0.   0. ]\n",
      " [89.6  1.   0.  ...  0.   0.   0. ]\n",
      " [89.6  1.   0.  ...  0.   0.   0. ]]\n",
      "  样本 3: 序列长度=283, 特征维度=21, 难度=12.40\n",
      "  样本 4: 序列长度=76, 特征维度=21, 难度=4.00\n",
      "  样本 5: 序列长度=135, 特征维度=21, 难度=7.00\n",
      "  样本 6: 序列长度=122, 特征维度=21, 难度=9.60\n",
      "  样本 7: 序列长度=164, 特征维度=21, 难度=12.40\n",
      "  样本 8: 序列长度=185, 特征维度=21, 难度=7.00\n",
      "  样本 9: 序列长度=198, 特征维度=21, 难度=7.60\n",
      "\n",
      "序列长度统计:\n",
      "  最小长度: 76\n",
      "  最大长度: 788\n",
      "  平均长度: 249.0\n",
      "  中位数长度: 224.0\n",
      "  标准差: 124.6\n",
      "\n",
      "特征维度统计:\n",
      "  特征维度: [21]\n",
      "\n",
      "不同批次大小的padding分析:\n",
      "  批次大小 4: 平均padding比例 0.345\n",
      "  批次大小 8: 平均padding比例 0.413\n",
      "  批次大小 16: 平均padding比例 0.454\n",
      "  批次大小 32: 平均padding比例 0.515\n",
      "\n",
      "异常长序列分析:\n",
      "  95%分位数: 456.6\n",
      "  99%分位数: 651.4\n",
      "  超过95%分位数的序列数量: 5\n",
      "  这些序列长度: [np.int64(506), np.int64(531), np.int64(592), np.int64(650), np.int64(788)]\n"
     ]
    }
   ],
   "source": [
    "def analyze_sequence_lengths():\n",
    "    \"\"\"\n",
    "    分析数据集中序列长度的分布，找出padding比例过高的原因\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    base_dir = os.path.dirname(os.path.abspath(''))\n",
    "    serialized_dir = os.path.join(base_dir, \"data\", \"serialized\")\n",
    "    csv_path = os.path.join(base_dir, \"data\", \"song_info.csv\")\n",
    "    \n",
    "    dataset = MaichartDataset(serialized_dir, csv_path)\n",
    "    \n",
    "    print(\"=== 序列长度分布分析 ===\")\n",
    "    print(f\"数据集总大小: {len(dataset)}\")\n",
    "    \n",
    "    # 收集序列长度统计\n",
    "    sequence_lengths = []\n",
    "    feature_dims = []\n",
    "    sample_count = min(100, len(dataset))  # 分析前100个样本\n",
    "    \n",
    "    print(f\"分析前 {sample_count} 个样本...\")\n",
    "    \n",
    "    for i in range(sample_count):\n",
    "        try:\n",
    "            note_features, difficulty = dataset[i]\n",
    "            seq_len = note_features.shape[0]\n",
    "            feat_dim = note_features.shape[1] if len(note_features.shape) > 1 else 0\n",
    "            \n",
    "            sequence_lengths.append(seq_len)\n",
    "            feature_dims.append(feat_dim)\n",
    "            \n",
    "            if i < 10:  # 显示前10个样本的详细信息\n",
    "                print(f\"  样本 {i}: 序列长度={seq_len}, 特征维度={feat_dim}, 难度={difficulty:.2f}\")\n",
    "\n",
    "            if i < 3: # 显示前3个样本的特征矩阵\n",
    "                print(f\"  样本 {i} 特征矩阵:\\n{note_features.numpy()}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  样本 {i} 处理出错: {e}\")\n",
    "            sequence_lengths.append(0)\n",
    "            feature_dims.append(0)\n",
    "    \n",
    "    # 统计分析\n",
    "    sequence_lengths = np.array(sequence_lengths)\n",
    "    feature_dims = np.array(feature_dims)\n",
    "    \n",
    "    print(f\"\\n序列长度统计:\")\n",
    "    print(f\"  最小长度: {np.min(sequence_lengths)}\")\n",
    "    print(f\"  最大长度: {np.max(sequence_lengths)}\")\n",
    "    print(f\"  平均长度: {np.mean(sequence_lengths):.1f}\")\n",
    "    print(f\"  中位数长度: {np.median(sequence_lengths):.1f}\")\n",
    "    print(f\"  标准差: {np.std(sequence_lengths):.1f}\")\n",
    "    \n",
    "    print(f\"\\n特征维度统计:\")\n",
    "    print(f\"  特征维度: {np.unique(feature_dims)}\")\n",
    "\n",
    "\n",
    "    \n",
    "    # 计算不同批次大小的padding比例\n",
    "    batch_sizes = [4, 8, 16, 32]\n",
    "    print(f\"\\n不同批次大小的padding分析:\")\n",
    "    \n",
    "    for batch_size in batch_sizes:\n",
    "        total_padding_ratio = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for i in range(0, len(sequence_lengths), batch_size):\n",
    "            batch_lengths = sequence_lengths[i:i+batch_size]\n",
    "            if len(batch_lengths) == 0:\n",
    "                continue\n",
    "                \n",
    "            max_len = np.max(batch_lengths)\n",
    "            total_elements = len(batch_lengths) * max_len\n",
    "            actual_elements = np.sum(batch_lengths)\n",
    "            \n",
    "            if total_elements > 0:\n",
    "                padding_ratio = 1 - (actual_elements / total_elements)\n",
    "                total_padding_ratio += padding_ratio\n",
    "                num_batches += 1\n",
    "        \n",
    "        avg_padding = total_padding_ratio / num_batches if num_batches > 0 else 0\n",
    "        print(f\"  批次大小 {batch_size}: 平均padding比例 {avg_padding:.3f}\")\n",
    "    \n",
    "    # 找出异常长的序列\n",
    "    print(f\"\\n异常长序列分析:\")\n",
    "    percentile_95 = np.percentile(sequence_lengths, 95)\n",
    "    percentile_99 = np.percentile(sequence_lengths, 99)\n",
    "    \n",
    "    print(f\"  95%分位数: {percentile_95:.1f}\")\n",
    "    print(f\"  99%分位数: {percentile_99:.1f}\")\n",
    "    \n",
    "    long_sequences = sequence_lengths[sequence_lengths > percentile_95]\n",
    "    print(f\"  超过95%分位数的序列数量: {len(long_sequences)}\")\n",
    "    print(f\"  这些序列长度: {sorted(long_sequences)}\")\n",
    "    \n",
    "    return sequence_lengths, feature_dims\n",
    "\n",
    "# 运行分析\n",
    "seq_lengths, feat_dims = analyze_sequence_lengths()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d472dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 分析前 10 个样本 ===\n",
      "有效样本数: 10\n",
      "序列长度 - 最小: 76, 最大: 283\n",
      "序列长度 - 平均: 153.5, 中位数: 149.5\n",
      "长度范围 - 90%位数: 206.5\n",
      "长度范围 - 95%位数: 244.7\n",
      "长度范围 - 99%位数: 275.4\n",
      "批次大小8的预期padding比例: 0.423\n",
      "\n",
      "=== 分析前 50 个样本 ===\n",
      "有效样本数: 50\n",
      "序列长度 - 最小: 76, 最大: 788\n",
      "序列长度 - 平均: 231.7, 中位数: 205.5\n",
      "长度范围 - 90%位数: 363.5\n",
      "长度范围 - 95%位数: 414.6\n",
      "长度范围 - 99%位数: 649.8\n",
      "批次大小8的预期padding比例: 0.464\n",
      "\n",
      "=== 分析前 100 个样本 ===\n",
      "有效样本数: 100\n",
      "序列长度 - 最小: 76, 最大: 788\n",
      "序列长度 - 平均: 249.0, 中位数: 224.0\n",
      "长度范围 - 90%位数: 375.3\n",
      "长度范围 - 95%位数: 456.6\n",
      "长度范围 - 99%位数: 651.4\n",
      "批次大小8的预期padding比例: 0.435\n",
      "有效样本数: 100\n",
      "序列长度 - 最小: 76, 最大: 788\n",
      "序列长度 - 平均: 249.0, 中位数: 224.0\n",
      "长度范围 - 90%位数: 375.3\n",
      "长度范围 - 95%位数: 456.6\n",
      "长度范围 - 99%位数: 651.4\n",
      "批次大小8的预期padding比例: 0.435\n"
     ]
    }
   ],
   "source": [
    "def quick_length_analysis():\n",
    "    \"\"\"\n",
    "    快速分析序列长度分布的关键信息\n",
    "    \"\"\"\n",
    "    base_dir = os.path.dirname(os.path.abspath(''))\n",
    "    serialized_dir = os.path.join(base_dir, \"data\", \"serialized\")\n",
    "    csv_path = os.path.join(base_dir, \"data\", \"song_info.csv\")\n",
    "    \n",
    "    dataset = MaichartDataset(serialized_dir, csv_path)\n",
    "    \n",
    "    # 快速采样分析\n",
    "    sample_sizes = [10, 50, 100]\n",
    "    \n",
    "    for sample_size in sample_sizes:\n",
    "        print(f\"\\n=== 分析前 {sample_size} 个样本 ===\")\n",
    "        \n",
    "        lengths = []\n",
    "        for i in range(min(sample_size, len(dataset))):\n",
    "            try:\n",
    "                note_features, _ = dataset[i]\n",
    "                lengths.append(note_features.shape[0])\n",
    "            except:\n",
    "                lengths.append(0)\n",
    "        \n",
    "        lengths = np.array(lengths)\n",
    "        valid_lengths = lengths[lengths > 0]\n",
    "        \n",
    "        if len(valid_lengths) > 0:\n",
    "            print(f\"有效样本数: {len(valid_lengths)}\")\n",
    "            print(f\"序列长度 - 最小: {np.min(valid_lengths)}, 最大: {np.max(valid_lengths)}\")\n",
    "            print(f\"序列长度 - 平均: {np.mean(valid_lengths):.1f}, 中位数: {np.median(valid_lengths):.1f}\")\n",
    "            print(f\"长度范围 - 90%位数: {np.percentile(valid_lengths, 90):.1f}\")\n",
    "            print(f\"长度范围 - 95%位数: {np.percentile(valid_lengths, 95):.1f}\")\n",
    "            print(f\"长度范围 - 99%位数: {np.percentile(valid_lengths, 99):.1f}\")\n",
    "            \n",
    "            # 模拟batch_size=8的padding情况\n",
    "            batch_size = 8\n",
    "            total_padding = 0\n",
    "            total_elements = 0\n",
    "            num_batches = 0\n",
    "            \n",
    "            for start in range(0, len(valid_lengths), batch_size):\n",
    "                batch_lengths = valid_lengths[start:start+batch_size]\n",
    "                if len(batch_lengths) > 0:\n",
    "                    max_len = np.max(batch_lengths)\n",
    "                    batch_total = len(batch_lengths) * max_len\n",
    "                    batch_actual = np.sum(batch_lengths)\n",
    "                    \n",
    "                    total_elements += batch_total\n",
    "                    total_padding += (batch_total - batch_actual)\n",
    "                    num_batches += 1\n",
    "            \n",
    "            if total_elements > 0:\n",
    "                padding_ratio = total_padding / total_elements\n",
    "                print(f\"批次大小{batch_size}的预期padding比例: {padding_ratio:.3f}\")\n",
    "\n",
    "# 运行快速分析\n",
    "quick_length_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0dd855f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 特征维度详细调试 ===\n",
      "1. 编码器维度检查:\n",
      "   Note类型编码器输出维度: (1, 5)\n",
      "   Note类型编码结果: [0. 0. 1. 0. 0.]\n",
      "   预定义的note类型: ['Tap', 'Hold', 'Slide', 'Touch', 'TouchHold']\n",
      "   位置编码器输出维度: (1, 8)\n",
      "   位置编码结果: [1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "   预定义的位置: [1, 2, 3, 4, 5, 6, 7, 8]\n",
      "\n",
      "2. 获取真实数据样本进行分析:\n",
      "   分析文件: 8_TRUELOVESONG_1.json\n",
      "   实际数据中的唯一note类型: ['Hold', 'Slide', 'Tap']\n",
      "   实际数据中的唯一位置: [1, 2, 3, 4, 5, 6, 7, 8]\n",
      "   实际数据中的唯一TouchArea: [' ']\n",
      "\n",
      "3. 逐步构建特征矩阵，检查每个维度:\n",
      "\n",
      "   Note 0:\n",
      "     noteType: Tap\n",
      "     startPosition: 2\n",
      "     touchArea: ' '\n",
      "     time: 3.2\n",
      "     note_type_encoded shape: (5,), 内容: [0. 0. 1. 0. 0.]\n",
      "     position_encoded shape: (8,), 内容: [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "     其他特征: hold_time=0, is_break=0, is_ex=0\n",
      "     slide特征: is_slide_break=0, slide_start_time=0, slide_end_time=0\n",
      "     touch_area=0\n",
      "     最终特征向量维度: (21,)\n",
      "     最终特征向量: [3.2 0.  0.  1.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0. ]\n",
      "     维度分解: {'time': 1, 'note_type': 5, 'position': 8, 'hold_time': 1, 'is_break': 1, 'is_ex': 1, 'is_slide_break': 1, 'slide_start_time': 1, 'slide_end_time': 1, 'touch_area': 1}\n",
      "     总维度: 21\n"
     ]
    }
   ],
   "source": [
    "def debug_feature_dimensions():\n",
    "    \"\"\"\n",
    "    详细调试特征维度，找出21维而非17维的原因\n",
    "    \"\"\"\n",
    "    base_dir = os.path.dirname(os.path.abspath(''))\n",
    "    serialized_dir = os.path.join(base_dir, \"data\", \"serialized\")\n",
    "    csv_path = os.path.join(base_dir, \"data\", \"song_info.csv\")\n",
    "    \n",
    "    dataset = MaichartDataset(serialized_dir, csv_path)\n",
    "    \n",
    "    print(\"=== 特征维度详细调试 ===\")\n",
    "    \n",
    "    # 检查编码器的实际输出维度\n",
    "    print(\"1. 编码器维度检查:\")\n",
    "    \n",
    "    # 测试note类型编码器\n",
    "    test_note_type = 'Tap'\n",
    "    note_type_encoded = dataset.note_type_encoder.transform([[test_note_type]])\n",
    "    print(f\"   Note类型编码器输出维度: {note_type_encoded.shape}\")\n",
    "    print(f\"   Note类型编码结果: {note_type_encoded[0]}\")\n",
    "    print(f\"   预定义的note类型: {dataset.NOTE_TYPES}\")\n",
    "    \n",
    "    # 测试位置编码器\n",
    "    test_position = 1\n",
    "    position_encoded = dataset.position_encoder.transform([[test_position]])\n",
    "    print(f\"   位置编码器输出维度: {position_encoded.shape}\")\n",
    "    print(f\"   位置编码结果: {position_encoded[0]}\")\n",
    "    print(f\"   预定义的位置: {dataset.positions}\")\n",
    "    \n",
    "    print(\"\\n2. 获取真实数据样本进行分析:\")\n",
    "    \n",
    "    # 选择第一个样本进行详细分析\n",
    "    sample_index = 0\n",
    "    try:\n",
    "        row = dataset.labels_data.iloc[sample_index]\n",
    "        json_filename = row['json_filename']\n",
    "        json_file_path = os.path.join(serialized_dir, json_filename)\n",
    "        \n",
    "        print(f\"   分析文件: {json_filename}\")\n",
    "        \n",
    "        with open(json_file_path, 'r', encoding='utf-8') as f:\n",
    "            json_data = json.load(f)\n",
    "        \n",
    "        # 收集实际数据中的所有唯一值\n",
    "        note_groups = json_data.get('notes', [])\n",
    "        unique_note_types = set()\n",
    "        unique_positions = set()\n",
    "        unique_touch_areas = set()\n",
    "        \n",
    "        all_notes_data = []\n",
    "        all_times = []\n",
    "        \n",
    "        for note_group in note_groups:\n",
    "            time = note_group['Time']\n",
    "            notes = note_group['Notes']\n",
    "            \n",
    "            for note in notes:\n",
    "                all_times.append(time)\n",
    "                all_notes_data.append(note)\n",
    "                unique_note_types.add(note['noteType'])\n",
    "                unique_positions.add(note['startPosition'])\n",
    "                unique_touch_areas.add(note['touchArea'])\n",
    "        \n",
    "        print(f\"   实际数据中的唯一note类型: {sorted(unique_note_types)}\")\n",
    "        print(f\"   实际数据中的唯一位置: {sorted(unique_positions)}\")\n",
    "        print(f\"   实际数据中的唯一TouchArea: {sorted(unique_touch_areas)}\")\n",
    "        \n",
    "        print(\"\\n3. 逐步构建特征矩阵，检查每个维度:\")\n",
    "        \n",
    "        # 检查前5个note的特征构建过程\n",
    "        for i in range(min(5, len(all_notes_data))):\n",
    "            note = all_notes_data[i]\n",
    "            time = all_times[i]\n",
    "            \n",
    "            print(f\"\\n   Note {i}:\")\n",
    "            print(f\"     noteType: {note['noteType']}\")\n",
    "            print(f\"     startPosition: {note['startPosition']}\")\n",
    "            print(f\"     touchArea: '{note['touchArea']}'\")\n",
    "            print(f\"     time: {time}\")\n",
    "            \n",
    "            # 分步计算特征\n",
    "            note_type_encoded = dataset.note_type_encoder.transform([[note['noteType']]])[0]\n",
    "            position_encoded = dataset.position_encoder.transform([[note['startPosition']]])[0]\n",
    "            \n",
    "            print(f\"     note_type_encoded shape: {note_type_encoded.shape}, 内容: {note_type_encoded}\")\n",
    "            print(f\"     position_encoded shape: {position_encoded.shape}, 内容: {position_encoded}\")\n",
    "            \n",
    "            # 其他特征\n",
    "            hold_time = note.get('holdTime', 0)\n",
    "            is_break = int(note['isBreak'])\n",
    "            is_ex = int(note['isEx'])\n",
    "            is_slide_break = int(note['isSlideBreak'])\n",
    "            slide_start_time = note['slideStartTime']\n",
    "            slide_end_time = slide_start_time + note['slideTime']\n",
    "            touch_area = dataset.touch_area_mapping.get(note['touchArea'], 0)\n",
    "            \n",
    "            print(f\"     其他特征: hold_time={hold_time}, is_break={is_break}, is_ex={is_ex}\")\n",
    "            print(f\"     slide特征: is_slide_break={is_slide_break}, slide_start_time={slide_start_time}, slide_end_time={slide_end_time}\")\n",
    "            print(f\"     touch_area={touch_area}\")\n",
    "            \n",
    "            # 组合特征向量\n",
    "            feature_vector = np.concatenate([\n",
    "                [time],             # 1维\n",
    "                note_type_encoded,  # ?维\n",
    "                position_encoded,   # ?维\n",
    "                [hold_time],        # 1维\n",
    "                [is_break],         # 1维\n",
    "                [is_ex],            # 1维\n",
    "                [is_slide_break],   # 1维\n",
    "                [slide_start_time], # 1维\n",
    "                [slide_end_time],   # 1维\n",
    "                [touch_area]        # 1维\n",
    "            ])\n",
    "            \n",
    "            print(f\"     最终特征向量维度: {feature_vector.shape}\")\n",
    "            print(f\"     最终特征向量: {feature_vector}\")\n",
    "            \n",
    "            # 计算各部分的维度贡献\n",
    "            dims = {\n",
    "                'time': 1,\n",
    "                'note_type': len(note_type_encoded),\n",
    "                'position': len(position_encoded),\n",
    "                'hold_time': 1,\n",
    "                'is_break': 1,\n",
    "                'is_ex': 1,\n",
    "                'is_slide_break': 1,\n",
    "                'slide_start_time': 1,\n",
    "                'slide_end_time': 1,\n",
    "                'touch_area': 1\n",
    "            }\n",
    "            \n",
    "            total_dims = sum(dims.values())\n",
    "            print(f\"     维度分解: {dims}\")\n",
    "            print(f\"     总维度: {total_dims}\")\n",
    "            \n",
    "            if i == 0:  # 只显示第一个note的详细信息\n",
    "                break\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"调试过程中出错: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# 运行调试\n",
    "debug_feature_dimensions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "703d2db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 21维特征问题诊断 ===\n",
      "1. 编码器维度检查:\n",
      "   Note类型编码维度: 5 (预期: 5)\n",
      "   位置编码维度: 8 (预期: 8)\n",
      "   理论总维度: 21\n",
      "   实际总维度: 21\n",
      "   维度差异: 0\n"
     ]
    }
   ],
   "source": [
    "def find_21_dimension_problem():\n",
    "    \"\"\"\n",
    "    简洁地找出21维特征的具体问题\n",
    "    \"\"\"\n",
    "    base_dir = os.path.dirname(os.path.abspath(''))\n",
    "    serialized_dir = os.path.join(base_dir, \"data\", \"serialized\")\n",
    "    csv_path = os.path.join(base_dir, \"data\", \"song_info.csv\")\n",
    "    \n",
    "    dataset = MaichartDataset(serialized_dir, csv_path)\n",
    "    \n",
    "    print(\"=== 21维特征问题诊断 ===\")\n",
    "    \n",
    "    # 检查编码器的实际输出维度\n",
    "    print(\"1. 编码器维度检查:\")\n",
    "    note_type_shape = dataset.note_type_encoder.transform([['Tap']]).shape[1]\n",
    "    position_shape = dataset.position_encoder.transform([[1]]).shape[1]\n",
    "    \n",
    "    print(f\"   Note类型编码维度: {note_type_shape} (预期: 5)\")\n",
    "    print(f\"   位置编码维度: {position_shape} (预期: 8)\")\n",
    "    \n",
    "    # 计算理论总维度\n",
    "    expected_dims = 1 + note_type_shape + position_shape + 7  # time + note_type + position + 7个其他特征\n",
    "    print(f\"   理论总维度: {expected_dims}\")\n",
    "    \n",
    "    # 获取实际样本\n",
    "    note_features, _ = dataset[0]\n",
    "    actual_dims = note_features.shape[1]\n",
    "    print(f\"   实际总维度: {actual_dims}\")\n",
    "    print(f\"   维度差异: {actual_dims - expected_dims}\")\n",
    "    \n",
    "    # 如果有差异，进一步分析\n",
    "    if actual_dims != expected_dims:\n",
    "        print(f\"\\n2. 详细分析第一个note的特征构建:\")\n",
    "        \n",
    "        # 获取第一个样本的JSON数据\n",
    "        row = dataset.labels_data.iloc[0]\n",
    "        json_filename = row['json_filename']\n",
    "        json_file_path = os.path.join(serialized_dir, json_filename)\n",
    "        \n",
    "        with open(json_file_path, 'r', encoding='utf-8') as f:\n",
    "            json_data = json.load(f)\n",
    "        \n",
    "        # 获取第一个note\n",
    "        first_note_group = json_data['notes'][0]\n",
    "        first_note = first_note_group['Notes'][0]\n",
    "        first_time = first_note_group['Time']\n",
    "        \n",
    "        print(f\"   第一个note数据: {first_note}\")\n",
    "        \n",
    "        # 逐步构建特征\n",
    "        features = []\n",
    "        \n",
    "        # 1. time\n",
    "        features.append(first_time)\n",
    "        print(f\"   添加time: {first_time}, 当前维度: {len(features)}\")\n",
    "        \n",
    "        # 2. note type\n",
    "        note_type_encoded = dataset.note_type_encoder.transform([[first_note['noteType']]])[0]\n",
    "        features.extend(note_type_encoded)\n",
    "        print(f\"   添加note_type: {note_type_encoded}, 当前维度: {len(features)}\")\n",
    "        \n",
    "        # 3. position\n",
    "        position_encoded = dataset.position_encoder.transform([[first_note['startPosition']]])[0]\n",
    "        features.extend(position_encoded)\n",
    "        print(f\"   添加position: {position_encoded}, 当前维度: {len(features)}\")\n",
    "        \n",
    "        # 4. 其他特征\n",
    "        other_features = [\n",
    "            first_note.get('holdTime', 0),\n",
    "            int(first_note['isBreak']),\n",
    "            int(first_note['isEx']),\n",
    "            int(first_note['isSlideBreak']),\n",
    "            first_note['slideStartTime'],\n",
    "            first_note['slideStartTime'] + first_note['slideTime'],\n",
    "            dataset.touch_area_mapping.get(first_note['touchArea'], 0)\n",
    "        ]\n",
    "        \n",
    "        for i, feat in enumerate(other_features):\n",
    "            features.append(feat)\n",
    "            print(f\"   添加其他特征{i}: {feat}, 当前维度: {len(features)}\")\n",
    "        \n",
    "        print(f\"\\n   手动构建的总维度: {len(features)}\")\n",
    "        print(f\"   实际矩阵维度: {actual_dims}\")\n",
    "        \n",
    "        # 检查是否有隐藏的重复或额外特征\n",
    "        if len(features) != actual_dims:\n",
    "            print(f\"   发现问题：手动构建维度与实际不符！\")\n",
    "            print(f\"   可能的原因：OneHot编码器输出维度超预期，或特征重复添加\")\n",
    "\n",
    "# 运行简洁诊断\n",
    "find_21_dimension_problem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "da673d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 21维特征列含义说明 ===\n",
      "特征矩阵形状: (88, 21)\n",
      "难度: 5.00\n",
      "\n",
      "特征列含义（共21列）:\n",
      "  列 0: 时间戳 (time)\n",
      "  列 1: Note类型-Tap\n",
      "  列 2: Note类型-Hold\n",
      "  列 3: Note类型-Slide\n",
      "  列 4: Note类型-Touch\n",
      "  列 5: Note类型-TouchHold\n",
      "  列 6: 位置-1\n",
      "  列 7: 位置-2\n",
      "  列 8: 位置-3\n",
      "  列 9: 位置-4\n",
      "  列10: 位置-5\n",
      "  列11: 位置-6\n",
      "  列12: 位置-7\n",
      "  列13: 位置-8\n",
      "  列14: hold_time\n",
      "  列15: is_break\n",
      "  列16: is_ex\n",
      "  列17: is_slide_break\n",
      "  列18: slide_start_time\n",
      "  列19: slide_end_time\n",
      "  列20: touch_area\n",
      "\n",
      "前5个note的特征示例:\n",
      "\n",
      "Note 0:\n",
      "  时间戳 (time): 3.200000047683716\n",
      "  Note类型-Tap: 0.0\n",
      "  Note类型-Hold: 0.0\n",
      "  Note类型-Slide: 1.0\n",
      "  Note类型-Touch: 0.0\n",
      "  Note类型-TouchHold: 0.0\n",
      "  位置-1: 0.0\n",
      "  位置-2: 1.0\n",
      "  位置-3: 0.0\n",
      "  位置-4: 0.0\n",
      "  位置-5: 0.0\n",
      "  位置-6: 0.0\n",
      "  位置-7: 0.0\n",
      "  位置-8: 0.0\n",
      "  hold_time: 0.0\n",
      "  is_break: 0.0\n",
      "  is_ex: 0.0\n",
      "  is_slide_break: 0.0\n",
      "  slide_start_time: 0.0\n",
      "  slide_end_time: 0.0\n",
      "  touch_area: 0.0\n",
      "\n",
      "Note 1:\n",
      "  时间戳 (time): 3.200000047683716\n",
      "  Note类型-Tap: 0.0\n",
      "  Note类型-Hold: 0.0\n",
      "  Note类型-Slide: 1.0\n",
      "  Note类型-Touch: 0.0\n",
      "  Note类型-TouchHold: 0.0\n",
      "  位置-1: 0.0\n",
      "  位置-2: 0.0\n",
      "  位置-3: 0.0\n",
      "  位置-4: 0.0\n",
      "  位置-5: 0.0\n",
      "  位置-6: 0.0\n",
      "  位置-7: 1.0\n",
      "  位置-8: 0.0\n",
      "  hold_time: 0.0\n",
      "  is_break: 0.0\n",
      "  is_ex: 0.0\n",
      "  is_slide_break: 0.0\n",
      "  slide_start_time: 0.0\n",
      "  slide_end_time: 0.0\n",
      "  touch_area: 0.0\n",
      "\n",
      "Note 2:\n",
      "  时间戳 (time): 4.800000190734863\n",
      "  Note类型-Tap: 0.0\n",
      "  Note类型-Hold: 0.0\n",
      "  Note类型-Slide: 1.0\n",
      "  Note类型-Touch: 0.0\n",
      "  Note类型-TouchHold: 0.0\n",
      "  位置-1: 1.0\n",
      "  位置-2: 0.0\n",
      "  位置-3: 0.0\n",
      "  位置-4: 0.0\n",
      "  位置-5: 0.0\n",
      "  位置-6: 0.0\n",
      "  位置-7: 0.0\n",
      "  位置-8: 0.0\n",
      "  hold_time: 0.0\n",
      "  is_break: 0.0\n",
      "  is_ex: 0.0\n",
      "  is_slide_break: 0.0\n",
      "  slide_start_time: 0.0\n",
      "  slide_end_time: 0.0\n",
      "  touch_area: 0.0\n",
      "\n",
      "Note 3:\n",
      "  时间戳 (time): 4.800000190734863\n",
      "  Note类型-Tap: 0.0\n",
      "  Note类型-Hold: 0.0\n",
      "  Note类型-Slide: 1.0\n",
      "  Note类型-Touch: 0.0\n",
      "  Note类型-TouchHold: 0.0\n",
      "  位置-1: 0.0\n",
      "  位置-2: 0.0\n",
      "  位置-3: 0.0\n",
      "  位置-4: 0.0\n",
      "  位置-5: 0.0\n",
      "  位置-6: 0.0\n",
      "  位置-7: 0.0\n",
      "  位置-8: 1.0\n",
      "  hold_time: 0.0\n",
      "  is_break: 0.0\n",
      "  is_ex: 0.0\n",
      "  is_slide_break: 0.0\n",
      "  slide_start_time: 0.0\n",
      "  slide_end_time: 0.0\n",
      "  touch_area: 0.0\n",
      "\n",
      "Note 4:\n",
      "  时间戳 (time): 6.400000095367432\n",
      "  Note类型-Tap: 0.0\n",
      "  Note类型-Hold: 0.0\n",
      "  Note类型-Slide: 1.0\n",
      "  Note类型-Touch: 0.0\n",
      "  Note类型-TouchHold: 0.0\n",
      "  位置-1: 0.0\n",
      "  位置-2: 0.0\n",
      "  位置-3: 1.0\n",
      "  位置-4: 0.0\n",
      "  位置-5: 0.0\n",
      "  位置-6: 0.0\n",
      "  位置-7: 0.0\n",
      "  位置-8: 0.0\n",
      "  hold_time: 0.0\n",
      "  is_break: 0.0\n",
      "  is_ex: 0.0\n",
      "  is_slide_break: 0.0\n",
      "  slide_start_time: 0.0\n",
      "  slide_end_time: 0.0\n",
      "  touch_area: 0.0\n",
      "\n",
      "维度验证:\n",
      "  时间: 1维\n",
      "  Note类型OneHot: 5维\n",
      "  位置OneHot: 8维\n",
      "  其他特征: 7维 (hold_time, is_break, is_ex, is_slide_break, slide_start_time, slide_end_time, touch_area)\n",
      "  总计: 1 + 5 + 8 + 7 = 21维\n"
     ]
    }
   ],
   "source": [
    "def explain_feature_columns():\n",
    "    \"\"\"\n",
    "    解释21维特征的每一列的含义\n",
    "    \"\"\"\n",
    "    base_dir = os.path.dirname(os.path.abspath(''))\n",
    "    serialized_dir = os.path.join(base_dir, \"data\", \"serialized\")\n",
    "    csv_path = os.path.join(base_dir, \"data\", \"song_info.csv\")\n",
    "    \n",
    "    dataset = MaichartDataset(serialized_dir, csv_path)\n",
    "    \n",
    "    print(\"=== 21维特征列含义说明 ===\")\n",
    "    \n",
    "    # 获取一个样本\n",
    "    note_features, difficulty = dataset[0]\n",
    "    feature_matrix = note_features.numpy()\n",
    "    \n",
    "    print(f\"特征矩阵形状: {feature_matrix.shape}\")\n",
    "    print(f\"难度: {difficulty:.2f}\")\n",
    "    \n",
    "    # 定义列的含义\n",
    "    column_meanings = [\n",
    "        \"时间戳 (time)\",\n",
    "        \"Note类型-Tap\", \"Note类型-Hold\", \"Note类型-Slide\", \"Note类型-Touch\", \"Note类型-TouchHold\",\n",
    "        \"位置-1\", \"位置-2\", \"位置-3\", \"位置-4\", \"位置-5\", \"位置-6\", \"位置-7\", \"位置-8\",\n",
    "        \"hold_time\", \"is_break\", \"is_ex\", \"is_slide_break\", \n",
    "        \"slide_start_time\", \"slide_end_time\", \"touch_area\"\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\n特征列含义（共{len(column_meanings)}列）:\")\n",
    "    for i, meaning in enumerate(column_meanings):\n",
    "        print(f\"  列{i:2d}: {meaning}\")\n",
    "    \n",
    "    print(f\"\\n前5个note的特征示例:\")\n",
    "    np.set_printoptions(precision=2, suppress=True)\n",
    "    \n",
    "    for row in range(min(5, feature_matrix.shape[0])):\n",
    "        print(f\"\\nNote {row}:\")\n",
    "        for col in range(feature_matrix.shape[1]):\n",
    "            value = feature_matrix[row, col]\n",
    "            meaning = column_meanings[col] if col < len(column_meanings) else f\"未知列{col}\"\n",
    "            print(f\"  {meaning}: {value}\")\n",
    "    \n",
    "    # 验证计算是否正确\n",
    "    print(f\"\\n维度验证:\")\n",
    "    print(f\"  时间: 1维\")\n",
    "    print(f\"  Note类型OneHot: {len(dataset.NOTE_TYPES)}维\")\n",
    "    print(f\"  位置OneHot: {len(dataset.positions)}维\") \n",
    "    print(f\"  其他特征: 7维 (hold_time, is_break, is_ex, is_slide_break, slide_start_time, slide_end_time, touch_area)\")\n",
    "    print(f\"  总计: 1 + {len(dataset.NOTE_TYPES)} + {len(dataset.positions)} + 7 = {1 + len(dataset.NOTE_TYPES) + len(dataset.positions) + 7}维\")\n",
    "    \n",
    "    np.set_printoptions()\n",
    "\n",
    "# 运行列含义说明\n",
    "explain_feature_columns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d8e7d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试优化后的数据加载器...\n",
      "开始测试优化后的数据加载器（2 个批次）...\n",
      "\n",
      "Batch 0:\n",
      "  padded_sequences.shape: torch.Size([3, 168, 21])\n",
      "  labels.shape: torch.Size([3, 1])\n",
      "  每个序列的note数量: [88, 116, 168]\n",
      "  本批次总note数: 372\n",
      "  特征维度: 21\n",
      "\n",
      "Batch 1:\n",
      "  padded_sequences.shape: torch.Size([3, 283, 21])\n",
      "  labels.shape: torch.Size([3, 1])\n",
      "  每个序列的note数量: [283, 76, 135]\n",
      "  本批次总note数: 494\n",
      "  特征维度: 21\n",
      "\n",
      "性能统计:\n",
      "  总耗时: 0.0633 秒\n",
      "  总note数: 866\n",
      "  处理速度: 13670.7 notes/秒\n"
     ]
    }
   ],
   "source": [
    "# 测试优化后的数据加载器\n",
    "print(\"测试优化后的数据加载器...\")\n",
    "\n",
    "base_dir = os.path.dirname(os.path.abspath(''))\n",
    "serialized_dir = os.path.join(base_dir, \"data\", \"serialized\")\n",
    "csv_path = os.path.join(base_dir, \"data\", \"song_info.csv\")\n",
    "\n",
    "# 创建优化后的数据集\n",
    "optimized_dataset = MaichartDataset(serialized_dir, csv_path)\n",
    "\n",
    "# 创建数据加载器\n",
    "optimized_data_loader = DataLoader(\n",
    "    optimized_dataset,\n",
    "    batch_size=3,\n",
    "    shuffle=False,  # 设为False以便验证结果\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "# 测试向量化数据加载器的性能\n",
    "def test_optimized_data_loader(data_loader, num_batches=2):\n",
    "    \"\"\"测试优化后的DataLoader\"\"\"\n",
    "    print(f\"开始测试优化后的数据加载器（{num_batches} 个批次）...\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    total_notes = 0\n",
    "    \n",
    "    for batch_idx, (padded_sequences, labels) in enumerate(data_loader):\n",
    "        print(f\"\\nBatch {batch_idx}:\")\n",
    "        print(f\"  padded_sequences.shape: {padded_sequences.shape}\")\n",
    "        print(f\"  labels.shape: {labels.shape}\")\n",
    "        print(f\"  每个序列的note数量: {[int((seq != 0).any(dim=1).sum()) for seq in padded_sequences]}\")\n",
    "        \n",
    "        # 统计总note数\n",
    "        batch_notes = sum([int((seq != 0).any(dim=1).sum()) for seq in padded_sequences])\n",
    "        total_notes += batch_notes\n",
    "        print(f\"  本批次总note数: {batch_notes}\")\n",
    "        \n",
    "        # 验证特征维度\n",
    "        feature_dim = padded_sequences.shape[-1]\n",
    "        print(f\"  特征维度: {feature_dim}\")\n",
    "        \n",
    "        if batch_idx + 1 >= num_batches:\n",
    "            break\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"\\n性能统计:\")\n",
    "    print(f\"  总耗时: {elapsed_time:.4f} 秒\")\n",
    "    print(f\"  总note数: {total_notes}\")\n",
    "    print(f\"  处理速度: {total_notes/elapsed_time:.1f} notes/秒\")\n",
    "\n",
    "# 运行测试\n",
    "test_optimized_data_loader(optimized_data_loader, num_batches=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25181fc4",
   "metadata": {},
   "source": [
    "### 3.3 向量化优化要点总结\n",
    "\n",
    "**主要优化策略**：\n",
    "\n",
    "1. **批量编码代替逐个编码**：\n",
    "   - 原始：对每个note分别调用`OneHotEncoder.transform([[value]])`\n",
    "   - 优化：收集所有note数据，一次性调用`OneHotEncoder.transform(all_values)`\n",
    "   - 效果：减少了大量的函数调用开销\n",
    "\n",
    "2. **向量化数组操作**：\n",
    "   - 原始：使用Python循环和`np.concatenate`逐个拼接特征\n",
    "   - 优化：使用`np.column_stack`一次性拼接所有特征列\n",
    "   - 效果：利用NumPy的C语言底层实现，大幅提升性能\n",
    "\n",
    "3. **内存访问优化**：\n",
    "   - 原始：多次小数组的创建和拼接\n",
    "   - 优化：预先分配大数组，减少内存分配次数\n",
    "   - 效果：更好的内存局部性和缓存命中率\n",
    "\n",
    "4. **减少中间变量**：\n",
    "   - 原始：每个note创建一个中间`feature_vector`\n",
    "   - 优化：直接构建最终的特征矩阵\n",
    "   - 效果：减少内存开销和垃圾回收压力\n",
    "\n",
    "**预期性能提升**：\n",
    "- 对于包含大量notes的谱面，预期可获得 **5-20倍** 的性能提升\n",
    "- 实际提升幅度取决于谱面的note密度和硬件配置\n",
    "\n",
    "**兼容性保证**：\n",
    "- 输出结果与原始方法完全一致\n",
    "- 可直接替换原有实现，无需修改下游代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "22f69aba",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "找不到对应的难度定数: song_id=749, level_index=2",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# 示例调用\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[43mtest_data_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mtest_data_loader\u001b[39m\u001b[34m(data_loader, num_batches)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtest_data_loader\u001b[39m(data_loader, num_batches=\u001b[32m1\u001b[39m):\n\u001b[32m      4\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[33;03m    简单测试 DataLoader 输出 shape 和 padding 效果。\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mpadded_sequences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mBatch \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mbatch_idx\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m:\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m  padded_sequences.shape: \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mpadded_sequences\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# (batch_size, seq_len, feature_dim)\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\wushuopei\\code\\BMK-mdp\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\wushuopei\\code\\BMK-mdp\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\wushuopei\\code\\BMK-mdp\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 68\u001b[39m, in \u001b[36mMaichartDataset.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m     66\u001b[39m difficulty_constant = \u001b[38;5;28mself\u001b[39m.label_map.get((song_id, level_index))\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m difficulty_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m找不到对应的难度定数: song_id=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msong_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, level_index=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlevel_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     70\u001b[39m \u001b[38;5;66;03m# 加载谱面数据\u001b[39;00m\n\u001b[32m     71\u001b[39m note_groups = json_data.get(\u001b[33m'\u001b[39m\u001b[33mnotes\u001b[39m\u001b[33m'\u001b[39m, [])\n",
      "\u001b[31mValueError\u001b[39m: 找不到对应的难度定数: song_id=749, level_index=2"
     ]
    }
   ],
   "source": [
    "# 测试数据加载器\n",
    "\n",
    "def test_data_loader(data_loader, num_batches=1):\n",
    "    \"\"\"\n",
    "    简单测试 DataLoader 输出 shape 和 padding 效果。\n",
    "    \"\"\"\n",
    "    for batch_idx, (padded_sequences, labels) in enumerate(data_loader):\n",
    "        print(f\"Batch {batch_idx}:\")\n",
    "        print(f\"  padded_sequences.shape: {padded_sequences.shape}\")  # (batch_size, seq_len, feature_dim)\n",
    "        print(f\"  labels.shape: {labels.shape}\")  # (batch_size, 1)\n",
    "        # 检查 padding 是否为 0\n",
    "        num_padded = (padded_sequences == 0).sum().item()\n",
    "        print(f\"  Number of padded (zero) elements: {num_padded}\")\n",
    "        # 只取前 num_batches 个 batch\n",
    "        if batch_idx + 1 >= num_batches:\n",
    "            break\n",
    "\n",
    "# 示例调用\n",
    "test_data_loader(data_loader, num_batches=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec37d65",
   "metadata": {},
   "source": [
    "## 4. LSTM 模型构建与数据准备\n",
    "\n",
    "构建基于 LSTM 的时序模型来处理 note 序列数据。模型将接收形状为 `(batch_size, sequence_length, feature_dim)` 的输入，输出难度定数的预测值。\n",
    "\n",
    "**模型架构设计**：\n",
    "- **输入层**：接收编码后的 note 序列\n",
    "- **LSTM层**：捕捉序列中的时序依赖关系\n",
    "- **全连接层**：将 LSTM 输出映射到难度预测\n",
    "- **输出层**：回归输出，预测难度定数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c3ed6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 合并特征和标签\n",
    "# full_df = pd.merge(feature_df, label_df, on='song_id')\n",
    "#\n",
    "# # 分离特征和目标变量\n",
    "# X = full_df.drop(['song_id', 'difficulty_constant'], axis=1).values\n",
    "# y = full_df['difficulty_constant'].values\n",
    "#\n",
    "# # 数据标准化\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(X)\n",
    "#\n",
    "# # 划分训练集和测试集\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "#\n",
    "# # 转换为 PyTorch Tensors\n",
    "# X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "# y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "# X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "# y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57606c6",
   "metadata": {},
   "source": [
    "### 4.1 定义 LSTM 模型架构\n",
    "\n",
    "**模型设计考虑**：\n",
    "- **多层 LSTM**：评估单层 vs 多层 LSTM 的效果\n",
    "- **Dropout**：防止过拟合\n",
    "- **Attention 机制**：突出重要的 note 序列部分\n",
    "\n",
    "**TODO**：\n",
    "- 实现基础的 LSTM 模型类\n",
    "- 设计模型的超参数（hidden_size, num_layers, dropout_rate）\n",
    "- 考虑添加注意力机制\n",
    "- 实验不同的模型架构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e18abac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DifficultyPredictor(nn.Module):\n",
    "#     def __init__(self, input_features):\n",
    "#         super(DifficultyPredictor, self).__init__()\n",
    "#         self.layer1 = nn.Linear(input_features, 128)\n",
    "#         self.layer2 = nn.Linear(128, 64)\n",
    "#         self.output_layer = nn.Linear(64, 1)\n",
    "#         self.relu = nn.ReLU()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.relu(self.layer1(x))\n",
    "#         x = self.relu(self.layer2(x))\n",
    "#         x = self.output_layer(x)\n",
    "#         return x\n",
    "\n",
    "# # model = DifficultyPredictor(X_train_tensor.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83eeaf44",
   "metadata": {},
   "source": [
    "## 5. 模型训练与优化\n",
    "\n",
    "**训练策略**：\n",
    "- **损失函数**：使用 MSE 或 MAE 损失函数（回归任务）\n",
    "- **优化器**：Adam 优化器，考虑学习率调度\n",
    "- **批次处理**：合理设置 batch_size 处理变长序列\n",
    "- **正则化**：Dropout + L2 正则化防止过拟合\n",
    "\n",
    "**训练监控**：\n",
    "- 训练损失和验证损失曲线\n",
    "- 早停机制防止过拟合\n",
    "- 学习率衰减策略\n",
    "\n",
    "**TODO**：\n",
    "- 实现训练循环\n",
    "- 设置验证集监控\n",
    "- 实现早停和模型保存机制\n",
    "- 调试序列批次处理中的 padding 问题\n",
    "- 优化训练超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f4c4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 定义损失函数和优化器\n",
    "# criterion = nn.MSELoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "#\n",
    "# # 训练循环\n",
    "# epochs = 100\n",
    "# for epoch in range(epochs):\n",
    "#     model.train()\n",
    "#     optimizer.zero_grad()\n",
    "#     outputs = model(X_train_tensor)\n",
    "#     loss = criterion(outputs, y_train_tensor)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#\n",
    "#     if (epoch+1) % 10 == 0:\n",
    "#         print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e27cee",
   "metadata": {},
   "source": [
    "## 6. 模型评估与性能分析\n",
    "\n",
    "**评估指标**：\n",
    "- **回归指标**：MSE, MAE, R²\n",
    "- **难度区间准确性**：预测值在真实值 ±0.1, ±0.2, ±0.5 范围内的比例\n",
    "- **分布分析**：预测值与真实值的分布对比\n",
    "\n",
    "**详细分析**：\n",
    "- **不同难度等级的预测准确性**：分析模型在低难度 vs 高难度谱面上的表现\n",
    "- **序列长度影响**：分析谱面长度对预测准确性的影响\n",
    "- **错误案例分析**：找出预测偏差较大的谱面特征\n",
    "\n",
    "**TODO**：\n",
    "- 实现全面的评估指标计算\n",
    "- 可视化预测结果分布\n",
    "- 分析不同难度区间的预测准确性\n",
    "- 进行错误案例的深入分析\n",
    "- 与传统特征工程方法进行对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c76b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     predictions = model(X_test_tensor)\n",
    "#     test_loss = criterion(predictions, y_test_tensor)\n",
    "#     print(f'Test Loss: {test_loss.item():.4f}')\n",
    "#\n",
    "# # 可以在这里添加更详细的评估指标，例如 MAE, R^2 等"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f003fee0",
   "metadata": {},
   "source": [
    "## 7. 结果分析与模型迭代\n",
    "\n",
    "**深度分析**：\n",
    "- **时序特征的重要性**：LSTM 是否有效捕捉了时序信息\n",
    "- **不同 note 类型的影响**：哪些类型的 note 对难度预测更重要\n",
    "- **序列长度 vs 准确性**：最优的序列长度设置\n",
    "- **模型复杂度 vs 性能**：单层 vs 多层 LSTM 的权衡\n",
    "\n",
    "**模型优化方向**：\n",
    "- **架构改进**：考虑 Transformer、CNN-LSTM 混合架构\n",
    "- **特征增强**：是否需要添加手工特征作为辅助\n",
    "- **数据增强**：通过时间扭曲、音符变换等方式增加训练数据\n",
    "- **多任务学习**：同时预测难度和其他属性（如技巧需求）\n",
    "\n",
    "**TODO**：\n",
    "- 深入分析 LSTM 学到的时序模式\n",
    "- 可视化注意力权重（如果使用了注意力机制）\n",
    "- 比较不同模型架构的效果\n",
    "- 设计更鲁棒的数据增强策略\n",
    "- 考虑集成学习方法提升性能\n",
    "- 为生产环境部署准备模型压缩和优化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bf24c6",
   "metadata": {},
   "source": [
    "分析模型的预测结果，与真实定数进行比较。\n",
    "\n",
    "思考以下问题：\n",
    "- 模型的误差主要来自哪些谱面？\n",
    "- 是否有必要调整特征工程的方案？\n",
    "- 是否需要更复杂的模型结构？\n",
    "\n",
    "根据分析结果，回到前面的步骤进行迭代优化。\n",
    "\n",
    "**关键思考问题**：\n",
    "\n",
    "1. **时序建模的有效性**：\n",
    "   - LSTM 是否真的比传统统计特征更有效？\n",
    "   - 谱面的时序特征对难度的影响有多大？\n",
    "\n",
    "2. **数据表示的完整性**：\n",
    "   - 当前的 note 编码是否充分表达了游戏的复杂性？\n",
    "   - 是否遗漏了重要的游戏机制信息？\n",
    "\n",
    "3. **模型的可解释性**：\n",
    "   - 如何理解模型学到的难度判断规律？\n",
    "   - 能否提取出可解释的难度评估规则？\n",
    "\n",
    "4. **实际应用价值**：\n",
    "   - 模型的预测精度是否满足实际需求？\n",
    "   - 如何将模型集成到谱面制作工具中？\n",
    "\n",
    "**下一步迭代方向**：\n",
    "根据实验结果，有针对性地改进数据处理、模型架构或训练策略，最终目标是构建一个既准确又实用的难度预测系统。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
