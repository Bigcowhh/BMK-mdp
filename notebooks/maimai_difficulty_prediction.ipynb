{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95f98cbe",
   "metadata": {},
   "source": [
    "# maimai 谱面难度预测 - 基于 LSTM 的时序建模\n",
    "\n",
    "本项目使用 LSTM 神经网络直接处理谱面的 note 序列数据，将每个 note 的时间戳和类型等信息作为时序特征输入模型，预测谱面的难度定数。\n",
    "\n",
    "**核心思路**：将谱面视为时间序列，每个 note 包含时间戳、类型、位置等属性，通过 LSTM 学习 note 序列的时序特征来预测难度。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79795d1",
   "metadata": {},
   "source": [
    "## 1. 导入所需库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8a4e1287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import csv\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Device configuration\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "# Global path variables\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(''))\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\")\n",
    "SERIALIZED_DIR = os.path.join(DATA_DIR, \"serialized\")\n",
    "SONG_INFO_PATH = os.path.join(DATA_DIR, \"song_info.csv\")\n",
    "EXCLUDED_SONGS_PATH = os.path.join(DATA_DIR, \"excluded_songs.csv\")\n",
    "TRAIN_DATA_PATH = os.path.join(DATA_DIR, \"train_data.csv\")\n",
    "TEST_DATA_PATH = os.path.join(DATA_DIR, \"test_data.csv\")\n",
    "MODEL_DIR = os.path.join(BASE_DIR, \"models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db9c020",
   "metadata": {},
   "source": [
    "## 2. 数据处理与序列化\n",
    "\n",
    "数据处理分为两个主要步骤：\n",
    "1. **谱面解析**：将 maidata.txt 格式解析为结构化的 note 序列数据\n",
    "2. **序列预处理**：将 note 序列转换为适合 LSTM 输入的格式\n",
    "\n",
    "**核心理念**：每个谱面是一个时间序列，包含按时间顺序排列的 note 序列。每个 note 具有时间戳、类型、位置等属性。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7bf94d",
   "metadata": {},
   "source": [
    "### 2.1 解析 maidata.txt\n",
    "\n",
    "我们使用外部工具 `SimaiSerializerFromMajdataEdit.exe` 来将 `maidata.txt` 格式的谱面文件解析并序列化为 JSON 文件。\n",
    "\n",
    "数据来源：maichart-converts\n",
    "\n",
    "**使用方法:**\n",
    "\n",
    "在终端中执行以下命令，它会将 `data\\maichart-converts` 目录下的所有谱面处理并输出到 `data\\serialized` 目录。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "83aba23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src\\serializer\\src\\bin\\Release\\net8.0\\SimaiSerializerFromMajdataEdit.exe data\\maichart-converts data\\serialized\n"
     ]
    }
   ],
   "source": [
    "command = (\n",
    "    r\"src\\serializer\\src\\bin\\Release\\net8.0\\SimaiSerializerFromMajdataEdit.exe \"\n",
    "    r\"data\\maichart-converts data\\serialized\"\n",
    ")\n",
    "print(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebc6c18",
   "metadata": {},
   "source": [
    "该工具的通用命令格式为： `SimaiSerializerFromMajdataEdit.exe <输入文件或目录> <输出目录>`\n",
    "\n",
    "执行完毕后，我们将得到包含 note 序列数据的 JSON 文件，每个文件对应一个特定难度的谱面。\n",
    "\n",
    "**TODO**：\n",
    "- 运行序列化工具并检查输出结果\n",
    "- 验证生成的 JSON 文件结构\n",
    "- 统计不同谱面的 note 数量分布，为序列长度标准化做准备\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50b50b8",
   "metadata": {},
   "source": [
    "### 2.2 处理谱面标签数据\n",
    "\n",
    "从 maimai-songs 库的 songs.json 中提取训练标签：\n",
    "- **歌曲ID**：song_id（json中为id）\n",
    "- **难度序号**：level_index（在json中并未显式标明，charts中依次对应level_index 1-5的数据）\n",
    "- **难度定数**：difficulty_constant（json中为level）- 这是我们的预测目标\n",
    "\n",
    "**TODO**：\n",
    "- 提取标签数据并与序列化的谱面数据进行匹配\n",
    "- 处理缺失的难度定数（null值）\n",
    "- 过滤掉六位数ID的宴谱数据\n",
    "- 从 flevel.json 中获取拟合等级数据作为辅助信息\n",
    "- 验证标签与谱面文件的一一对应关系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "49f5452c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功提取 5478 条记录（均有json文件），已写入 d:\\wushuopei\\code\\BMK-mdp\\data\\song_info.csv\n"
     ]
    }
   ],
   "source": [
    "def extract_and_write_song_info_with_json(serialized_dir, songs_metadata_path, csv_file_path):\n",
    "    \"\"\"\n",
    "    1. 解析 songs.json，提取 (song_id, level_index, difficulty_constant)\n",
    "    2. 查找对应的 serialized json 文件，读取 total_notes 并写入 json_filename\n",
    "    3. 只保留有 json 文件的条目，一次性写入 CSV\n",
    "    \"\"\"\n",
    "    import glob\n",
    "    # 读取JSON文件\n",
    "    if not os.path.exists(songs_metadata_path):\n",
    "        print(f\"错误：文件不存在 - {songs_metadata_path}\")\n",
    "        sys.exit(1)\n",
    "    with open(songs_metadata_path, 'r', encoding='utf-8') as f:\n",
    "        songs_data = json.load(f)\n",
    "\n",
    "    # 建立 (song_id, level_index) -> (json_filename, total_notes) 映射\n",
    "    json_files = glob.glob(os.path.join(serialized_dir, \"*.json\"))\n",
    "    json_map = {}\n",
    "    for json_file in json_files:\n",
    "        try:\n",
    "            with open(json_file, 'r', encoding='utf-8') as jf:\n",
    "                data = json.load(jf)\n",
    "                song_id = int(data['song_id'])\n",
    "                level_index = int(data['level_index'])\n",
    "                total_notes = data.get('total_notes', 0)  # 获取 total_notes，默认为 0\n",
    "                json_map[(song_id, level_index)] = (os.path.basename(json_file), total_notes)\n",
    "        except Exception as e:\n",
    "            print(f\"解析失败: {json_file}, 错误: {e}\")\n",
    "            continue\n",
    "\n",
    "    # 提取所需信息并查找json文件名和total_notes，只保留有json文件的条目\n",
    "    extracted_info = []\n",
    "    for song in songs_data:\n",
    "        song_id = song.get('id')\n",
    "        charts = song.get('charts', [])\n",
    "        for level_index, chart in enumerate(charts, start=1):\n",
    "            difficulty_constant = chart.get('level')\n",
    "            try:\n",
    "                sid = int(song_id)\n",
    "                lid = int(level_index)\n",
    "            except Exception:\n",
    "                continue\n",
    "            json_info = json_map.get((sid, lid))\n",
    "            if json_info is not None:\n",
    "                json_filename, total_notes = json_info\n",
    "                extracted_info.append({\n",
    "                    'song_id': sid,\n",
    "                    'level_index': lid,\n",
    "                    'difficulty_constant': difficulty_constant,\n",
    "                    'total_notes': total_notes,\n",
    "                    'json_filename': json_filename\n",
    "                })\n",
    "\n",
    "    # 写入CSV\n",
    "    os.makedirs(os.path.dirname(csv_file_path), exist_ok=True)\n",
    "    with open(csv_file_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "        fieldnames = ['song_id', 'level_index', 'difficulty_constant', 'total_notes', 'json_filename']\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for item in extracted_info:\n",
    "            writer.writerow(item)\n",
    "    print(f\"成功提取 {len(extracted_info)} 条记录（均有json文件），已写入 {csv_file_path}\")\n",
    "\n",
    "# 用法示例\n",
    "#songs_json_path = os.path.join(BASE_DIR, \"data\", \"maimai-songs\", \"songs.json\")\n",
    "#extract_and_write_song_info_with_json(SERIALIZED_DIR, songs_json_path, SONG_INFO_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9ffdf1",
   "metadata": {},
   "source": [
    "### 2.3 划分训练集与测试集\n",
    "\n",
    "我们需要排除宴谱，然后：\n",
    "- 对绿/黄/红/紫白谱分别进行训练集/测试集划分，保证每组比例相同\n",
    "- 可能甚至要将紫白谱的每个定数都保证内部均分\n",
    "- 允许将特定谱面排除在训练集之外"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5049dc9c",
   "metadata": {},
   "source": [
    "### 2.3.1 创建排除列表CSV\n",
    "\n",
    "排除列表CSV文件的格式为：\n",
    "```\n",
    "song_id,level_index\n",
    "10001,3       # 排除歌曲ID=10001的第3个难度\n",
    "10002,*       # 排除歌曲ID=10002的所有难度\n",
    "```\n",
    "\n",
    "- `song_id`: 需要排除的歌曲ID\n",
    "- `level_index`: 需要排除的难度等级，使用 `*` 表示所有难度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "36d79652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "读取主数据集：d:\\wushuopei\\code\\BMK-mdp\\data\\song_info.csv\n",
      "读取排除列表：d:\\wushuopei\\code\\BMK-mdp\\data\\excluded_songs.csv\n",
      "排除了 2 条记录，剩余 5476 条\n",
      "按level_index分层，划分训练集和测试集...\n",
      "训练集：4380 条记录，已写入 d:\\wushuopei\\code\\BMK-mdp\\data\\train_data.csv\n",
      "测试集：1096 条记录，已写入 d:\\wushuopei\\code\\BMK-mdp\\data\\test_data.csv\n"
     ]
    }
   ],
   "source": [
    "def split_train_test_data(\n",
    "    input_csv_path,          # 输入的 song_info.csv 路径\n",
    "    exclusion_csv_path,      # 排除列表 CSV 路径\n",
    "    train_csv_path,          # 输出的训练集 CSV 路径\n",
    "    test_csv_path,           # 输出的测试集 CSV 路径\n",
    "    test_size=0.2,           # 测试集比例，默认 20%\n",
    "    random_state=42          # 随机种子\n",
    "):\n",
    "    \"\"\"\n",
    "    根据排除列表CSV划分训练集和测试集，按难度等级（level_index）分层抽样。\n",
    "    \n",
    "    Args:\n",
    "        input_csv_path: 输入的 song_info.csv 路径\n",
    "        exclusion_csv_path: 排除列表 CSV 路径\n",
    "        train_csv_path: 输出的训练集 CSV 路径\n",
    "        test_csv_path: 输出的测试集 CSV 路径\n",
    "        test_size: 测试集比例，默认 20%\n",
    "        random_state: 随机种子\n",
    "        \n",
    "    Returns:\n",
    "        包含划分统计信息的字典\n",
    "    \"\"\"\n",
    "    # 1. 读取主数据集\n",
    "    print(f\"读取主数据集：{input_csv_path}\")\n",
    "    main_data = pd.read_csv(input_csv_path)\n",
    "    original_count = len(main_data)\n",
    "    \n",
    "    # 2. 读取排除列表并应用排除\n",
    "    filtered_data = main_data.copy()\n",
    "    if os.path.exists(exclusion_csv_path):\n",
    "        print(f\"读取排除列表：{exclusion_csv_path}\")\n",
    "        exclusion_list = pd.read_csv(exclusion_csv_path)\n",
    "        \n",
    "        # 创建一个标记要排除的行的掩码\n",
    "        exclude_mask = pd.Series(False, index=range(len(filtered_data)))\n",
    "        \n",
    "        # 遍历排除列表中的每一行\n",
    "        for _, row in exclusion_list.iterrows():\n",
    "            song_id = row['song_id']\n",
    "            level_index = row['level_index']\n",
    "            \n",
    "            if level_index == '*':  # 排除整首歌曲的所有难度\n",
    "                song_mask = filtered_data['song_id'] == song_id\n",
    "                exclude_mask = exclude_mask | song_mask\n",
    "            else:  # 排除特定难度\n",
    "                try:\n",
    "                    level_index = int(level_index)  # 确保level_index是整数\n",
    "                    chart_mask = (filtered_data['song_id'] == song_id) & (filtered_data['level_index'] == level_index)\n",
    "                    exclude_mask = exclude_mask | chart_mask\n",
    "                except ValueError:\n",
    "                    print(f\"警告：无法解析的level_index值：{level_index}，跳过\")\n",
    "                    continue\n",
    "        \n",
    "        # 过滤数据\n",
    "        filtered_data = filtered_data[~exclude_mask]\n",
    "        excluded_count = original_count - len(filtered_data)\n",
    "        print(f\"排除了 {excluded_count} 条记录，剩余 {len(filtered_data)} 条\")\n",
    "    else:\n",
    "        print(f\"排除列表文件不存在：{exclusion_csv_path}，使用全部数据\")\n",
    "    \n",
    "    # 3. 按level_index进行分层抽样\n",
    "    print(\"按level_index分层，划分训练集和测试集...\")\n",
    "    train_data, test_data = train_test_split(\n",
    "        filtered_data, \n",
    "        test_size=test_size,\n",
    "        stratify=filtered_data['level_index'], # 确保测试集中各难度等级的比例与原数据集一致\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # 4. 写入CSV\n",
    "    train_data.to_csv(train_csv_path, index=False)\n",
    "    test_data.to_csv(test_csv_path, index=False)\n",
    "    print(f\"训练集：{len(train_data)} 条记录，已写入 {train_csv_path}\")\n",
    "    print(f\"测试集：{len(test_data)} 条记录，已写入 {test_csv_path}\")\n",
    "    return\n",
    "\n",
    "#split_train_test_data(SONG_INFO_PATH, EXCLUDED_SONGS_PATH, TRAIN_DATA_PATH,  TEST_DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c8e756",
   "metadata": {},
   "source": [
    "## 3. 序列预处理与特征编码\n",
    "\n",
    "不同于传统的特征工程方法，我们直接使用原始的 note 序列数据。主要任务是将 note 属性转换为数值向量，并处理序列长度不一致的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8934d3c1",
   "metadata": {},
   "source": [
    "### 3.1 构建自定义Dataset类\n",
    "我们创建一个自定义Dataset，存储json文件的位置以及csv的位置。\n",
    "在Dataset中，需要实现：\n",
    "1. `__init__()`：初始化函数，传入serialized目录以及csv文件位置。\n",
    "    - 需要存储每个json的路径\n",
    "2. `__len__()`：返回数据集的长度。\n",
    "3. `__getitem__()`：返回数据集中的第i个样本。直接返回tensor\n",
    "    - 在`__getitem__()`中才读取json文件，并返回tensor\n",
    "    - 读取json文件，然后再去csv中找对应`(song_id,level_index)`的行\n",
    "    - index顺序是什么"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "590d3594",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {DEVICE} device\")\n",
    "\n",
    "class MaichartDataset(Dataset):\n",
    "    def __init__(self, serialized_dir, labels_csv, cache_size=None):\n",
    "        self.serialized_dir = serialized_dir\n",
    "        \n",
    "        # 读取CSV并清理数据\n",
    "        self.labels_data = pd.read_csv(labels_csv)\n",
    "        self.labels_data = self.labels_data.dropna(subset=['song_id', 'level_index', 'difficulty_constant','total_notes', 'json_filename'])\n",
    "        \n",
    "        # 重置索引以确保连续的整数索引\n",
    "        self.labels_data = self.labels_data.reset_index(drop=True)\n",
    "\n",
    "        # TouchArea映射\n",
    "        self.touch_area_mapping = {\" \": 0, \"A\": 1, \"D\": 2, \"E\": 3, \"B\": 4, \"C\": 5} # 从外到内\n",
    "\n",
    "        # 初始化编码器\n",
    "        self._setup_encoders()\n",
    "        \n",
    "        # 缓存机制\n",
    "        self.cache_size = cache_size\n",
    "        self.cache = {}  # 存储处理后的数据\n",
    "        self.cache_access_order = []  # 记录访问顺序，用于LRU淘汰\n",
    "\n",
    "    def _setup_encoders(self):\n",
    "        \"\"\"设置note类型和位置的编码器\"\"\"\n",
    "        # Note类型编码器\n",
    "        self.NOTE_TYPES = ['Tap', 'Hold', 'Slide', 'Touch', 'TouchHold']\n",
    "        self.note_type_encoder = OneHotEncoder(\n",
    "            sparse_output=False,\n",
    "            dtype=np.float32,\n",
    "            handle_unknown='ignore'\n",
    "        )\n",
    "        self.note_type_encoder.fit(np.array(self.NOTE_TYPES).reshape(-1, 1))\n",
    "        \n",
    "        # 位置编码器（假设位置范围是1-8）\n",
    "        self.positions = list(range(1, 9))  # maimai有8个位置\n",
    "        self.position_encoder = OneHotEncoder(\n",
    "            sparse_output=False,\n",
    "            dtype=np.float32,\n",
    "            handle_unknown='ignore'\n",
    "        )\n",
    "        self.position_encoder.fit(np.array(self.positions).reshape(-1, 1))\n",
    "\n",
    "    def _manage_cache(self, key):\n",
    "        \"\"\"管理缓存，实现LRU淘汰策略\"\"\"\n",
    "        # 如果key已在缓存中，更新访问顺序\n",
    "        if key in self.cache:\n",
    "            self.cache_access_order.remove(key)\n",
    "            self.cache_access_order.append(key)\n",
    "            return\n",
    "        \n",
    "        # 如果缓存已满，删除最久未使用的项\n",
    "        if self.cache_size is not None and len(self.cache) >= self.cache_size:\n",
    "            oldest_key = self.cache_access_order.pop(0)\n",
    "            del self.cache[oldest_key]\n",
    "        \n",
    "        # 添加新key到访问顺序\n",
    "        self.cache_access_order.append(key)\n",
    "\n",
    "    def _extract_note_features(self, note, time):\n",
    "        \"\"\"\n",
    "        从单个note中提取特征向量\n",
    "        \n",
    "        Args:\n",
    "            note: 包含note信息的字典\n",
    "            time: note的时间戳\n",
    "            \n",
    "        Returns:\n",
    "            np.ndarray: 21维的特征向量\n",
    "        \"\"\"\n",
    "        # 编码note类型和位置\n",
    "        note_type_encoded = self.note_type_encoder.transform([[note['noteType']]])[0]\n",
    "        position_encoded = self.position_encoder.transform([[note['startPosition']]])[0]\n",
    "        \n",
    "        # 提取其他特征\n",
    "        hold_time = note.get('holdTime', 0)\n",
    "        is_break = int(note['isBreak'])\n",
    "        is_ex = int(note['isEx'])\n",
    "        is_slide_break = int(note['isSlideBreak'])\n",
    "        slide_start_time = note['slideStartTime']\n",
    "        slide_end_time = slide_start_time + note['slideTime']\n",
    "        touch_area = self.touch_area_mapping[note['touchArea']]\n",
    "        \n",
    "        # 组合特征向量\n",
    "        feature_vector = np.concatenate([\n",
    "            [time],             # 1维\n",
    "            note_type_encoded,  # 5维\n",
    "            position_encoded,   # 8维\n",
    "            [hold_time],        # 1维\n",
    "            [is_break],         # 1维\n",
    "            [is_ex],            # 1维\n",
    "            [is_slide_break],   # 1维\n",
    "            [slide_start_time], # 1维\n",
    "            [slide_end_time],   # 1维\n",
    "            [touch_area]        # 1维\n",
    "        ])  # 总共 21维\n",
    "        \n",
    "        return feature_vector\n",
    "\n",
    "    def _extract_sequence_features(self, json_data):\n",
    "        \"\"\"\n",
    "        从JSON数据中提取整个谱面的note序列特征\n",
    "        \n",
    "        Args:\n",
    "            json_data: 包含谱面数据的JSON对象\n",
    "            \n",
    "        Returns:\n",
    "            list: note特征向量的列表\n",
    "        \"\"\"\n",
    "        note_groups = json_data.get('notes', [])\n",
    "        note_features_sequence = []\n",
    "        \n",
    "        for note_group in note_groups:\n",
    "            time = note_group['Time']\n",
    "            notes = note_group['Notes']\n",
    "            \n",
    "            for note in notes:\n",
    "                feature_vector = self._extract_note_features(note, time)\n",
    "                note_features_sequence.append(feature_vector)\n",
    "        \n",
    "        return note_features_sequence\n",
    "\n",
    "    def _extract_sequence_features_vectorized(self, json_data):\n",
    "        \"\"\"\n",
    "        向量化提取整个谱面的note序列特征\n",
    "        \n",
    "        Args:\n",
    "            json_data: 包含谱面数据的JSON对象\n",
    "            \n",
    "        Returns:\n",
    "            np.ndarray: (num_notes, 21) 的特征矩阵\n",
    "        \"\"\"\n",
    "        note_groups = json_data.get('notes', [])\n",
    "        if not note_groups:\n",
    "            raise ValueError(f\"未找到{json_data}的note group信息\")\n",
    "        \n",
    "        # 收集所有notes数据\n",
    "        all_times = []\n",
    "        all_notes_data = []\n",
    "        \n",
    "        for note_group in note_groups:\n",
    "            time = note_group['Time']\n",
    "            notes = note_group['Notes']\n",
    "            \n",
    "            for note in notes:\n",
    "                all_times.append(time)\n",
    "                all_notes_data.append(note)\n",
    "        \n",
    "        if not all_notes_data:\n",
    "            raise ValueError(f\"未找到{json_data}的note信息\")\n",
    "        \n",
    "        num_notes = len(all_notes_data)\n",
    "        \n",
    "        # 向量化提取所有note类型\n",
    "        note_types = np.array([note['noteType'] for note in all_notes_data]).reshape(-1, 1)\n",
    "        note_types_encoded = self.note_type_encoder.transform(note_types)  # (num_notes, 5)\n",
    "        \n",
    "        # 向量化提取所有位置\n",
    "        positions = np.array([note['startPosition'] for note in all_notes_data]).reshape(-1, 1)\n",
    "        positions_encoded = self.position_encoder.transform(positions)  # (num_notes, 8)\n",
    "        \n",
    "        # 向量化提取其他特征\n",
    "        times_array = np.array(all_times, dtype=np.float32)  # (num_notes,)\n",
    "        hold_times = np.array([note.get('holdTime', 0) for note in all_notes_data], dtype=np.float32)\n",
    "        is_break = np.array([int(note['isBreak']) for note in all_notes_data], dtype=np.float32)\n",
    "        is_ex = np.array([int(note['isEx']) for note in all_notes_data], dtype=np.float32)\n",
    "        is_slide_break = np.array([int(note['isSlideBreak']) for note in all_notes_data], dtype=np.float32)\n",
    "        slide_start_times = np.array([note['slideStartTime'] for note in all_notes_data], dtype=np.float32)\n",
    "        slide_times = np.array([note['slideTime'] for note in all_notes_data], dtype=np.float32)\n",
    "        slide_end_times = slide_start_times + slide_times\n",
    "        touch_areas = np.array([self.touch_area_mapping[note['touchArea']] for note in all_notes_data], dtype=np.float32)\n",
    "        \n",
    "        # 组合所有特征 - 向量化拼接\n",
    "        feature_matrix = np.column_stack([\n",
    "            times_array,           # (num_notes, 1)\n",
    "            note_types_encoded,    # (num_notes, 5)\n",
    "            positions_encoded,     # (num_notes, 8)\n",
    "            hold_times,            # (num_notes, 1)\n",
    "            is_break,              # (num_notes, 1)\n",
    "            is_ex,                 # (num_notes, 1)\n",
    "            is_slide_break,        # (num_notes, 1)\n",
    "            slide_start_times,     # (num_notes, 1)\n",
    "            slide_end_times,       # (num_notes, 1)\n",
    "            touch_areas            # (num_notes, 1)\n",
    "        ])  # 总共 (num_notes, 21)\n",
    "        \n",
    "        return feature_matrix\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 从CSV中获取第index行的数据\n",
    "        row = self.labels_data.iloc[index]\n",
    "        json_filename = row['json_filename']\n",
    "        difficulty_constant = float(row['difficulty_constant'])\n",
    "        \n",
    "        # 检查缓存中是否已有处理好的数据\n",
    "        cache_key = json_filename\n",
    "        if cache_key in self.cache:\n",
    "            # 缓存命中，更新访问顺序并返回缓存数据\n",
    "            self._manage_cache(cache_key)\n",
    "            note_features_tensor = self.cache[cache_key]\n",
    "        else:\n",
    "            # 缓存未命中，读取并处理JSON文件\n",
    "            json_file_path = os.path.join(self.serialized_dir, json_filename)\n",
    "            \n",
    "            # 检查文件是否存在\n",
    "            if not os.path.exists(json_file_path):\n",
    "                raise FileNotFoundError(f\"JSON文件不存在: {json_file_path}\")\n",
    "            \n",
    "            with open(json_file_path, 'r', encoding='utf-8') as f:\n",
    "                try:\n",
    "                    json_data = json.load(f)\n",
    "                except json.JSONDecodeError as e:\n",
    "                    raise ValueError(f\"JSON解析失败: {json_file_path}\") from e\n",
    "\n",
    "            # 使用向量化方法提取谱面特征序列\n",
    "            note_features_matrix = self._extract_sequence_features_vectorized(json_data)\n",
    "\n",
    "            # 将谱面数据转换为张量\n",
    "            note_features_tensor = torch.from_numpy(note_features_matrix)\n",
    "            \n",
    "            # 将处理好的数据存入缓存\n",
    "            self._manage_cache(cache_key)\n",
    "            self.cache[cache_key] = note_features_tensor\n",
    "\n",
    "        difficulty_constant_tensor = torch.tensor(difficulty_constant, dtype=torch.float32)\n",
    "        return note_features_tensor, difficulty_constant_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels_data)\n",
    "    \n",
    "    def get_cache_stats(self):\n",
    "        \"\"\"\n",
    "        获取缓存统计信息\n",
    "        \n",
    "        Returns:\n",
    "            dict:\n",
    "                - cache_size: 当前缓存中的条目数\n",
    "                - max_cache_size: 最大缓存大小（如果未设置则为无限制）\n",
    "                - cache_usage: 缓存使用百分比\n",
    "                - cached_files: 当前缓存中的文件名列表\n",
    "        \"\"\"\n",
    "        if self.cache_size is None:\n",
    "            cache_usage_percent = 100.0 if len(self.cache) > 0 else 0.0\n",
    "            max_cache_display = \"无限制\"\n",
    "        else:\n",
    "            cache_usage_percent = len(self.cache) / self.cache_size * 100\n",
    "            max_cache_display = self.cache_size\n",
    "        return {\n",
    "            'cache_size': len(self.cache),\n",
    "            'max_cache_size': max_cache_display,\n",
    "            'cache_usage': cache_usage_percent,\n",
    "            'cached_files': list(self.cache.keys())\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd0e33f",
   "metadata": {},
   "source": [
    "### 序列处理：\n",
    "- **序列长度标准化**：使用 padding 或截断将所有序列调整为相同长度\n",
    "- **序列归一化**：对时间特征进行归一化处理（暂不处理）\n",
    "- **序列排序**：确保 note 按时间顺序排列（好像不需要）\n",
    "\n",
    "**TODO**：\n",
    "- 设计 note 特征的编码方案\n",
    "- 确定最佳的序列长度\n",
    "- 实现序列预处理管道\n",
    "- 考虑是否需要添加全局特征（如 BPM、总时长等）\n",
    "\n",
    "我们已经在自定义数据集中完成了note属性编码。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cff541",
   "metadata": {},
   "source": [
    "#### 特征列含义（共21列）:\n",
    "```\n",
    "  列 0: 时间戳 (time)\n",
    "  列 1: Note类型-Tap\n",
    "  列 2: Note类型-Hold\n",
    "  列 3: Note类型-Slide\n",
    "  列 4: Note类型-Touch\n",
    "  列 5: Note类型-TouchHold\n",
    "  列 6: 位置-1\n",
    "  列 7: 位置-2\n",
    "  列 8: 位置-3\n",
    "  列 9: 位置-4\n",
    "  列10: 位置-5\n",
    "  列11: 位置-6\n",
    "  列12: 位置-7\n",
    "  列13: 位置-8\n",
    "  列14: hold_time\n",
    "  列15: is_break\n",
    "  列16: is_ex\n",
    "  列17: is_slide_break\n",
    "  列18: slide_start_time\n",
    "  列19: slide_end_time\n",
    "  列20: touch_area\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7762880",
   "metadata": {},
   "source": [
    "### 3.1.1 缓存机制"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c47f399",
   "metadata": {},
   "source": [
    "\n",
    "在 `MaichartDataset` 类的 `__getitem__` 方法中实现了一个高效的缓存机制：\n",
    "\n",
    "**缓存特性：**\n",
    "- **LRU淘汰策略**：当缓存达到最大容量时，自动删除最久未使用的数据\n",
    "- **内存优化**：缓存处理后的tensor数据，避免重复的JSON解析和特征提取\n",
    "- **访问统计**：提供缓存使用情况的统计信息\n",
    "\n",
    "**缓存工作流程：**\n",
    "1. 检查请求的数据是否在缓存中\n",
    "2. 如果缓存命中，直接返回缓存数据并更新访问顺序\n",
    "3. 如果缓存未命中，读取JSON文件，处理数据，存入缓存\n",
    "4. 当缓存满时，使用LRU策略删除最久未使用的项\n",
    "\n",
    "**性能优势：**\n",
    "- 减少重复的文件I/O操作\n",
    "- 避免重复的JSON解析\n",
    "- 减少特征提取的计算开销\n",
    "- 特别适合训练时的多轮epoch访问"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e00a9a9",
   "metadata": {},
   "source": [
    "### 3.1.2 缓存使用建议\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b0d421",
   "metadata": {},
   "source": [
    "\n",
    "**缓存大小配置：**\n",
    "- **小型数据集**：可以设置较大的缓存大小，甚至缓存整个数据集\n",
    "- **大型数据集**：根据可用内存设置合理的缓存大小，建议设置为数据集大小的10%-30%\n",
    "- **内存限制**：每个缓存项包含处理后的tensor，需要考虑内存占用\n",
    "\n",
    "**适用场景：**\n",
    "- ✅ **训练阶段**：多个epoch重复访问相同数据，缓存效果显著\n",
    "- ✅ **随机采样**：DataLoader使用随机采样时，热点数据会被重复访问\n",
    "- ✅ **调试阶段**：频繁访问少量样本进行测试\n",
    "- ❌ **单次遍历**：如果数据只被访问一次，缓存意义不大"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60d6303",
   "metadata": {},
   "source": [
    "### 3.2 性能测试和验证\n",
    "\n",
    "让我们测试向量化实现的性能提升，并验证结果的正确性："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0ec48956",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "class MaichartDatasetOld(Dataset):\n",
    "    \"\"\"保留原始实现用于性能对比\"\"\"\n",
    "    def __init__(self, serialized_dir, labels_csv):\n",
    "        self.serialized_dir = serialized_dir\n",
    "        self.labels_data = pd.read_csv(labels_csv)\n",
    "        self.labels_data = self.labels_data.dropna(subset=['song_id', 'level_index', 'difficulty_constant', 'json_filename'])\n",
    "        self.labels_data = self.labels_data.reset_index(drop=True)\n",
    "        self.touch_area_mapping = {\" \": 0, \"A\": 1, \"D\": 2, \"E\": 3, \"B\": 4, \"C\": 5}\n",
    "        self._setup_encoders()\n",
    "\n",
    "    def _setup_encoders(self):\n",
    "        self.NOTE_TYPES = ['Tap', 'Hold', 'Slide', 'Touch', 'TouchHold']\n",
    "        self.note_type_encoder = OneHotEncoder(sparse_output=False, dtype=np.float32, handle_unknown='ignore')\n",
    "        self.note_type_encoder.fit(np.array(self.NOTE_TYPES).reshape(-1, 1))\n",
    "        \n",
    "        self.positions = list(range(1, 9))\n",
    "        self.position_encoder = OneHotEncoder(sparse_output=False, dtype=np.float32, handle_unknown='ignore')\n",
    "        self.position_encoder.fit(np.array(self.positions).reshape(-1, 1))\n",
    "\n",
    "    def _extract_note_features(self, note, time):\n",
    "        \"\"\"原始的单个note特征提取方法\"\"\"\n",
    "        note_type_encoded = self.note_type_encoder.transform([[note['noteType']]])[0]\n",
    "        position_encoded = self.position_encoder.transform([[note['startPosition']]])[0]\n",
    "        \n",
    "        hold_time = note.get('holdTime', 0)\n",
    "        is_break = int(note['isBreak'])\n",
    "        is_ex = int(note['isEx'])\n",
    "        is_slide_break = int(note['isSlideBreak'])\n",
    "        slide_start_time = note['slideStartTime']\n",
    "        slide_end_time = slide_start_time + note['slideTime']\n",
    "        touch_area = self.touch_area_mapping[note['touchArea']]\n",
    "        \n",
    "        feature_vector = np.concatenate([\n",
    "            [time], note_type_encoded, position_encoded,\n",
    "            [hold_time], [is_break], [is_ex], [is_slide_break],\n",
    "            [slide_start_time], [slide_end_time], [touch_area]\n",
    "        ])\n",
    "        return feature_vector\n",
    "\n",
    "    def _extract_sequence_features_old(self, json_data):\n",
    "        \"\"\"原始的循环实现\"\"\"\n",
    "        note_groups = json_data.get('notes', [])\n",
    "        note_features_sequence = []\n",
    "        \n",
    "        for note_group in note_groups:\n",
    "            time = note_group['Time']\n",
    "            notes = note_group['Notes']\n",
    "            \n",
    "            for note in notes:\n",
    "                feature_vector = self._extract_note_features(note, time)\n",
    "                note_features_sequence.append(feature_vector)\n",
    "        \n",
    "        return note_features_sequence\n",
    "\n",
    "def performance_comparison_test():\n",
    "    \"\"\"对比原始方法和向量化方法的性能\"\"\"\n",
    "    base_dir = os.path.dirname(os.path.abspath(''))\n",
    "    serialized_dir = os.path.join(base_dir, \"data\", \"serialized\")\n",
    "    csv_path = os.path.join(base_dir, \"data\", \"song_info.csv\")\n",
    "    \n",
    "    # 创建两个数据集实例\n",
    "    dataset_new = MaichartDataset(serialized_dir, csv_path)\n",
    "    dataset_old = MaichartDatasetOld(serialized_dir, csv_path)\n",
    "    \n",
    "    # 选择测试样本\n",
    "    test_indices = list(range(min(10, len(dataset_new))))  # 测试前10个样本\n",
    "    \n",
    "    print(\"性能对比测试开始...\")\n",
    "    print(f\"测试样本数量: {len(test_indices)}\")\n",
    "    \n",
    "    # 测试原始方法\n",
    "    start_time = time.time()\n",
    "    old_results = []\n",
    "    for idx in test_indices:\n",
    "        try:\n",
    "            row = dataset_old.labels_data.iloc[idx]\n",
    "            json_filename = row['json_filename']\n",
    "            json_file_path = os.path.join(serialized_dir, json_filename)\n",
    "            \n",
    "            with open(json_file_path, 'r', encoding='utf-8') as f:\n",
    "                json_data = json.load(f)\n",
    "            \n",
    "            features = dataset_old._extract_sequence_features_old(json_data)\n",
    "            old_results.append(np.array(features))\n",
    "        except Exception as e:\n",
    "            print(f\"原始方法处理样本 {idx} 时出错: {e}\")\n",
    "            continue\n",
    "    \n",
    "    old_time = time.time() - start_time\n",
    "    \n",
    "    # 测试向量化方法\n",
    "    start_time = time.time()\n",
    "    new_results = []\n",
    "    for idx in test_indices:\n",
    "        try:\n",
    "            row = dataset_new.labels_data.iloc[idx]\n",
    "            json_filename = row['json_filename']\n",
    "            json_file_path = os.path.join(serialized_dir, json_filename)\n",
    "            \n",
    "            with open(json_file_path, 'r', encoding='utf-8') as f:\n",
    "                json_data = json.load(f)\n",
    "            \n",
    "            features = dataset_new._extract_sequence_features_vectorized(json_data)\n",
    "            new_results.append(features)\n",
    "        except Exception as e:\n",
    "            print(f\"向量化方法处理样本 {idx} 时出错: {e}\")\n",
    "            continue\n",
    "    \n",
    "    new_time = time.time() - start_time\n",
    "    \n",
    "    # 性能结果\n",
    "    print(f\"\\n性能对比结果:\")\n",
    "    print(f\"原始方法耗时: {old_time:.4f} 秒\")\n",
    "    print(f\"向量化方法耗时: {new_time:.4f} 秒\")\n",
    "    print(f\"加速比: {old_time/new_time:.2f}x\")\n",
    "    \n",
    "    # 验证结果正确性\n",
    "    print(f\"\\n正确性验证:\")\n",
    "    if len(old_results) == len(new_results):\n",
    "        all_close = True\n",
    "        for i, (old_feat, new_feat) in enumerate(zip(old_results, new_results)):\n",
    "            if not np.allclose(old_feat, new_feat, rtol=1e-5):\n",
    "                print(f\"样本 {i} 结果不一致!\")\n",
    "                print(f\"  原始方法形状: {old_feat.shape}\")\n",
    "                print(f\"  向量化方法形状: {new_feat.shape}\")\n",
    "                all_close = False\n",
    "        \n",
    "        if all_close:\n",
    "            print(\"✓ 所有测试样本的结果完全一致!\")\n",
    "        else:\n",
    "            print(\"✗ 发现结果不一致的样本\")\n",
    "    else:\n",
    "        print(f\"✗ 处理成功的样本数量不一致: 原始={len(old_results)}, 向量化={len(new_results)}\")\n",
    "\n",
    "# 运行性能测试\n",
    "# performance_comparison_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dcec0d",
   "metadata": {},
   "source": [
    "### 3.3 向量化优化要点总结\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb230dbc",
   "metadata": {},
   "source": [
    "\n",
    "**主要优化策略**：\n",
    "\n",
    "1. **批量编码代替逐个编码**：\n",
    "   - 原始：对每个note分别调用`OneHotEncoder.transform([[value]])`\n",
    "   - 优化：收集所有note数据，一次性调用`OneHotEncoder.transform(all_values)`\n",
    "   - 效果：减少了大量的函数调用开销\n",
    "\n",
    "2. **向量化数组操作**：\n",
    "   - 原始：使用Python循环和`np.concatenate`逐个拼接特征\n",
    "   - 优化：使用`np.column_stack`一次性拼接所有特征列\n",
    "   - 效果：利用NumPy的C语言底层实现，大幅提升性能\n",
    "\n",
    "3. **内存访问优化**：\n",
    "   - 原始：多次小数组的创建和拼接\n",
    "   - 优化：预先分配大数组，减少内存分配次数\n",
    "   - 效果：更好的内存局部性和缓存命中率\n",
    "\n",
    "4. **减少中间变量**：\n",
    "   - 原始：每个note创建一个中间`feature_vector`\n",
    "   - 优化：直接构建最终的特征矩阵\n",
    "   - 效果：减少内存开销和垃圾回收压力\n",
    "\n",
    "**预期性能提升**：\n",
    "- 对于包含大量notes的谱面，预期可获得 **5-20倍** 的性能提升\n",
    "- 实际提升幅度取决于谱面的note密度和硬件配置\n",
    "\n",
    "**兼容性保证**：\n",
    "- 输出结果与原始方法完全一致\n",
    "- 可直接替换原有实现，无需修改下游代码"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4626766e",
   "metadata": {},
   "source": [
    "### 3.4 Collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3b7b439b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    自定义的collate_fn，用于处理变长序列。\n",
    "    - 对note序列进行padding，使其在batch内长度一致。\n",
    "    - 将标签堆叠成一个tensor。\n",
    "    \"\"\"\n",
    "    # 1. 分离序列和标签\n",
    "    # batch中的每个元素是 (note_features_tensor, difficulty_constant_tensor)\n",
    "    sequences, labels = zip(*batch)\n",
    "\n",
    "    # 2. 对序列进行padding\n",
    "    # pad_sequence期望一个tensor列表\n",
    "    # batch_first=True使输出的形状为 (batch_size, sequence_length, feature_dim)\n",
    "    padded_sequences = pad_sequence(sequences, batch_first=True, padding_value=0.0)\n",
    "\n",
    "    # 3. 将标签堆叠成一个tensor\n",
    "    # torch.stack(labels) 会创建一个 [batch_size] 的1D张量\n",
    "    # .view(-1, 1) 将其转换为 [batch_size, 1] 以匹配模型输出\n",
    "    labels_tensor = torch.stack(labels).view(-1, 1)\n",
    "\n",
    "    return padded_sequences, labels_tensor\n",
    "\n",
    "class CollateWithStatsWrapper:\n",
    "    \"\"\"\n",
    "    包装器类，允许在需要时获取统计信息，同时保持与现有代码的兼容性\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.last_stats = None\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        if not batch:\n",
    "            raise ValueError(\"batch为0\")\n",
    "        sequences, labels = zip(*batch)\n",
    "        \n",
    "        # 收集统计信息\n",
    "        seq_lengths = [seq.size(0) for seq in sequences]\n",
    "        stats = {\n",
    "            'batch_size': len(sequences),\n",
    "            'min_seq_length': min(seq_lengths),\n",
    "            'max_seq_length': max(seq_lengths),\n",
    "            'avg_seq_length': sum(seq_lengths) / len(seq_lengths),\n",
    "            'total_notes': sum(seq_lengths),\n",
    "            'padding_ratio': 0\n",
    "        }\n",
    "        \n",
    "        # Padding\n",
    "        padded_sequences = pad_sequence(sequences, batch_first=True, padding_value=0.0)\n",
    "        labels_tensor = torch.stack(labels).view(-1, 1)\n",
    "        \n",
    "        # 计算padding比例\n",
    "        total_elements = padded_sequences.numel()\n",
    "        padding_elements = (padded_sequences == 0).sum().item()\n",
    "        stats['padding_ratio'] = padding_elements / total_elements if total_elements > 0 else 0\n",
    "        \n",
    "        # 存储统计信息\n",
    "        self.last_stats = stats\n",
    "        \n",
    "        return padded_sequences, labels_tensor\n",
    "    \n",
    "    def get_last_stats(self):\n",
    "        \"\"\"获取最后一个batch的统计信息\"\"\"\n",
    "        return self.last_stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a273d1",
   "metadata": {},
   "source": [
    "### 3.5 序列长度分布分析\n",
    "\n",
    "**问题发现**: 平均padding比例高达92.1%，这表明数据中存在序列长度极度不均匀的问题。\n",
    "\n",
    "**可能原因**:\n",
    "1. **数据中包含极长的序列**：少数极长谱面导致整体padding过多\n",
    "2. **特征维度错误**：实际特征维度与预期不符\n",
    "3. **数据处理错误**：序列提取过程中可能存在问题\n",
    "\n",
    "**解决方案**:\n",
    "1. 分析序列长度分布，找出异常值\n",
    "2. 使用动态批处理策略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "16979d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sequence_lengths():\n",
    "    \"\"\"\n",
    "    分析数据集中序列长度的分布，找出padding比例过高的原因\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    base_dir = os.path.dirname(os.path.abspath(''))\n",
    "    serialized_dir = os.path.join(base_dir, \"data\", \"serialized\")\n",
    "    csv_path = os.path.join(base_dir, \"data\", \"song_info.csv\")\n",
    "    \n",
    "    dataset = MaichartDataset(serialized_dir, csv_path)\n",
    "    \n",
    "    print(\"=== 序列长度分布分析 ===\")\n",
    "    print(f\"数据集总大小: {len(dataset)}\")\n",
    "    \n",
    "    # 收集序列长度统计\n",
    "    sequence_lengths = []\n",
    "    feature_dims = []\n",
    "    sample_count = min(100, len(dataset))  # 分析前100个样本\n",
    "    \n",
    "    print(f\"分析前 {sample_count} 个样本...\")\n",
    "    \n",
    "    for i in range(sample_count):\n",
    "        try:\n",
    "            note_features, difficulty = dataset[i]\n",
    "            seq_len = note_features.shape[0]\n",
    "            feat_dim = note_features.shape[1] if len(note_features.shape) > 1 else 0\n",
    "            \n",
    "            sequence_lengths.append(seq_len)\n",
    "            feature_dims.append(feat_dim)\n",
    "            \n",
    "            if i < 10:  # 显示前10个样本的详细信息\n",
    "                print(f\"  样本 {i}: 序列长度={seq_len}, 特征维度={feat_dim}, 难度={difficulty:.2f}\")\n",
    "\n",
    "            if i < 3: # 显示前3个样本的特征矩阵\n",
    "                print(f\"  样本 {i} 特征矩阵:\\n{note_features.numpy()}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  样本 {i} 处理出错: {e}\")\n",
    "            sequence_lengths.append(0)\n",
    "            feature_dims.append(0)\n",
    "    \n",
    "    # 统计分析\n",
    "    sequence_lengths = np.array(sequence_lengths)\n",
    "    feature_dims = np.array(feature_dims)\n",
    "    \n",
    "    print(f\"\\n序列长度统计:\")\n",
    "    print(f\"  最小长度: {np.min(sequence_lengths)}\")\n",
    "    print(f\"  最大长度: {np.max(sequence_lengths)}\")\n",
    "    print(f\"  平均长度: {np.mean(sequence_lengths):.1f}\")\n",
    "    print(f\"  中位数长度: {np.median(sequence_lengths):.1f}\")\n",
    "    print(f\"  标准差: {np.std(sequence_lengths):.1f}\")\n",
    "    \n",
    "    print(f\"\\n特征维度统计:\")\n",
    "    print(f\"  特征维度: {np.unique(feature_dims)}\")\n",
    "\n",
    "\n",
    "    \n",
    "    # 计算不同批次大小的padding比例\n",
    "    batch_sizes = [4, 8, 16, 32]\n",
    "    print(f\"\\n不同批次大小的padding分析:\")\n",
    "    \n",
    "    for batch_size in batch_sizes:\n",
    "        total_padding_ratio = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for i in range(0, len(sequence_lengths), batch_size):\n",
    "            batch_lengths = sequence_lengths[i:i+batch_size]\n",
    "            if len(batch_lengths) == 0:\n",
    "                continue\n",
    "                \n",
    "            max_len = np.max(batch_lengths)\n",
    "            total_elements = len(batch_lengths) * max_len\n",
    "            actual_elements = np.sum(batch_lengths)\n",
    "            \n",
    "            if total_elements > 0:\n",
    "                padding_ratio = 1 - (actual_elements / total_elements)\n",
    "                total_padding_ratio += padding_ratio\n",
    "                num_batches += 1\n",
    "        \n",
    "        avg_padding = total_padding_ratio / num_batches if num_batches > 0 else 0\n",
    "        print(f\"  批次大小 {batch_size}: 平均padding比例 {avg_padding:.3f}\")\n",
    "    \n",
    "    # 找出异常长的序列\n",
    "    print(f\"\\n异常长序列分析:\")\n",
    "    percentile_95 = np.percentile(sequence_lengths, 95)\n",
    "    percentile_99 = np.percentile(sequence_lengths, 99)\n",
    "    \n",
    "    print(f\"  95%分位数: {percentile_95:.1f}\")\n",
    "    print(f\"  99%分位数: {percentile_99:.1f}\")\n",
    "    \n",
    "    long_sequences = sequence_lengths[sequence_lengths > percentile_95]\n",
    "    print(f\"  超过95%分位数的序列数量: {len(long_sequences)}\")\n",
    "    print(f\"  这些序列长度: {sorted(long_sequences)}\")\n",
    "    \n",
    "    return sequence_lengths, feature_dims\n",
    "\n",
    "# 运行分析\n",
    "# seq_lengths, feat_dims = analyze_sequence_lengths()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f2f54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "level_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "69201b99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ec37d65",
   "metadata": {},
   "source": [
    "## 4. LSTM 模型构建与数据准备\n",
    "\n",
    "构建基于 LSTM 的时序模型来处理 note 序列数据。模型将接收形状为 `(batch_size, sequence_length, feature_dim)` 的输入，输出难度定数的预测值。\n",
    "\n",
    "**模型架构设计**：\n",
    "- **输入层**：接收编码后的 note 序列\n",
    "- **LSTM层**：捕捉序列中的时序依赖关系\n",
    "- **全连接层**：将 LSTM 输出映射到难度预测\n",
    "- **输出层**：回归输出，预测难度定数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57606c6",
   "metadata": {},
   "source": [
    "### 4.1 定义 LSTM 模型架构\n",
    "\n",
    "**模型设计考虑**：\n",
    "- **多层 LSTM**：评估单层 vs 多层 LSTM 的效果\n",
    "- **Dropout**：防止过拟合\n",
    "- **Attention 机制**：突出重要的 note 序列部分\n",
    "\n",
    "**TODO**：\n",
    "- 实现基础的 LSTM 模型类\n",
    "- 设计模型的超参数（hidden_size, num_layers, dropout_rate）\n",
    "- 考虑添加注意力机制\n",
    "- 实验不同的模型架构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3e18abac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(SimpleLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        # 取最后一个时间步的输出\n",
    "        last_out = lstm_out[:, -1, :]\n",
    "        output = self.fc(last_out)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83eeaf44",
   "metadata": {},
   "source": [
    "## 5. 模型训练与优化\n",
    "\n",
    "**训练策略**：\n",
    "- **损失函数**：使用 MSE 或 MAE 损失函数（回归任务）\n",
    "- **优化器**：Adam 优化器，考虑学习率调度\n",
    "- **批次处理**：合理设置 batch_size 处理变长序列\n",
    "- **正则化**：Dropout + L2 正则化防止过拟合\n",
    "\n",
    "**训练监控**：\n",
    "- 训练损失和验证损失曲线\n",
    "- 早停机制防止过拟合\n",
    "- 学习率衰减策略\n",
    "\n",
    "**TODO**：\n",
    "- 实现训练循环\n",
    "- 设置验证集监控\n",
    "- 实现早停和模型保存机制\n",
    "- 调试序列批次处理中的 padding 问题\n",
    "- 优化训练超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "07f4c4d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "实验配置: {'model_type': 'SimpleLSTM', 'input_size': 21, 'hidden_size': 64, 'output_size': 1, 'num_layers': 2, 'batch_size': 16, 'learning_rate': 0.001, 'num_epochs': 50, 'early_stop_patience': 10}\n",
      "================================================================================\n",
      "创建数据集...\n",
      "数据集创建完成 (0.017s)\n",
      "训练集大小: 4380, 测试集大小: 1096\n",
      "\n",
      "创建数据加载器...\n",
      "数据加载器创建完成 (0.000s)\n",
      "测试批次数: 69\n",
      "\n",
      "创建模型...\n",
      "模型创建完成 (0.000s)\n",
      "模型参数数量: 55,617\n",
      "模型设备: cpu\n",
      "\n",
      "Epoch [1/50] - 训练阶段\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[56]\u001b[39m\u001b[32m, line 310\u001b[39m\n\u001b[32m    307\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model, train_losses, val_losses, train_results, test_results\n\u001b[32m    309\u001b[39m \u001b[38;5;66;03m# 运行完整的训练和评估流程\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m model, train_losses, val_losses, train_results, test_results = \u001b[43mtrain_complete_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[56]\u001b[39m\u001b[32m, line 290\u001b[39m, in \u001b[36mtrain_complete_pipeline\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    287\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m模型设备: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mnext\u001b[39m(model.parameters()).device\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    289\u001b[39m \u001b[38;5;66;03m# 训练模型\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m290\u001b[39m train_losses, val_losses = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    292\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mnum_epochs\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    293\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlearning_rate\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[38;5;66;03m# 加载最佳模型并评估\u001b[39;00m\n\u001b[32m    297\u001b[39m model.load_state_dict(torch.load(os.path.join(MODEL_DIR, \u001b[33m\"\u001b[39m\u001b[33mbest_model.pth\u001b[39m\u001b[33m\"\u001b[39m)))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[56]\u001b[39m\u001b[32m, line 55\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, val_loader, num_epochs, learning_rate)\u001b[39m\n\u001b[32m     53\u001b[39m outputs = model(sequences)\n\u001b[32m     54\u001b[39m loss = criterion(outputs, labels)\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# 梯度裁剪防止梯度爆炸\u001b[39;00m\n\u001b[32m     58\u001b[39m torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=\u001b[32m1.0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\wushuopei\\code\\BMK-mdp\\.venv\\Lib\\site-packages\\torch\\_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\wushuopei\\code\\BMK-mdp\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\wushuopei\\code\\BMK-mdp\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def train_model(model, train_loader, val_loader, num_epochs=50, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    改进的模型训练函数，包含验证集监控、早停机制和学习率调度。\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): 要训练的 PyTorch 模型\n",
    "        train_loader (DataLoader): 训练数据加载器，包含 (sequences, labels) 或 (sequences, labels, padding_mask)\n",
    "        val_loader (DataLoader): 验证数据加载器，格式同训练数据加载器\n",
    "        num_epochs (int, optional): 最大训练轮数。默认为 50\n",
    "        learning_rate (float, optional): 初始学习率。默认为 0.001\n",
    "    \n",
    "    Returns:\n",
    "        tuple[list, list]: 包含两个列表的元组\n",
    "            - train_losses (list): 每个 epoch 的平均训练损失\n",
    "            - val_losses (list): 每个 epoch 的平均验证损失\n",
    "    \n",
    "    Training Strategy:\n",
    "        - Loss Function: MSE (均方误差) 用于回归任务\n",
    "        - Optimizer: Adam 优化器，包含 L2 正则化 (weight_decay=1e-5)\n",
    "        - Scheduler: ReduceLROnPlateau，验证损失停止改善时降低学习率\n",
    "        - Early Stopping: 连续 10 个 epoch 验证损失无改善时停止训练\n",
    "        - Gradient Clipping: 最大梯度范数限制为 1.0，防止梯度爆炸\n",
    "    \n",
    "    Model Checkpointing:\n",
    "        - 自动保存验证损失最低的模型权重到 'best_model.pth'\n",
    "        - 训练结束后可通过 model.load_state_dict(torch.load('best_model.pth')) 加载最佳模型\n",
    "    \"\"\"\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    early_stop_patience = 10\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    # 总训练开始时间\n",
    "    total_start_time = time.time()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # 训练阶段\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_batch_times = []\n",
    "        print(f\"\\nEpoch [{epoch+1}/{num_epochs}] - 训练阶段\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        for batch_idx, (sequences, labels) in enumerate(train_loader):\n",
    "            batch_start_time = time.time()\n",
    "            \n",
    "            # 将数据移动到 GPU\n",
    "            sequences, labels = sequences.to(DEVICE), labels.to(DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(sequences)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            \n",
    "            # 梯度裁剪防止梯度爆炸\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            batch_time = time.time() - batch_start_time\n",
    "            train_batch_times.append(batch_time)\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            # 每10个batch输出一次进度（可调整频率）\n",
    "            if (batch_idx + 1) % 10 == 0 or (batch_idx + 1) == len(train_loader):\n",
    "                avg_batch_time = np.mean(train_batch_times[-10:])  # 最近10个batch的平均时间\n",
    "                print(f\"  Batch [{batch_idx+1:4d}/{len(train_loader):4d}] | \"\n",
    "                      f\"Loss: {loss.item():.4f} | \"\n",
    "                      f\"Batch Time: {batch_time:.2f}s | \"\n",
    "                      f\"Avg Time: {avg_batch_time:.2f}s | \"\n",
    "                      f\"Seq Shape: {sequences.shape}\")\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # === 验证阶段 ===\n",
    "        print(f\"\\nEpoch [{epoch+1}/{num_epochs}] - 验证阶段\")\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for sequences, labels in val_loader:\n",
    "                # 将数据移动到 GPU\n",
    "                sequences, labels = sequences.to(DEVICE), labels.to(DEVICE)\n",
    "                \n",
    "                outputs = model(sequences)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        # 学习率调度\n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        # 早停检查\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            # 保存最佳模型\n",
    "            model_save_path = os.path.join(MODEL_DIR, \"best_model.pth\")\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            print(f\"新的最佳模型已保存 (验证损失: {best_val_loss:.6f})\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"早停计数器: {patience_counter}/{early_stop_patience}\")\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        # 估算剩余时间\n",
    "        elapsed_time = time.time() - total_start_time\n",
    "        avg_epoch_time = elapsed_time / (epoch + 1)\n",
    "        remaining_epochs = num_epochs - (epoch + 1)\n",
    "        estimated_remaining_time = avg_epoch_time * remaining_epochs\n",
    "        \n",
    "        print(f\"  已用时间: {elapsed_time/60:.1f}分钟 | 预计剩余: {estimated_remaining_time/60:.1f}分钟\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        if patience_counter >= early_stop_patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "        \n",
    "    \n",
    "    total_training_time = time.time() - total_start_time\n",
    "    print(f\"\\n🎉 训练完成!\")\n",
    "    print(f\"总训练时间: {total_training_time/60:.1f} 分钟\")\n",
    "    print(f\"最佳验证损失: {best_val_loss:.6f}\")\n",
    "    \n",
    "    return train_losses, val_losses\n",
    "\n",
    "def evaluate_model(model, data_loader):\n",
    "    \"\"\"全面评估模型性能\"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_values = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sequences, labels in data_loader:\n",
    "            outputs = model(sequences)\n",
    "            predictions.extend(outputs.cpu().numpy().flatten())\n",
    "            true_values.extend(labels.cpu().numpy().flatten())\n",
    "    \n",
    "    predictions = np.array(predictions)\n",
    "    true_values = np.array(true_values)\n",
    "    \n",
    "    # 计算各种指标\n",
    "    mse = np.mean((predictions - true_values) ** 2)\n",
    "    mae = np.mean(np.abs(predictions - true_values))\n",
    "    r2 = 1 - np.sum((true_values - predictions) ** 2) / np.sum((true_values - np.mean(true_values)) ** 2)\n",
    "    \n",
    "    # 准确性分析（在不同误差范围内的比例）\n",
    "    accuracy_01 = np.mean(np.abs(predictions - true_values) <= 0.1)\n",
    "    accuracy_02 = np.mean(np.abs(predictions - true_values) <= 0.2)\n",
    "    accuracy_05 = np.mean(np.abs(predictions - true_values) <= 0.5)\n",
    "    \n",
    "    print(f\"评估结果:\")\n",
    "    print(f\"  MSE: {mse:.4f}\")\n",
    "    print(f\"  MAE: {mae:.4f}\")\n",
    "    print(f\"  R²: {r2:.4f}\")\n",
    "    print(f\"  ±0.1准确率: {accuracy_01:.3f}\")\n",
    "    print(f\"  ±0.2准确率: {accuracy_02:.3f}\")\n",
    "    print(f\"  ±0.5准确率: {accuracy_05:.3f}\")\n",
    "    \n",
    "    return {\n",
    "        'mse': mse, 'mae': mae, 'r2': r2,\n",
    "        'acc_01': accuracy_01, 'acc_02': accuracy_02, 'acc_05': accuracy_05,\n",
    "        'predictions': predictions, 'true_values': true_values\n",
    "    }\n",
    "\n",
    "import json\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "\n",
    "def save_experiment_log(model, train_losses, val_losses, train_results, test_results, config):\n",
    "    \"\"\"保存实验记录\"\"\"\n",
    "    \n",
    "    # 创建实验日志目录\n",
    "    log_dir = os.path.join(BASE_DIR, \"experiment_logs\")\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    \n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    # 实验配置和结果\n",
    "    experiment_log = {\n",
    "        'timestamp': timestamp,\n",
    "        'model_config': config,\n",
    "        'model_parameters': sum(p.numel() for p in model.parameters()),\n",
    "        'training': {\n",
    "            'num_epochs': len(train_losses),\n",
    "            'final_train_loss': train_losses[-1],\n",
    "            'final_val_loss': val_losses[-1],\n",
    "            'best_val_loss': min(val_losses),\n",
    "            'train_losses': train_losses,\n",
    "            'val_losses': val_losses\n",
    "        },\n",
    "        'evaluation': {\n",
    "            'train_results': {k: float(v) if isinstance(v, (int, float, np.number)) else str(v) \n",
    "                            for k, v in train_results.items() if k not in ['predictions', 'true_values']},\n",
    "            'test_results': {k: float(v) if isinstance(v, (int, float, np.number)) else str(v) \n",
    "                           for k, v in test_results.items() if k not in ['predictions', 'true_values']}\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # 保存日志\n",
    "    log_file = os.path.join(log_dir, f\"experiment_{timestamp}.json\")\n",
    "    with open(log_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(experiment_log, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"实验日志已保存: {log_file}\")\n",
    "    return log_file\n",
    "\n",
    "def train_complete_pipeline():\n",
    "\n",
    "    # 实验配置\n",
    "    config = {\n",
    "        'model_type': 'SimpleLSTM',\n",
    "        'input_size': 21,\n",
    "        'hidden_size': 64,\n",
    "        'output_size': 1,\n",
    "        'num_layers': 2,\n",
    "        'batch_size': 16,\n",
    "        'learning_rate': 0.001,\n",
    "        'num_epochs': 50,\n",
    "        'early_stop_patience': 10\n",
    "    }\n",
    "    print(f\"实验配置: {config}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # 创建数据集\n",
    "    print(\"创建数据集...\")\n",
    "    dataset_start_time = time.time()\n",
    "    train_dataset = MaichartDataset(SERIALIZED_DIR, TRAIN_DATA_PATH)\n",
    "    test_dataset = MaichartDataset(SERIALIZED_DIR, TEST_DATA_PATH)\n",
    "    dataset_time = time.time() - dataset_start_time\n",
    "    print(f\"数据集创建完成 ({dataset_time:.3f}s)\")\n",
    "    print(f\"训练集大小: {len(train_dataset)}, 测试集大小: {len(test_dataset)}\")\n",
    "\n",
    "    # 创建数据加载器\n",
    "    print(\"\\n创建数据加载器...\")\n",
    "    loader_start_time = time.time()\n",
    "\n",
    "    # 使用分桶采样器创建训练数据加载器\n",
    "    # 注意：使用 batch_sampler 时，DataLoader的 batch_size, shuffle, sampler, drop_last 参数必须为默认值\n",
    "    # train_sampler = LevelIndexBucketSampler(\n",
    "    #     train_dataset,\n",
    "    #     batch_size=config['batch_size'],\n",
    "    #     shuffle=True,\n",
    "    #     drop_last=True # 在训练时丢弃不完整的batch通常是好的实践\n",
    "    # )\n",
    "    # train_loader = DataLoader(\n",
    "    #     train_dataset,\n",
    "    #     batch_sampler=train_sampler,\n",
    "    #     collate_fn=collate_fn,\n",
    "    #     num_workers=0   \n",
    "    # )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=0\n",
    "    )\n",
    "\n",
    "    # 测试加载器不需要分桶或打乱\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=0\n",
    "    )\n",
    "    loader_time = time.time() - loader_start_time\n",
    "    print(f\"数据加载器创建完成 ({loader_time:.3f}s)\")\n",
    "    print(f\"测试批次数: {len(test_loader)}\")\n",
    "\n",
    "    # 创建模型\n",
    "    print(\"\\n创建模型...\")\n",
    "    model_start_time = time.time()\n",
    "    model = SimpleLSTM(\n",
    "        config['input_size'], \n",
    "        config['hidden_size'], \n",
    "        config['output_size'], \n",
    "        config['num_layers']\n",
    "    )\n",
    "    model_time = time.time() - model_start_time\n",
    "    param_count = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"模型创建完成 ({model_time:.3f}s)\")\n",
    "    print(f\"模型参数数量: {param_count:,}\")\n",
    "    print(f\"模型设备: {next(model.parameters()).device}\")\n",
    "\n",
    "    # 训练模型\n",
    "    train_losses, val_losses = train_model(\n",
    "        model, train_loader, test_loader, \n",
    "        num_epochs=config['num_epochs'], \n",
    "        learning_rate=config['learning_rate']\n",
    "    )\n",
    "    \n",
    "    # 加载最佳模型并评估\n",
    "    model.load_state_dict(torch.load(os.path.join(MODEL_DIR, \"best_model.pth\")))\n",
    "    print(\"\\n=== 训练集评估 ===\")\n",
    "    train_results = evaluate_model(model, train_loader)\n",
    "    print(\"\\n=== 测试集评估 ===\")\n",
    "    test_results = evaluate_model(model, test_loader)\n",
    "\n",
    "    # 保存实验记录\n",
    "    log_file = save_experiment_log(model, train_losses, val_losses, train_results, test_results, config)\n",
    "\n",
    "\n",
    "    return model, train_losses, val_losses, train_results, test_results\n",
    "\n",
    "# 运行完整的训练和评估流程\n",
    "model, train_losses, val_losses, train_results, test_results = train_complete_pipeline()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e27cee",
   "metadata": {},
   "source": [
    "## 6. 模型评估与性能分析\n",
    "\n",
    "**评估指标**：\n",
    "- **回归指标**：MSE, MAE, R²\n",
    "- **难度区间准确性**：预测值在真实值 ±0.1, ±0.2, ±0.5 范围内的比例\n",
    "- **分布分析**：预测值与真实值的分布对比\n",
    "\n",
    "**详细分析**：\n",
    "- **不同难度等级的预测准确性**：分析模型在低难度 vs 高难度谱面上的表现\n",
    "- **序列长度影响**：分析谱面长度对预测准确性的影响\n",
    "- **错误案例分析**：找出预测偏差较大的谱面特征\n",
    "\n",
    "**TODO**：\n",
    "- 实现全面的评估指标计算\n",
    "- 可视化预测结果分布\n",
    "- 分析不同难度区间的预测准确性\n",
    "- 进行错误案例的深入分析\n",
    "- 与传统特征工程方法进行对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c76b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     predictions = model(X_test_tensor)\n",
    "#     test_loss = criterion(predictions, y_test_tensor)\n",
    "#     print(f'Test Loss: {test_loss.item():.4f}')\n",
    "#\n",
    "# # 可以在这里添加更详细的评估指标，例如 MAE, R^2 等"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f003fee0",
   "metadata": {},
   "source": [
    "## 7. 结果分析与模型迭代\n",
    "\n",
    "**深度分析**：\n",
    "- **时序特征的重要性**：LSTM 是否有效捕捉了时序信息\n",
    "- **不同 note 类型的影响**：哪些类型的 note 对难度预测更重要\n",
    "- **序列长度 vs 准确性**：最优的序列长度设置\n",
    "- **模型复杂度 vs 性能**：单层 vs 多层 LSTM 的权衡\n",
    "\n",
    "**模型优化方向**：\n",
    "- **架构改进**：考虑 Transformer、CNN-LSTM 混合架构\n",
    "- **特征增强**：是否需要添加手工特征作为辅助\n",
    "- **数据增强**：通过时间扭曲、音符变换等方式增加训练数据\n",
    "- **多任务学习**：同时预测难度和其他属性（如技巧需求）\n",
    "\n",
    "**TODO**：\n",
    "- 深入分析 LSTM 学到的时序模式\n",
    "- 可视化注意力权重（如果使用了注意力机制）\n",
    "- 比较不同模型架构的效果\n",
    "- 设计更鲁棒的数据增强策略\n",
    "- 考虑集成学习方法提升性能\n",
    "- 为生产环境部署准备模型压缩和优化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bf24c6",
   "metadata": {},
   "source": [
    "分析模型的预测结果，与真实定数进行比较。\n",
    "\n",
    "思考以下问题：\n",
    "- 模型的误差主要来自哪些谱面？\n",
    "- 是否有必要调整特征工程的方案？\n",
    "- 是否需要更复杂的模型结构？\n",
    "\n",
    "根据分析结果，回到前面的步骤进行迭代优化。\n",
    "\n",
    "**关键思考问题**：\n",
    "\n",
    "1. **时序建模的有效性**：\n",
    "   - LSTM 是否真的比传统统计特征更有效？\n",
    "   - 谱面的时序特征对难度的影响有多大？\n",
    "\n",
    "2. **数据表示的完整性**：\n",
    "   - 当前的 note 编码是否充分表达了游戏的复杂性？\n",
    "   - 是否遗漏了重要的游戏机制信息？\n",
    "\n",
    "3. **模型的可解释性**：\n",
    "   - 如何理解模型学到的难度判断规律？\n",
    "   - 能否提取出可解释的难度评估规则？\n",
    "\n",
    "4. **实际应用价值**：\n",
    "   - 模型的预测精度是否满足实际需求？\n",
    "   - 如何将模型集成到谱面制作工具中？\n",
    "\n",
    "**下一步迭代方向**：\n",
    "根据实验结果，有针对性地改进数据处理、模型架构或训练策略，最终目标是构建一个既准确又实用的难度预测系统。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
