{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95f98cbe",
   "metadata": {},
   "source": [
    "# maimai 谱面难度预测 - 基于 LSTM 的时序建模\n",
    "\n",
    "本项目使用 LSTM 神经网络直接处理谱面的 note 序列数据，将每个 note 的时间戳和类型等信息作为时序特征输入模型，预测谱面的难度定数。\n",
    "\n",
    "**核心思路**：将谱面视为时间序列，每个 note 包含时间戳、类型、位置等属性，通过 LSTM 学习 note 序列的时序特征来预测难度。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79795d1",
   "metadata": {},
   "source": [
    "## 1. 导入所需库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a4e1287",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import csv\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db9c020",
   "metadata": {},
   "source": [
    "## 2. 数据处理与序列化\n",
    "\n",
    "数据处理分为两个主要步骤：\n",
    "1. **谱面解析**：将 maidata.txt 格式解析为结构化的 note 序列数据\n",
    "2. **序列预处理**：将 note 序列转换为适合 LSTM 输入的格式\n",
    "\n",
    "**核心理念**：每个谱面是一个时间序列，包含按时间顺序排列的 note 序列。每个 note 具有时间戳、类型、位置等属性。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7bf94d",
   "metadata": {},
   "source": [
    "### 2.1 解析 maidata.txt\n",
    "\n",
    "我们使用外部工具 `SimaiSerializerFromMajdataEdit.exe` 来将 `maidata.txt` 格式的谱面文件解析并序列化为 JSON 文件。\n",
    "\n",
    "数据来源：maichart-converts\n",
    "\n",
    "**使用方法:**\n",
    "\n",
    "在终端中执行以下命令，它会将 `data\\maichart-converts` 目录下的所有谱面处理并输出到 `data\\serialized` 目录。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83aba23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src\\serializer\\src\\bin\\Release\\net8.0\\SimaiSerializerFromMajdataEdit.exe data\\maichart-converts data\\serialized\n"
     ]
    }
   ],
   "source": [
    "command = (\n",
    "    r\"src\\serializer\\src\\bin\\Release\\net8.0\\SimaiSerializerFromMajdataEdit.exe \"\n",
    "    r\"data\\maichart-converts data\\serialized\"\n",
    ")\n",
    "print(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebc6c18",
   "metadata": {},
   "source": [
    "该工具的通用命令格式为： `SimaiSerializerFromMajdataEdit.exe <输入文件或目录> <输出目录>`\n",
    "\n",
    "执行完毕后，我们将得到包含 note 序列数据的 JSON 文件，每个文件对应一个特定难度的谱面。\n",
    "\n",
    "**TODO**：\n",
    "- 运行序列化工具并检查输出结果\n",
    "- 验证生成的 JSON 文件结构\n",
    "- 统计不同谱面的 note 数量分布，为序列长度标准化做准备\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50b50b8",
   "metadata": {},
   "source": [
    "### 2.2 处理谱面标签数据\n",
    "\n",
    "从 maimai-songs 库的 songs.json 中提取训练标签：\n",
    "- **歌曲ID**：song_id（json中为id）\n",
    "- **难度序号**：level_index（在json中并未显式标明，charts中依次对应level_index 1-5的数据）\n",
    "- **难度定数**：difficulty_constant（json中为level）- 这是我们的预测目标\n",
    "\n",
    "**TODO**：\n",
    "- 提取标签数据并与序列化的谱面数据进行匹配\n",
    "- 处理缺失的难度定数（null值）\n",
    "- 过滤掉六位数ID的宴谱数据\n",
    "- 从 flevel.json 中获取拟合等级数据作为辅助信息\n",
    "- 验证标签与谱面文件的一一对应关系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f5452c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已更新 d:\\wushuopei\\code\\BMK-mdp\\data\\song_info.csv，共写入 5478 条json路径。\n"
     ]
    }
   ],
   "source": [
    "def extract_song_info(songs_metadata_path):\n",
    "    # 添加文件存在性检查\n",
    "    if not os.path.exists(songs_metadata_path):\n",
    "        print(f\"错误：文件不存在 - {songs_metadata_path}\")\n",
    "        print(\"请检查：\")\n",
    "        print(f\"1. 文件实际位置（当前工作目录：{os.getcwd()}）\")\n",
    "        print(\"2. 路径是否正确（注意大小写）\")\n",
    "        sys.exit(1)  # 退出程序\n",
    "    \n",
    "    # 读取JSON文件\n",
    "    with open(songs_metadata_path, 'r', encoding='utf-8') as f:\n",
    "        songs_data = json.load(f)\n",
    "    \n",
    "    # 提取所需信息\n",
    "    extracted_info = []\n",
    "    for song in songs_data:\n",
    "        song_id = song.get('id')\n",
    "        charts = song.get('charts', [])\n",
    "        \n",
    "        for level_index, chart in enumerate(charts, start=1):\n",
    "            difficulty_constant = chart.get('level')\n",
    "            extracted_info.append({\n",
    "                'song_id': song_id,\n",
    "                'level_index': level_index,\n",
    "                'difficulty_constant': difficulty_constant\n",
    "            })\n",
    "    \n",
    "    return extracted_info\n",
    "\n",
    "def write_to_csv(data, csv_file_path):\n",
    "    # 创建输出目录（如果不存在）\n",
    "    os.makedirs(os.path.dirname(csv_file_path), exist_ok=True)\n",
    "    \n",
    "    # 写入CSV文件\n",
    "    with open(csv_file_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "        fieldnames = ['song_id', 'level_index', 'difficulty_constant']\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "        \n",
    "        writer.writeheader()\n",
    "        for item in data:\n",
    "            writer.writerow(item)\n",
    "\n",
    "# 使用跨平台路径处理\n",
    "base_dir = os.path.dirname(os.path.abspath(''))  # 获取当前工作目录\n",
    "json_path = os.path.join(base_dir, \"data\", \"maimai-songs\", \"songs.json\")  # 修正路径\n",
    "csv_path = os.path.join(base_dir, \"data\", \"song_info.csv\")  # 修正路径\n",
    "\n",
    "print(f\"尝试读取JSON文件: {json_path}\")\n",
    "extracted_data = extract_song_info(json_path)\n",
    "write_to_csv(extracted_data, csv_path)\n",
    "print(f\"成功提取 {len(extracted_data)} 条记录，已写入 {csv_path}\")\n",
    "\n",
    "# 将对应谱面json的文件名写入csv\n",
    "import glob\n",
    "\n",
    "def update_song_info_with_json_paths(serialized_dir, csv_path):\n",
    "    # 读取csv\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df = df.dropna(subset=['song_id', 'level_index','difficulty_constant'])  # 过滤掉缺失值\n",
    "    # 去除.0，确保为整数\n",
    "    df['song_id'] = df['song_id'].apply(lambda x: int(float(x)))\n",
    "    df['level_index'] = df['level_index'].apply(lambda x: int(float(x)))\n",
    "    # 新增一列用于存储json文件名\n",
    "    df['json_filename'] = None\n",
    "\n",
    "    # 遍历所有json文件\n",
    "    json_files = glob.glob(os.path.join(serialized_dir, \"*.json\"))\n",
    "    for json_file in json_files:\n",
    "        with open(json_file, 'r', encoding='utf-8') as f:\n",
    "            try:\n",
    "                data = json.load(f)\n",
    "                song_id = int(data['song_id'])\n",
    "                level_index = int(data['level_index'])\n",
    "            except Exception as e:\n",
    "                print(f\"解析失败: {json_file}, 错误: {e}\")\n",
    "                continue\n",
    "\n",
    "        # 找到对应行并写入文件名\n",
    "        mask = (df['song_id'] == song_id) & (df['level_index'] == level_index)\n",
    "        df.loc[mask, 'json_filename'] = os.path.basename(json_file)\n",
    "\n",
    "    # 保存回csv\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"已更新 {csv_path}，共写入 {df['json_filename'].notnull().sum()} 条json路径。\")\n",
    "\n",
    "# 用法示例\n",
    "base_dir = os.path.dirname(os.path.abspath(''))\n",
    "serialized_dir = os.path.join(base_dir, \"data\", \"serialized\")\n",
    "csv_path = os.path.join(base_dir, \"data\", \"song_info.csv\")\n",
    "update_song_info_with_json_paths(serialized_dir, csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c8e756",
   "metadata": {},
   "source": [
    "## 3. 序列预处理与特征编码\n",
    "\n",
    "不同于传统的特征工程方法，我们直接使用原始的 note 序列数据。主要任务是将 note 属性转换为数值向量，并处理序列长度不一致的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8934d3c1",
   "metadata": {},
   "source": [
    "### 3.1 构建自定义Dataset类\n",
    "我们创建一个自定义Dataset，存储json文件的位置以及csv的位置。\n",
    "在Dataset中，需要实现：\n",
    "1. `__init__()`：初始化函数，传入serialized目录以及csv文件位置。\n",
    "    - 需要存储每个json的路径\n",
    "2. `__len__()`：返回数据集的长度。\n",
    "3. `__getitem__()`：返回数据集中的第i个样本。直接返回tensor\n",
    "    - 在`__getitem__()`中才读取json文件，并返回tensor\n",
    "    - 读取json文件，然后再去csv中找对应`(song_id,level_index)`的行\n",
    "    - index顺序是什么"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "590d3594",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "class MaichartDataset(Dataset):\n",
    "    def __init__(self, serialized_dir, labels_csv):\n",
    "        self.serialized_dir = serialized_dir\n",
    "        self.labels_data = pd.read_csv(labels_csv)\n",
    "\n",
    "        cleaned_data = self.labels_data.dropna(subset=['song_id', 'level_index', 'difficulty_constant'])\n",
    "\n",
    "        self.label_map = (\n",
    "            cleaned_data.astype({'song_id': int, 'level_index': int, 'difficulty_constant': float})\n",
    "            .set_index(['song_id', 'level_index'])['difficulty_constant']\n",
    "            .to_dict()\n",
    "        )\n",
    "\n",
    "        # TouchArea映射\n",
    "        self.touch_area_mapping = {\" \": 0, \"A\": 1, \"D\": 2, \"E\": 3, \"B\": 4, \"C\": 5} # 从外到内\n",
    "\n",
    "        # 初始化编码器\n",
    "        self._setup_encoders()\n",
    "        \n",
    "        # 获取JSON文件路径\n",
    "        self.json_paths = glob.glob(os.path.join(serialized_dir, \"*.json\"))\n",
    "\n",
    "    def _setup_encoders(self):\n",
    "        \"\"\"设置note类型和位置的编码器\"\"\"\n",
    "        # Note类型编码器\n",
    "        NOTE_TYPES = ['Tap', 'Hold', 'Slide', 'Touch', 'TouchHold']\n",
    "        self.note_type_encoder = OneHotEncoder(\n",
    "            sparse_output=False,\n",
    "            dtype=np.float32,\n",
    "            handle_unknown='ignore'\n",
    "            )\n",
    "\n",
    "        self.note_type_encoder.fit(np.array(NOTE_TYPES).reshape(-1, 1))\n",
    "        \n",
    "        # 位置编码器（假设位置范围是1-8）\n",
    "        positions = list(range(1, 9))  # maimai有8个位置\n",
    "        self.position_encoder = OneHotEncoder(\n",
    "            sparse_output=False,\n",
    "            dtype=np.float32,\n",
    "            handle_unknown='ignore'\n",
    "        )\n",
    "        self.position_encoder.fit(np.array(positions).reshape(-1, 1))\n",
    "\n",
    "    def _encode_note_type(self, note_type):\n",
    "        \"\"\"将note类型编码为one-hot向量\"\"\"\n",
    "        return self.note_type_encoder.transform([[note_type]])[0]\n",
    "    \n",
    "    def _encode_position(self, position):\n",
    "        \"\"\"将位置编码为one-hot向量\"\"\"\n",
    "        return self.position_encoder.transform([[position]])[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        json_file_name = self.json_paths[index]\n",
    "        json_file_path = os.path.join(self.serialized_dir, json_file_name)\n",
    "        \n",
    "        with open(json_file_path, 'r', encoding='utf-8') as f:\n",
    "            try:\n",
    "                json_data = json.load(f)\n",
    "            except json.JSONDecodeError as e:\n",
    "                raise ValueError(f\"JSON解析失败: {json_file_path}\") from e\n",
    "            \n",
    "            # 获取定数标签\n",
    "            song_id = int(json_data['song_id'])\n",
    "            level_index = int(json_data['level_index'])\n",
    "            difficulty_constant = self.label_map.get((song_id, level_index))\n",
    "            if difficulty_constant is None:\n",
    "                raise ValueError(f\"找不到对应的难度定数: song_id={song_id}, level_index={level_index}\")\n",
    "\n",
    "            # 加载谱面数据\n",
    "            note_groups = json_data.get('notes', [])\n",
    "            note_features_sequence = []\n",
    "            for note_group in note_groups:\n",
    "                time = note_group['Time']\n",
    "                notes = note_group['Notes']\n",
    "                for note in notes:\n",
    "                    note_type = note['noteType']\n",
    "                    # 将note类型转换为数值编码\n",
    "                    note_type_encoded = self._encode_note_type(note_type)\n",
    "                    position = note['startPosition']\n",
    "                    # 将位置转换为数值编码\n",
    "                    position_encoded = self._encode_position(position)\n",
    "                    hold_time = note.get('holdTime', 0)\n",
    "                    is_break = int(note['isBreak'])\n",
    "                    is_ex = int(note['isEx'])\n",
    "                    is_slide_break = int(note['isSlideBreak'])\n",
    "                    slide_start_time = note['slideStartTime']\n",
    "                    slide_end_time = slide_start_time + note['slideTime']\n",
    "                    touch_area = self.touch_area_mapping[note['touchArea']]\n",
    "                    feature_vector = np.concatenate([\n",
    "                        [time],             # 1维\n",
    "                        note_type_encoded,  # 5维\n",
    "                        position_encoded,   # 8维\n",
    "                        [hold_time],        # 1维\n",
    "                        [is_break],         # 1维\n",
    "                        [is_ex],            # 1维\n",
    "                        [is_slide_break],   # 1维\n",
    "                        [slide_start_time], # 1维\n",
    "                        [slide_end_time],   # 1维\n",
    "                        [touch_area]        # 1维\n",
    "                    ]) # 总共 17维\n",
    "                    note_features_sequence.append(feature_vector)\n",
    "\n",
    "        # 将谱面数据转换为张量\n",
    "        note_features_tensor = torch.from_numpy(np.array(note_features_sequence, dtype=np.float32))\n",
    "        difficulty_constant_tensor = torch.tensor(difficulty_constant, dtype=torch.float32)\n",
    "        return note_features_tensor, difficulty_constant_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.json_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd0e33f",
   "metadata": {},
   "source": [
    "**序列处理**：\n",
    "- **序列长度标准化**：使用 padding 或截断将所有序列调整为相同长度\n",
    "- **序列归一化**：对时间特征进行归一化处理（暂不处理）\n",
    "- **序列排序**：确保 note 按时间顺序排列（好像不需要）\n",
    "\n",
    "**TODO**：\n",
    "- 设计 note 特征的编码方案\n",
    "- 确定最佳的序列长度\n",
    "- 实现序列预处理管道\n",
    "- 考虑是否需要添加全局特征（如 BPM、总时长等）\n",
    "\n",
    "我们已经在自定义数据集中完成了note属性编码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d627f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    自定义的collate_fn，用于处理变长序列。\n",
    "    - 对note序列进行padding，使其在batch内长度一致。\n",
    "    - 将标签堆叠成一个tensor。\n",
    "    \"\"\"\n",
    "    # 1. 分离序列和标签\n",
    "    # batch中的每个元素是 (note_features_tensor, difficulty_constant_tensor)\n",
    "    sequences, labels = zip(*batch)\n",
    "\n",
    "    # 2. 对序列进行padding\n",
    "    # pad_sequence期望一个tensor列表\n",
    "    # batch_first=True使输出的形状为 (batch_size, sequence_length, feature_dim)\n",
    "    padded_sequences = pad_sequence(sequences, batch_first=True, padding_value=0.0)\n",
    "\n",
    "    # 3. 将标签堆叠成一个tensor\n",
    "    # torch.stack(labels) 会创建一个 [batch_size] 的1D张量\n",
    "    # .view(-1, 1) 将其转换为 [batch_size, 1] 以匹配模型输出\n",
    "    labels_tensor = torch.stack(labels).view(-1, 1)\n",
    "\n",
    "    return padded_sequences, labels_tensor\n",
    "\n",
    "base_dir = os.path.dirname(os.path.abspath(''))\n",
    "\n",
    "# 创建数据集和数据加载器\n",
    "serialized_dir = os.path.join(base_dir, \"data\", \"serialized\")\n",
    "csv_path = os.path.join(base_dir, \"data\", \"song_info.csv\")\n",
    "maichart_dataset = MaichartDataset(serialized_dir, csv_path)\n",
    "\n",
    "data_loader = DataLoader(\n",
    "    maichart_dataset,\n",
    "    batch_size=3    ,  # 可以根据需要调整批次大小\n",
    "    shuffle=True,   # 是否打乱数据\n",
    "    collate_fn=collate_fn,  # 使用自定义的collate_fn\n",
    "    num_workers=0  # 根据系统性能调整工作线程数\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "22f69aba",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "找不到对应的难度定数: song_id=749, level_index=2",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# 示例调用\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[43mtest_data_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mtest_data_loader\u001b[39m\u001b[34m(data_loader, num_batches)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtest_data_loader\u001b[39m(data_loader, num_batches=\u001b[32m1\u001b[39m):\n\u001b[32m      4\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[33;03m    简单测试 DataLoader 输出 shape 和 padding 效果。\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mpadded_sequences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mBatch \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mbatch_idx\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m:\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m  padded_sequences.shape: \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mpadded_sequences\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# (batch_size, seq_len, feature_dim)\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\wushuopei\\code\\BMK-mdp\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\wushuopei\\code\\BMK-mdp\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\wushuopei\\code\\BMK-mdp\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 68\u001b[39m, in \u001b[36mMaichartDataset.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m     66\u001b[39m difficulty_constant = \u001b[38;5;28mself\u001b[39m.label_map.get((song_id, level_index))\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m difficulty_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m找不到对应的难度定数: song_id=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msong_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, level_index=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlevel_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     70\u001b[39m \u001b[38;5;66;03m# 加载谱面数据\u001b[39;00m\n\u001b[32m     71\u001b[39m note_groups = json_data.get(\u001b[33m'\u001b[39m\u001b[33mnotes\u001b[39m\u001b[33m'\u001b[39m, [])\n",
      "\u001b[31mValueError\u001b[39m: 找不到对应的难度定数: song_id=749, level_index=2"
     ]
    }
   ],
   "source": [
    "# 测试数据加载器\n",
    "\n",
    "def test_data_loader(data_loader, num_batches=1):\n",
    "    \"\"\"\n",
    "    简单测试 DataLoader 输出 shape 和 padding 效果。\n",
    "    \"\"\"\n",
    "    for batch_idx, (padded_sequences, labels) in enumerate(data_loader):\n",
    "        print(f\"Batch {batch_idx}:\")\n",
    "        print(f\"  padded_sequences.shape: {padded_sequences.shape}\")  # (batch_size, seq_len, feature_dim)\n",
    "        print(f\"  labels.shape: {labels.shape}\")  # (batch_size, 1)\n",
    "        # 检查 padding 是否为 0\n",
    "        num_padded = (padded_sequences == 0).sum().item()\n",
    "        print(f\"  Number of padded (zero) elements: {num_padded}\")\n",
    "        # 只取前 num_batches 个 batch\n",
    "        if batch_idx + 1 >= num_batches:\n",
    "            break\n",
    "\n",
    "# 示例调用\n",
    "test_data_loader(data_loader, num_batches=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec37d65",
   "metadata": {},
   "source": [
    "## 4. LSTM 模型构建与数据准备\n",
    "\n",
    "构建基于 LSTM 的时序模型来处理 note 序列数据。模型将接收形状为 `(batch_size, sequence_length, feature_dim)` 的输入，输出难度定数的预测值。\n",
    "\n",
    "**模型架构设计**：\n",
    "- **输入层**：接收编码后的 note 序列\n",
    "- **LSTM层**：捕捉序列中的时序依赖关系\n",
    "- **全连接层**：将 LSTM 输出映射到难度预测\n",
    "- **输出层**：回归输出，预测难度定数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c3ed6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 合并特征和标签\n",
    "# full_df = pd.merge(feature_df, label_df, on='song_id')\n",
    "#\n",
    "# # 分离特征和目标变量\n",
    "# X = full_df.drop(['song_id', 'difficulty_constant'], axis=1).values\n",
    "# y = full_df['difficulty_constant'].values\n",
    "#\n",
    "# # 数据标准化\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(X)\n",
    "#\n",
    "# # 划分训练集和测试集\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "#\n",
    "# # 转换为 PyTorch Tensors\n",
    "# X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "# y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "# X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "# y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57606c6",
   "metadata": {},
   "source": [
    "### 4.1 定义 LSTM 模型架构\n",
    "\n",
    "**模型设计考虑**：\n",
    "- **多层 LSTM**：评估单层 vs 多层 LSTM 的效果\n",
    "- **Dropout**：防止过拟合\n",
    "- **Attention 机制**：突出重要的 note 序列部分\n",
    "\n",
    "**TODO**：\n",
    "- 实现基础的 LSTM 模型类\n",
    "- 设计模型的超参数（hidden_size, num_layers, dropout_rate）\n",
    "- 考虑添加注意力机制\n",
    "- 实验不同的模型架构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e18abac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DifficultyPredictor(nn.Module):\n",
    "#     def __init__(self, input_features):\n",
    "#         super(DifficultyPredictor, self).__init__()\n",
    "#         self.layer1 = nn.Linear(input_features, 128)\n",
    "#         self.layer2 = nn.Linear(128, 64)\n",
    "#         self.output_layer = nn.Linear(64, 1)\n",
    "#         self.relu = nn.ReLU()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.relu(self.layer1(x))\n",
    "#         x = self.relu(self.layer2(x))\n",
    "#         x = self.output_layer(x)\n",
    "#         return x\n",
    "\n",
    "# # model = DifficultyPredictor(X_train_tensor.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83eeaf44",
   "metadata": {},
   "source": [
    "## 5. 模型训练与优化\n",
    "\n",
    "**训练策略**：\n",
    "- **损失函数**：使用 MSE 或 MAE 损失函数（回归任务）\n",
    "- **优化器**：Adam 优化器，考虑学习率调度\n",
    "- **批次处理**：合理设置 batch_size 处理变长序列\n",
    "- **正则化**：Dropout + L2 正则化防止过拟合\n",
    "\n",
    "**训练监控**：\n",
    "- 训练损失和验证损失曲线\n",
    "- 早停机制防止过拟合\n",
    "- 学习率衰减策略\n",
    "\n",
    "**TODO**：\n",
    "- 实现训练循环\n",
    "- 设置验证集监控\n",
    "- 实现早停和模型保存机制\n",
    "- 调试序列批次处理中的 padding 问题\n",
    "- 优化训练超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f4c4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 定义损失函数和优化器\n",
    "# criterion = nn.MSELoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "#\n",
    "# # 训练循环\n",
    "# epochs = 100\n",
    "# for epoch in range(epochs):\n",
    "#     model.train()\n",
    "#     optimizer.zero_grad()\n",
    "#     outputs = model(X_train_tensor)\n",
    "#     loss = criterion(outputs, y_train_tensor)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#\n",
    "#     if (epoch+1) % 10 == 0:\n",
    "#         print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e27cee",
   "metadata": {},
   "source": [
    "## 6. 模型评估与性能分析\n",
    "\n",
    "**评估指标**：\n",
    "- **回归指标**：MSE, MAE, R²\n",
    "- **难度区间准确性**：预测值在真实值 ±0.1, ±0.2, ±0.5 范围内的比例\n",
    "- **分布分析**：预测值与真实值的分布对比\n",
    "\n",
    "**详细分析**：\n",
    "- **不同难度等级的预测准确性**：分析模型在低难度 vs 高难度谱面上的表现\n",
    "- **序列长度影响**：分析谱面长度对预测准确性的影响\n",
    "- **错误案例分析**：找出预测偏差较大的谱面特征\n",
    "\n",
    "**TODO**：\n",
    "- 实现全面的评估指标计算\n",
    "- 可视化预测结果分布\n",
    "- 分析不同难度区间的预测准确性\n",
    "- 进行错误案例的深入分析\n",
    "- 与传统特征工程方法进行对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c76b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     predictions = model(X_test_tensor)\n",
    "#     test_loss = criterion(predictions, y_test_tensor)\n",
    "#     print(f'Test Loss: {test_loss.item():.4f}')\n",
    "#\n",
    "# # 可以在这里添加更详细的评估指标，例如 MAE, R^2 等"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f003fee0",
   "metadata": {},
   "source": [
    "## 7. 结果分析与模型迭代\n",
    "\n",
    "**深度分析**：\n",
    "- **时序特征的重要性**：LSTM 是否有效捕捉了时序信息\n",
    "- **不同 note 类型的影响**：哪些类型的 note 对难度预测更重要\n",
    "- **序列长度 vs 准确性**：最优的序列长度设置\n",
    "- **模型复杂度 vs 性能**：单层 vs 多层 LSTM 的权衡\n",
    "\n",
    "**模型优化方向**：\n",
    "- **架构改进**：考虑 Transformer、CNN-LSTM 混合架构\n",
    "- **特征增强**：是否需要添加手工特征作为辅助\n",
    "- **数据增强**：通过时间扭曲、音符变换等方式增加训练数据\n",
    "- **多任务学习**：同时预测难度和其他属性（如技巧需求）\n",
    "\n",
    "**TODO**：\n",
    "- 深入分析 LSTM 学到的时序模式\n",
    "- 可视化注意力权重（如果使用了注意力机制）\n",
    "- 比较不同模型架构的效果\n",
    "- 设计更鲁棒的数据增强策略\n",
    "- 考虑集成学习方法提升性能\n",
    "- 为生产环境部署准备模型压缩和优化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bf24c6",
   "metadata": {},
   "source": [
    "分析模型的预测结果，与真实定数进行比较。\n",
    "\n",
    "思考以下问题：\n",
    "- 模型的误差主要来自哪些谱面？\n",
    "- 是否有必要调整特征工程的方案？\n",
    "- 是否需要更复杂的模型结构？\n",
    "\n",
    "根据分析结果，回到前面的步骤进行迭代优化。\n",
    "\n",
    "**关键思考问题**：\n",
    "\n",
    "1. **时序建模的有效性**：\n",
    "   - LSTM 是否真的比传统统计特征更有效？\n",
    "   - 谱面的时序特征对难度的影响有多大？\n",
    "\n",
    "2. **数据表示的完整性**：\n",
    "   - 当前的 note 编码是否充分表达了游戏的复杂性？\n",
    "   - 是否遗漏了重要的游戏机制信息？\n",
    "\n",
    "3. **模型的可解释性**：\n",
    "   - 如何理解模型学到的难度判断规律？\n",
    "   - 能否提取出可解释的难度评估规则？\n",
    "\n",
    "4. **实际应用价值**：\n",
    "   - 模型的预测精度是否满足实际需求？\n",
    "   - 如何将模型集成到谱面制作工具中？\n",
    "\n",
    "**下一步迭代方向**：\n",
    "根据实验结果，有针对性地改进数据处理、模型架构或训练策略，最终目标是构建一个既准确又实用的难度预测系统。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
